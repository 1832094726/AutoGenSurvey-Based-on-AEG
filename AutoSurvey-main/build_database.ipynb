{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss \n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(\"11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Embedding Model\n",
    "### 加载embedding模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: NomicBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_model = \"./model/nomic-embed-text-v1\"\n",
    "embedding_model = SentenceTransformer('nomic-ai/nomic-embed-text-v1', trust_remote_code=True)\n",
    "# embedding_model = SentenceTransformer(embedding_model, trust_remote_code=True)\n",
    "\n",
    "embedding_model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the pre-prepared JSON file in the format required by TinyDB (refer to arxiv_paper_db.json for the specific format). Each index corresponds to a single paper.\n",
    "### 读取准备好符合tinydb要求json文件（具体格式参考arxiv_paper_db.json）, 每个索引index对应一篇paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/AutoGenSurvey/AutoSurvey-main/database1/arxiv_paper_db.json','r') as f:\n",
    "    papers = json.loads(f.read())\n",
    "papers_l = list(papers['cs_paper_info'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  {'id': '1905.13319',\n",
       "   'title': 'MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms',\n",
       "   'url': 'https://arxiv.org/pdf/1905.13319.pdf',\n",
       "   'date': '2019-05-30',\n",
       "   'abs': 'We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types.',\n",
       "   'cat': 'cs.CL',\n",
       "   'authors': ['Aida Amini',\n",
       "    'Saadia Gabriel',\n",
       "    'Peter Lin',\n",
       "    'Rik Koncel-Kedziorski',\n",
       "    'Yejin Choi',\n",
       "    'Hannaneh Hajishirzi']}),\n",
       " ('2',\n",
       "  {'id': '2009.11506',\n",
       "   'title': 'Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems',\n",
       "   'url': 'https://arxiv.org/pdf/2009.11506.pdf',\n",
       "   'date': '2020-09-24',\n",
       "   'abs': 'We release a new large-scale and template-rich math word problem dataset named Ape210K. It consists of 210K Chinese elementary school-level math problems, each containing both the gold answer and the equations needed to derive the answer.',\n",
       "   'cat': 'cs.CL',\n",
       "   'authors': ['Wei Zhao',\n",
       "    'Mingyue Shang',\n",
       "    'Yang Liu',\n",
       "    'Liang Wang',\n",
       "    'Jingming Liu']}),\n",
       " ('3',\n",
       "  {'id': '2208.05358',\n",
       "   'title': 'CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning',\n",
       "   'url': 'https://arxiv.org/pdf/2208.05358.pdf',\n",
       "   'date': '2022-08-10',\n",
       "   'abs': 'We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario.',\n",
       "   'cat': 'cs.CL',\n",
       "   'authors': ['Adam Dahlgren Lindström', 'Savitha Sam Abraham']}),\n",
       " ('4',\n",
       "  {'id': '2401.11819',\n",
       "   'title': 'SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese',\n",
       "   'url': 'https://arxiv.org/pdf/2401.11819.pdf',\n",
       "   'date': '2024-01-22',\n",
       "   'abs': 'We introduce SuperCLUE-Math6 (SC-Math6), a new benchmark dataset designed to evaluate the mathematical reasoning abilities of Chinese language models.',\n",
       "   'cat': 'cs.CL',\n",
       "   'authors': ['Liang Xu', 'Hang Xue', 'Lei Zhu', 'Kangkang Zhao']}),\n",
       " ('5',\n",
       "  {'id': '2109.04981',\n",
       "   'title': 'GSM8K: A Dataset for Grade School Math Word Problems',\n",
       "   'url': 'https://arxiv.org/pdf/2109.04981.pdf',\n",
       "   'date': '2021-09-10',\n",
       "   'abs': 'We introduce GSM8K, a dataset of 8,000 high-quality linguistically diverse grade school math word problems, each annotated with a detailed step-by-step solution.',\n",
       "   'cat': 'cs.CL',\n",
       "   'authors': ['Dan Hendrycks',\n",
       "    'Saurav Kadavath',\n",
       "    'Evan Lin',\n",
       "    'Steven Basart',\n",
       "    'Mantas Mazeika',\n",
       "    'Andy Zou',\n",
       "    'Dawn Song',\n",
       "    'Jacob Steinhardt']})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_l[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings of abs and title\n",
    "## 对title和abs做embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_l, batch_size=32):\n",
    "    res = []\n",
    "    for i in trange(0, len(text_l), batch_size):\n",
    "        batch_text = ['search_document: ' + _ for _ in text_l[i:i+batch_size]]\n",
    "        res.append(embedding_model.encode(batch_text))\n",
    "    return np.concatenate(res,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_l = [paper[1]['title'] for paper in papers_l]\n",
    "abs_l = [paper[1]['abs'] for paper in papers_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "title_embeddings = get_embeddings(title_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "abs_embeddings = get_embeddings(abs_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert embeddings into faiss-index\n",
    "### 将向量储存为faiss index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_index = faiss.IndexFlatL2(title_embeddings.shape[1])\n",
    "title_index.add(title_embeddings)\n",
    "\n",
    "abs_index = faiss.IndexFlatL2(abs_embeddings.shape[1])\n",
    "abs_index.add(abs_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save faiss-index, replacing the .bin file in the database folder.\n",
    "### 向量保存到本地，替换掉database文件夹中的.bin文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(faiss.index_gpu_to_cpu(title_index), 'titles.index')\n",
    "\n",
    "faiss.write_index(faiss.index_gpu_to_cpu(abs_index), 'abstracts.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the mapping from paper ID to index locally, replacing the arxivid_to_index_abs.json file.\n",
    "### 将paper id到索引的映射保存到本地，替换掉 arxivid_to_index_abs.json文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "paperid_2_index = {}\n",
    "for paper in papers_l:\n",
    "    paper_id = paper[1]['id']\n",
    "    index = paper[0]\n",
    "    paperid_2_index[paper_id] = int(index)\n",
    "with open('./paperid_to_index.json', 'w') as f:\n",
    "    json.dump(paperid_2_index, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the file paths in the __init__ fuction within src/database.py.\n",
    "### 对src/database.py中的__init__部分的初始化文件路径做相应的修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoGenSurvey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
