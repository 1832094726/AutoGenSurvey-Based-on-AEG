{
  "entities": [
    {
      "algorithm_entity": {
        "algorithm_id": "Chang2013_LatentLeftLinkingModel",
        "entity_type": "Algorithm",
        "name": "Latent Left-Linking Model (L3M)",
        "title": "A Constrained Latent Variable Model for Coreference Resolution",
        "year": 2013,
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": [
          "Coreference Resolution"
        ],
        "datasets": [
          "ACE 2004",
          "Ontonotes-5.0"
        ],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Pairwise Mention Scorer",
            "Latent Left-Linking Model"
          ],
          "connections": [
            "Left-Linking"
          ],
          "mechanisms": [
            "Max-Margin Learning",
            "Stochastic Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Structured Prediction",
            "Latent Structural SVM"
          ],
          "parameter_tuning": [
            "Threshold Tuning",
            "Temperature Parameter Tuning"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Compatibility Score Calculation"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chang2013_ConstrainedLatentLeftLinkingModel",
        "entity_type": "Algorithm",
        "name": "Constrained Latent Left-Linking Model (CL3M)",
        "title": "A Constrained Latent Variable Model for Coreference Resolution",
        "year": 2013,
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": [
          "Coreference Resolution"
        ],
        "datasets": [
          "ACE 2004",
          "Ontonotes-5.0"
        ],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Pairwise Mention Scorer",
            "Latent Left-Linking Model",
            "Constraint Injection"
          ],
          "connections": [
            "Left-Linking"
          ],
          "mechanisms": [
            "Max-Margin Learning",
            "Stochastic Gradient Descent",
            "Constraint Augmentation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Structured Prediction",
            "Latent Structural SVM"
          ],
          "parameter_tuning": [
            "Threshold Tuning",
            "Temperature Parameter Tuning",
            "Constraint Weight Tuning"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Compatibility Score Calculation",
          "Constraint Application"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chang2013_ProbabilisticLatentLeftLinkingModel",
        "entity_type": "Algorithm",
        "name": "Probabilistic Latent Left-Linking Model (PL3M)",
        "title": "A Constrained Latent Variable Model for Coreference Resolution",
        "year": 2013,
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": [
          "Coreference Resolution"
        ],
        "datasets": [
          "ACE 2004",
          "Ontonotes-5.0"
        ],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Pairwise Mention Scorer",
            "Probabilistic Latent Left-Linking Model"
          ],
          "connections": [
            "Left-Linking"
          ],
          "mechanisms": [
            "Probabilistic Linking",
            "Stochastic Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Likelihood-Based Learning",
            "Regularized Negative Log-Likelihood"
          ],
          "parameter_tuning": [
            "Threshold Tuning",
            "Temperature Parameter Tuning"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Compatibility Score Calculation"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE_2004",
        "entity_type": "Dataset",
        "name": "ACE 2004",
        "description": "A dataset for coreference resolution containing 443 documents.",
        "domain": "Natural Language Processing",
        "size": 443,
        "year": 2004,
        "creators": [
          "NIST"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Ontonotes_2012",
        "entity_type": "Dataset",
        "name": "Ontonotes-5.0",
        "description": "A large annotated corpus for coreference resolution containing 3,145 documents from various sources.",
        "domain": "Natural Language Processing",
        "size": 3145,
        "year": 2012,
        "creators": [
          "Pradhan et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "MUC_Coreference",
        "entity_type": "Metric",
        "name": "MUC",
        "description": "A metric for evaluating coreference resolution systems.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper"
      }
    },
    {
      "metric_entity": {
        "metric_id": "BCUB_Coreference",
        "entity_type": "Metric",
        "name": "BCUB",
        "description": "A metric for evaluating coreference resolution systems.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper"
      }
    },
    {
      "metric_entity": {
        "metric_id": "CEAF_Coreference",
        "entity_type": "Metric",
        "name": "CEAF",
        "description": "A metric for evaluating coreference resolution systems.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chen2014_NeuralNetworkParser",
        "entity_type": "Algorithm",
        "name": "Neural Network Parser",
        "title": "A Fast and Accurate Dependency Parser using Neural Networks",
        "year": 2014,
        "authors": [
          "Danqi Chen",
          "Christopher D. Manning"
        ],
        "task": [
          "Dependency Parsing"
        ],
        "datasets": [
          "English Penn Treebank",
          "Chinese Penn Treebank"
        ],
        "metrics": [
          "Unlabeled Attachment Score",
          "Labeled Attachment Score"
        ],
        "architecture": {
          "components": [
            "Word Embeddings",
            "POS Tag Embeddings",
            "Arc Label Embeddings",
            "Hidden Layer",
            "Softmax Layer"
          ],
          "connections": [
            "Cube Activation Function"
          ],
          "mechanisms": [
            "Dense Representations",
            "Transition-based Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Entropy Loss",
            "L2 Regularization",
            "Mini-batched AdaGrad",
            "Dropout"
          ],
          "parameter_tuning": [
            "Pre-trained Word Embeddings",
            "Random Initialization"
          ]
        },
        "feature_processing": [
          "Word Embeddings",
          "POS Tag Embeddings",
          "Arc Label Embeddings"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "English_Penn_Treebank_2014",
        "entity_type": "Dataset",
        "name": "English Penn Treebank",
        "description": "A dataset for syntactic parsing of English text.",
        "domain": "Natural Language Processing",
        "size": 39832,
        "year": 2014,
        "creators": [
          "Johansson, R.",
          "Nugues, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Chinese_Penn_Treebank_2014",
        "entity_type": "Dataset",
        "name": "Chinese Penn Treebank",
        "description": "A dataset for syntactic parsing of Chinese text.",
        "domain": "Natural Language Processing",
        "size": 16091,
        "year": 2014,
        "creators": [
          "Zhang, Y.",
          "Clark, S."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Unlabeled_Attachment_Score_Parsing",
        "entity_type": "Metric",
        "name": "Unlabeled Attachment Score",
        "description": "Measures the accuracy of dependency parsing without considering labels.",
        "category": "Dependency Parsing Evaluation",
        "formula": "Number of correctly attached words / Total number of words"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Labeled_Attachment_Score_Parsing",
        "entity_type": "Metric",
        "name": "Labeled Attachment Score",
        "description": "Measures the accuracy of dependency parsing considering both attachments and labels.",
        "category": "Dependency Parsing Evaluation",
        "formula": "Number of correctly attached and labeled words / Total number of words"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Raghunathan2010_MultiPassSieve",
        "entity_type": "Algorithm",
        "name": "Multi-Pass Sieve",
        "title": "A Multi-Pass Sieve for Coreference Resolution",
        "year": 2010,
        "authors": [
          "Karthik Raghunathan",
          "Heeyoung Lee",
          "Sudarshan Rangarajan",
          "Nathanael Chambers",
          "Mihai Surdeanu",
          "Dan Jurafsky",
          "Christopher Manning"
        ],
        "task": [
          "Coreference Resolution"
        ],
        "datasets": [
          "ACE2004-ROTH-DEV2",
          "ACE2004-CULOTTA-TEST",
          "ACE2004-NWIRE",
          "MUC6-TEST"
        ],
        "metrics": [
          "Pairwise F1",
          "MUC",
          "B3"
        ],
        "architecture": {
          "components": [
            "Pass 1- Exact Match",
            "Pass 2- Precise Constructs",
            "Pass 3- Strict Head Matching",
            "Pass 4- Variants of Strict Head",
            "Pass 5- Variants of Strict Head",
            "Pass 6- Relaxed Head Matching",
            "Pass 7- Pronouns"
          ],
          "connections": [
            "Attribute Sharing",
            "Cluster Information"
          ],
          "mechanisms": [
            "Deterministic Models",
            "Tiered Model"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Unsupervised"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Syntactic Information",
          "Attribute Detection",
          "Modifier Information",
          "Discourse Salience"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-ROTH-DEV2_2004",
        "entity_type": "Dataset",
        "name": "ACE2004-ROTH-DEV2",
        "description": "Development split of Bengston and Roth(2008) from the 2004 Automatic Content Extraction (ACE) evaluation.",
        "domain": "Natural Language Processing",
        "size": 68,
        "year": 2004,
        "creators": [
          "Bengston and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-CULOTTA-TEST_2004",
        "entity_type": "Dataset",
        "name": "ACE2004-CULOTTA-TEST",
        "description": "Partition of ACE 2004 corpus reserved for testing by several previous works.",
        "domain": "Natural Language Processing",
        "size": 107,
        "year": 2004,
        "creators": [
          "Culotta et al.",
          "Bengston and Roth",
          "Haghighi and Klein"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-NWIRE_2004",
        "entity_type": "Dataset",
        "name": "ACE2004-NWIRE",
        "description": "Newswire subset of the ACE 2004 corpus utilized for testing.",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": 2004,
        "creators": [
          "Poon and Domingos",
          "Haghighi and Klein"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MUC6-TEST_1995",
        "entity_type": "Dataset",
        "name": "MUC6-TEST",
        "description": "Test corpus from the sixth Message Understanding Conference (MUC-6) evaluation.",
        "domain": "Natural Language Processing",
        "size": 30,
        "year": 1995,
        "creators": [
          "Vilain et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Pairwise_F1_Coreference",
        "entity_type": "Metric",
        "name": "Pairwise F1",
        "description": "Computed over mention pairs in the same entity cluster.",
        "category": "Coreference Evaluation",
        "formula": "2 * (precision * recall) / (precision + recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "B3_Coreference",
        "entity_type": "Metric",
        "name": "B3",
        "description": "Uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the predicted and gold clusters as denominators for precision and recall.",
        "category": "Coreference Evaluation",
        "formula": "Not specified in the paper"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
        "entity_type": "Algorithm",
        "name": "Structured Self-attentive Sentence Embedding",
        "title": "A Structured Self-attentive Sentence Embedding",
        "year": 2017,
        "authors": [
          "Zhouhan Lin",
          "Minwei Feng",
          "Cicero Nogueira dos Santos",
          "Mo Yu",
          "Bing Xiang",
          "Bowen Zhou",
          "Yoshua Bengio"
        ],
        "task": [
          "Author Profiling",
          "Sentiment Classification",
          "Textual Entailment"
        ],
        "datasets": [
          "Age Dataset",
          "Yelp Dataset",
          "SNLI Corpus"
        ],
        "metrics": [
          "Classification Accuracy"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM",
            "Self-attention Mechanism"
          ],
          "connections": [
            "Weighted Sum of Hidden States"
          ],
          "mechanisms": [
            "Softmax",
            "Tanh",
            "Penalization Term"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent",
            "AdaGrad"
          ],
          "parameter_tuning": [
            "Dropout",
            "L2 Regularization",
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Embeddings",
          "Tokenization"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Age_Dataset_2017",
        "entity_type": "Dataset",
        "name": "Age Dataset",
        "description": "Twitter tweets in English, Spanish, and Dutch with age and gender labels",
        "domain": "Social Media Analysis",
        "size": 76485,
        "year": 2017,
        "creators": [
          "PAN-Webis"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Yelp_Dataset_2017",
        "entity_type": "Dataset",
        "name": "Yelp Dataset",
        "description": "2.7M Yelp reviews with star ratings",
        "domain": "Sentiment Analysis",
        "size": 2700000,
        "year": 2017,
        "creators": [
          "Yelp"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SNLI_Corpus_2015",
        "entity_type": "Dataset",
        "name": "SNLI Corpus",
        "description": "570k human-written English sentence pairs manually labeled for entailment, contradiction, and neutral",
        "domain": "Textual Entailment",
        "size": 570000,
        "year": 2015,
        "creators": [
          "Bowman et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Classification",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Classification accuracy",
        "category": "Classification Evaluation",
        "formula": "Correct classifications / Total samples"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Liang2016_TagBasedSolver",
        "entity_type": "Algorithm",
        "name": "Tag-based English Math Word Problem Solver",
        "title": "A Tag-based English Math Word Problem Solver with Understanding, Reasoning and Explanation",
        "year": 2016,
        "authors": [
          "Chao-Chun Liang",
          "Kuang-Yi Hsu",
          "Chien-Tsung Huang",
          "Chung-Min Li",
          "Shen-Yu Miao",
          "Keh-Yih Su"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "MA1_2014",
          "MA2_2014",
          "IXL_2014"
        ],
        "metrics": [
          "Accuracy",
          "Solution Type Accuracy"
        ],
        "architecture": {
          "components": [
            "Language Analyzer",
            "Solution Type Classifier",
            "Logic Form Converter",
            "Inference Engine",
            "Explanation Generator"
          ],
          "connections": [
            "Pipeline"
          ],
          "mechanisms": [
            "Tag-based annotation",
            "First-order logic predicates",
            "Logic inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SVM classifier with linear kernel"
          ],
          "parameter_tuning": [
            "Feature sets: verb category features, keyword indicators, pattern-matching indicators"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "POS tagging",
          "Lemmatization",
          "Named entity recognition",
          "Parsing",
          "Co-reference resolution"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MA1_2014",
        "entity_type": "Dataset",
        "name": "MA1",
        "description": "Simple math word problems on addition and subtraction for third, fourth, and fifth graders",
        "domain": "Mathematics",
        "size": 395,
        "year": 2014,
        "creators": [
          "Hosseini et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MA2_2014",
        "entity_type": "Dataset",
        "name": "MA2",
        "description": "Math word problems with more irrelevant information",
        "domain": "Mathematics",
        "size": 395,
        "year": 2014,
        "creators": [
          "Hosseini et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IXL_2014",
        "entity_type": "Dataset",
        "name": "IXL",
        "description": "Math word problems with more information gaps",
        "domain": "Mathematics",
        "size": 395,
        "year": 2014,
        "creators": [
          "Hosseini et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "SolutionTypeAccuracy_Classification",
        "entity_type": "Metric",
        "name": "Solution Type Accuracy",
        "description": "Accuracy of identifying the correct solution type",
        "category": "Classification",
        "formula": "Correct solution types / Total solution types"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goldberg2010_EasyFirstNonDirectionalParser",
        "entity_type": "Algorithm",
        "name": "Easy-First Non-Directional Dependency Parsing",
        "title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing",
        "year": 2010,
        "authors": [
          "Yoav Goldberg",
          "Michael Elhadad"
        ],
        "task": [
          "Dependency Parsing"
        ],
        "datasets": [
          "WSJ Treebank",
          "CoNLL 2007 English dataset"
        ],
        "metrics": [
          "Accuracy",
          "Root",
          "Complete"
        ],
        "architecture": {
          "components": [
            "ATTACHLEFT",
            "ATTACHRIGHT"
          ],
          "connections": [
            "Dependency edges"
          ],
          "mechanisms": [
            "Non-directional parsing",
            "Best-first parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Structured Perceptron"
          ],
          "parameter_tuning": [
            "Parameter averaging"
          ]
        },
        "feature_processing": [
          "POS tagging",
          "Feature templates"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WSJ_Treebank_2010",
        "entity_type": "Dataset",
        "name": "WSJ Treebank",
        "description": "Wall Street Journal Treebank",
        "domain": "Natural Language Processing",
        "size": "Sections 2-21 for training, Section 22 for development, Section 23 for testing",
        "year": 2010,
        "creators": [
          "Penn Treebank"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CoNLL_2007_English_dataset_2010",
        "entity_type": "Dataset",
        "name": "CoNLL 2007 English dataset",
        "description": "English dataset from the CoNLL 2007 shared task",
        "domain": "Natural Language Processing",
        "size": "Smaller than WSJ Treebank, created using a different conversion procedure",
        "year": 2010,
        "creators": [
          "CoNLL 2007"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Root_Prediction",
        "entity_type": "Metric",
        "name": "Root",
        "description": "Percentage of sentences in which the ROOT attachment is correct",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correct ROOT attachments / Total sentences"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Complete_Correct_Parses",
        "entity_type": "Metric",
        "name": "Complete",
        "description": "Percentage of sentences in which all tokens were assigned their correct parent",
        "category": "Dependency Parsing Evaluation",
        "formula": "Sentences with all correct attachments / Total sentences"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Earley1970_EfficientParsingAlgorithm",
        "entity_type": "Algorithm",
        "name": "Efficient Context-Free Parsing Algorithm",
        "title": "An Efficient Context-Free Parsing Algorithm",
        "year": 1970,
        "authors": [
          "Jay Earley"
        ],
        "task": [
          "Parsing context-free grammars"
        ],
        "datasets": [],
        "metrics": [
          "Time complexity",
          "Space complexity"
        ],
        "architecture": {
          "components": [
            "Predictor",
            "Completer",
            "Scanner"
          ],
          "connections": [
            "State transitions"
          ],
          "mechanisms": [
            "LR(k) parsing",
            "Top-down parsing",
            "Bottom-up parsing"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Look-ahead",
          "State set construction"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "TimeComplexity_Parsing",
        "entity_type": "Metric",
        "name": "Time complexity",
        "description": "Time required to parse a string",
        "category": "Algorithm performance",
        "formula": "O(n^3) for general context-free grammars, O(n^2) for unambiguous grammars, O(n) for bounded state grammars"
      }
    },
    {
      "metric_entity": {
        "metric_id": "SpaceComplexity_Parsing",
        "entity_type": "Metric",
        "name": "Space complexity",
        "description": "Memory required to parse a string",
        "category": "Algorithm performance",
        "formula": "O(n^2)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Alvin2015_GeoTutor",
        "entity_type": "Algorithm",
        "name": "GeoTutor",
        "title": "Automatic Synthesis of Geometry Problems for an Intelligent Tutoring System",
        "year": 2015,
        "authors": [
          "Chris Alvin",
          "Sumit Gulwani",
          "Rupak Majumdar",
          "Supratik Mukhopadhyay"
        ],
        "task": [
          "Euclidean Geometry Problem Synthesis"
        ],
        "datasets": [
          "High School Geometry Problems"
        ],
        "metrics": [
          "Proof Width",
          "Proof Length",
          "Deductive Steps"
        ],
        "architecture": {
          "components": [
            "Hypergraph Construction",
            "Pebbling Algorithm"
          ],
          "connections": [
            "Forward Edges",
            "Back-Edges"
          ],
          "mechanisms": [
            "Traversal Algorithm",
            "Coarse Problem Homomorphism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Breadth-First Pebbling",
            "Coordinate-Based Computation"
          ],
          "parameter_tuning": [
            "User Query Restrictions"
          ]
        },
        "feature_processing": [
          "Assumption Filtering",
          "Geometric Object Representation"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "HighSchoolGeometryProblems_2015",
        "entity_type": "Dataset",
        "name": "High School Geometry Problems",
        "description": "A collection of geometry problems from standard high school textbooks",
        "domain": "Education",
        "size": 110,
        "year": 2015,
        "creators": [
          "Sinclair",
          "Dikshit",
          "Boyd",
          "Larson",
          "Jurgensen",
          "Brown"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "ProofWidth_Classification",
        "entity_type": "Metric",
        "name": "Proof Width",
        "description": "The width of the problem hypergraph",
        "category": "Classification Assessment",
        "formula": "Width of the hypergraph"
      }
    },
    {
      "metric_entity": {
        "metric_id": "ProofLength_Classification",
        "entity_type": "Metric",
        "name": "Proof Length",
        "description": "The diameter of the problem hypergraph",
        "category": "Classification Assessment",
        "formula": "Diameter of the hypergraph"
      }
    },
    {
      "metric_entity": {
        "metric_id": "DeductiveSteps_Classification",
        "entity_type": "Metric",
        "name": "Deductive Steps",
        "description": "The number of hyperedges in the problem hypergraph",
        "category": "Classification Assessment",
        "formula": "Number of hyperedges"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Shi2015_SigmaDolphin",
        "entity_type": "Algorithm",
        "name": "SigmaDolphin",
        "title": "Automatically Solving Number Word Problems by Semantic Parsing and Reasoning",
        "year": 2015,
        "authors": [
          "Shuming Shi",
          "Yuehui Wang",
          "Chin-Yew Lin",
          "Xiaojiang Liu",
          "Yong Rui"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Algebra.com",
          "Answers.Yahoo.com"
        ],
        "metrics": [
          "Precision",
          "Recall",
          "F1"
        ],
        "architecture": {
          "components": [
            "Semantic Parser",
            "Reasoning Module"
          ],
          "connections": [
            "CFG Parser",
            "DOL Trees"
          ],
          "mechanisms": [
            "Context-Free Grammar",
            "Semantic Interpretation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Top-down Parsing",
            "Earley Algorithm"
          ],
          "parameter_tuning": [
            "Type Compatibility Checking"
          ]
        },
        "feature_processing": [
          "Natural Language Parsing",
          "Math Expression Derivation"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Algebra_com_2015",
        "entity_type": "Dataset",
        "name": "Algebra.com",
        "description": "A website for users to post math problems and get help from tutors",
        "domain": "Mathematics",
        "size": 1878,
        "year": 2015,
        "creators": [
          "Various Contributors"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Answers_Yahoo_com_2015",
        "entity_type": "Dataset",
        "name": "Answers.Yahoo.com",
        "description": "A website where users can ask and answer questions, including math problems",
        "domain": "Mathematics",
        "size": 1878,
        "year": 2015,
        "creators": [
          "Various Contributors"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Precision_MathWordProblemSolving",
        "entity_type": "Metric",
        "name": "Precision",
        "description": "The proportion of correctly solved problems among all attempted problems",
        "category": "Math Word Problem Solving",
        "formula": "Correctly solved problems / Attempted problems"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall_MathWordProblemSolving",
        "entity_type": "Metric",
        "name": "Recall",
        "description": "The proportion of correctly solved problems among all problems",
        "category": "Math Word Problem Solving",
        "formula": "Correctly solved problems / Total problems"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_MathWordProblemSolving",
        "entity_type": "Metric",
        "name": "F1",
        "description": "The harmonic mean of precision and recall",
        "category": "Math Word Problem Solving",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_Aristo",
        "entity_type": "Algorithm",
        "name": "Aristo",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Clark, P.",
          "Etzioni, O.",
          "Khot, T.",
          "Sabharwal, A.",
          "Tafjord, O.",
          "Turney, P.",
          "Khashabi, D."
        ],
        "task": [
          "Elementary Science Question Answering"
        ],
        "datasets": [
          "NY Regents Science Exam"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "IR Solver",
            "PMI Solver",
            "SVM Solver",
            "RULE Solver",
            "ILP Solver"
          ],
          "connections": [
            "Logistic Regression Combiner"
          ],
          "mechanisms": [
            "Information Retrieval",
            "Pointwise Mutual Information",
            "Support Vector Machine",
            "Rule-based Reasoning",
            "Integer Linear Programming"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Logistic Regression Calibration",
            "TF-IDF Scoring"
          ],
          "parameter_tuning": [
            "Solver-specific Feature Weights"
          ]
        },
        "feature_processing": [
          "Text Parsing",
          "Syntactic Pattern Matching",
          "Lexical Chunk Matching",
          "Semantic Constraints"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NY_Regents_Science_Exam_2016",
        "entity_type": "Dataset",
        "name": "NY Regents Science Exam",
        "description": "Standardized science exam questions for 4th grade students",
        "domain": "Elementary Education",
        "size": 237,
        "year": 2016,
        "creators": [
          "New York State Education Department"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_IRSolver",
        "entity_type": "Algorithm",
        "name": "IR Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Clark, P.",
          "Etzioni, O.",
          "Khot, T.",
          "Sabharwal, A.",
          "Tafjord, O.",
          "Turney, P.",
          "Khashabi, D."
        ],
        "task": [
          "Elementary Science Question Answering"
        ],
        "datasets": [
          "NY Regents Science Exam"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Lucene Search Engine"
          ],
          "connections": [],
          "mechanisms": [
            "Information Retrieval"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Query Construction",
          "Non-stopword Overlap"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_PMISolver",
        "entity_type": "Algorithm",
        "name": "PMI Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Clark, P.",
          "Etzioni, O.",
          "Khot, T.",
          "Sabharwal, A.",
          "Tafjord, O.",
          "Turney, P.",
          "Khashabi, D."
        ],
        "task": [
          "Elementary Science Question Answering"
        ],
        "datasets": [
          "Web Corpus"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Pointwise Mutual Information Calculation"
          ],
          "connections": [],
          "mechanisms": [
            "Statistical Association"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Unigram Extraction",
          "Bigram Extraction",
          "Trigram Extraction",
          "Skip-Bigram Extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_SVMSolver",
        "entity_type": "Algorithm",
        "name": "SVM Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Clark, P.",
          "Etzioni, O.",
          "Khot, T.",
          "Sabharwal, A.",
          "Tafjord, O.",
          "Turney, P.",
          "Khashabi, D."
        ],
        "task": [
          "Elementary Science Question Answering"
        ],
        "datasets": [
          "Elementary Science Corpus"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Word Embeddings",
            "Recurrent Neural Network Language Model"
          ],
          "connections": [],
          "mechanisms": [
            "Cosine Similarity",
            "Support Vector Machine"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SVM Ranker"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Domain-appropriate Embeddings Generation"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_RULESolver",
        "entity_type": "Algorithm",
        "name": "RULE Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Clark, P.",
          "Etzioni, O.",
          "Khot, T.",
          "Sabharwal, A.",
          "Tafjord, O.",
          "Turney, P.",
          "Khashabi, D."
        ],
        "task": [
          "Elementary Science Question Answering"
        ],
        "datasets": [
          "Elementary Science Corpus"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Probabilistic First-Order Logic Rules"
          ],
          "connections": [],
          "mechanisms": [
            "Rule-based Reasoning",
            "Textual Entailment Service"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Syntactic Structure Mapping",
          "Generic Statement Conversion"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_ILPSolver",
        "entity_type": "Algorithm",
        "name": "ILP Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Clark, P.",
          "Etzioni, O.",
          "Khot, T.",
          "Sabharwal, A.",
          "Tafjord, O.",
          "Turney, P.",
          "Khashabi, D."
        ],
        "task": [
          "Elementary Science Question Answering"
        ],
        "datasets": [
          "Curated Tables"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Knowledge Tables",
            "Integer Linear Programming"
          ],
          "connections": [],
          "mechanisms": [
            "Table Joining",
            "Proof Graph Construction"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Lexical Chunk Matching",
          "Semantic Constraints"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Robaidek2018_BiLSTMClassifier",
        "entity_type": "Algorithm",
        "name": "BiLSTM Classifier",
        "title": "Data-Driven Methods for Solving Algebra Word Problems",
        "year": 2018,
        "authors": [
          "Robaidek, Benjamin",
          "Koncel-Kedziorski, Rik",
          "Hajishirzi, Hannaneh"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "DRAW",
          "MAWPS",
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "BiLSTM"
          ],
          "connections": [
            "Softmax"
          ],
          "mechanisms": [
            "Cross Entropy Loss"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-End Training"
          ],
          "parameter_tuning": [
            "Learning Rate Decay",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lin2017_StructuredSelfAttention",
        "entity_type": "Algorithm",
        "name": "Structured Self-Attention",
        "title": "Data-Driven Methods for Solving Algebra Word Problems",
        "year": 2018,
        "authors": [
          "Robaidek, Benjamin",
          "Koncel-Kedziorski, Rik",
          "Hajishirzi, Hannaneh"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "DRAW",
          "MAWPS",
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "BiLSTM",
            "Self-Attention"
          ],
          "connections": [
            "Multi-Hop Attention"
          ],
          "mechanisms": [
            "Redundancy Reduction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-End Training"
          ],
          "parameter_tuning": [
            "Learning Rate Decay",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sutskever2014_Seq2Seq",
        "entity_type": "Algorithm",
        "name": "Seq2Seq Model",
        "title": "Data-Driven Methods for Solving Algebra Word Problems",
        "year": 2018,
        "authors": [
          "Robaidek, Benjamin",
          "Koncel-Kedziorski, Rik",
          "Hajishirzi, Hannaneh"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "DRAW",
          "MAWPS",
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "LSTM"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Teacher Forcing"
          ],
          "parameter_tuning": [
            "Learning Rate Decay",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Klein2017_OpenNMT",
        "entity_type": "Algorithm",
        "name": "OpenNMT",
        "title": "Data-Driven Methods for Solving Algebra Word Problems",
        "year": 2018,
        "authors": [
          "Robaidek, Benjamin",
          "Koncel-Kedziorski, Rik",
          "Hajishirzi, Hannaneh"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "DRAW",
          "MAWPS",
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Neural Machine Translation"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "LSTM"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-End Training"
          ],
          "parameter_tuning": [
            "Learning Rate Decay",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DRAW_2015",
        "entity_type": "Dataset",
        "name": "DRAW",
        "description": "A challenging and diverse algebra word problem set",
        "domain": "Natural Language Processing",
        "size": 1000,
        "year": 2015,
        "creators": [
          "Upadhyay, Shyam",
          "Chang, Ming-Wei"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MAWPS_2016",
        "entity_type": "Dataset",
        "name": "MAWPS",
        "description": "A math word problem repository",
        "domain": "Natural Language Processing",
        "size": 2373,
        "year": 2016,
        "creators": [
          "Koncel-Kedziorski, Rik",
          "Roy, Subhro",
          "Amini, Aida",
          "Kushman, Nate",
          "Hajishirzi, Hannaneh"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Math23K_2017",
        "entity_type": "Dataset",
        "name": "Math23K",
        "description": "A large dataset of Chinese algebra word problems",
        "domain": "Natural Language Processing",
        "size": 23164,
        "year": 2017,
        "creators": [
          "Wang, Yan",
          "Liu, Xiaojiang",
          "Shi, Shuming"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "OracleAccuracy_Classification",
        "entity_type": "Metric",
        "name": "Oracle Accuracy",
        "description": "Upper bound accuracy based on the presence of test equation templates in the training data",
        "category": "Classification Evaluation",
        "formula": "Number of test equation templates appearing in the training data / Total test equation templates"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_RNNBasedSeq2SeqModel",
        "entity_type": "Algorithm",
        "name": "RNN-based Seq2Seq Model",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": 2017,
        "authors": [
          "Wang, Yan",
          "Liu, Xiaojiang",
          "Shi, Shuming"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K",
          "Alg514"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Word Embedding Layer",
            "GRU Encoder",
            "LSTM Decoder"
          ],
          "connections": [
            "GRU",
            "LSTM"
          ],
          "mechanisms": [
            "Gated Recurrent Unit",
            "Long Short-Term Memory"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Tokenization"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_SignificantNumberIdentification",
        "entity_type": "Algorithm",
        "name": "Significant Number Identification (SNI)",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": 2017,
        "authors": [
          "Wang, Yan",
          "Liu, Xiaojiang",
          "Shi, Shuming"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K",
          "Alg514"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "LSTM"
          ],
          "connections": [
            "LSTM"
          ],
          "mechanisms": [
            "Long Short-Term Memory"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Binary Classification"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Context Window"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_HybridModel",
        "entity_type": "Algorithm",
        "name": "Hybrid Model",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": 2017,
        "authors": [
          "Wang, Yan",
          "Liu, Xiaojiang",
          "Shi, Shuming"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K",
          "Alg514"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Retrieval Model"
          ],
          "connections": [
            "Threshold-Based Selection"
          ],
          "mechanisms": [
            "Jaccard Similarity"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Threshold-Based Selection"
          ],
          "parameter_tuning": [
            "Similarity Threshold"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Tokenization"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Alg514_2014",
        "entity_type": "Dataset",
        "name": "Alg514",
        "description": "A dataset of 514 linear algebra problems",
        "domain": "Natural Language Processing",
        "size": 514,
        "year": 2014,
        "creators": [
          "Kushman, Nate",
          "Artzi, Yoav",
          "Zettlemoyer, Luke",
          "Barzilay, Regina"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Seo2014_G-ALIGNER",
        "entity_type": "Algorithm",
        "name": "G-ALIGNER",
        "title": "Diagram Understanding in Geometry Questions",
        "year": 2014,
        "authors": [
          "Min Joon Seo",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Oren Etzioni"
        ],
        "task": [
          "Diagram Understanding"
        ],
        "datasets": [
          "Geometry Questions Dataset_2014"
        ],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Primitive Detection",
            "Textual Alignment"
          ],
          "connections": [
            "Submodular Optimization"
          ],
          "mechanisms": [
            "Greedy Approximation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Submodular Function Maximization"
          ],
          "parameter_tuning": [
            "No Parameter Tuning Required"
          ]
        },
        "feature_processing": [
          "Hough Transform",
          "Gaussian Blur",
          "Binarization",
          "Corner Detection"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Geometry_Questions_Dataset_2014",
        "entity_type": "Dataset",
        "name": "Geometry Questions Dataset",
        "description": "A dataset of geometry questions including textual descriptions and diagrams",
        "domain": "Geometry",
        "size": 100,
        "year": 2014,
        "creators": [
          "Min Joon Seo",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Oren Etzioni"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Identifying_Primitives",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Classification Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Aligning_Visual_Elements",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Alignment Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Watanabe1991_PBFDiagramUnderstandingFramework",
        "entity_type": "Algorithm",
        "name": "PBF Diagram Understanding Framework",
        "title": "Diagram Understanding Using Integration of Layout Information and Textual Information",
        "year": 1991,
        "authors": [
          "Yasuhiko Watanabe",
          "Makoto Nagao"
        ],
        "task": [
          "Diagram Understanding"
        ],
        "datasets": [
          "PBF (in Japanese)"
        ],
        "metrics": [
          "Semantic Analysis Accuracy"
        ],
        "architecture": {
          "components": [
            "Layout Information Extraction",
            "Natural Language Information Extraction"
          ],
          "connections": [
            "Integration of Layout and Natural Language Information"
          ],
          "mechanisms": [
            "Symbol Connection",
            "Spatial Relationship Similarity",
            "Expression Pattern Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Manual Annotation of Layout Information",
            "Japanese Morphological Analysis"
          ],
          "parameter_tuning": [
            "Pattern Matching Rules"
          ]
        },
        "feature_processing": [
          "ID Number Assignment",
          "Position Description",
          "Expression Pattern Extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PBF_1991",
        "entity_type": "Dataset",
        "name": "PBF (in Japanese)",
        "description": "Pictorial Book of Flora containing diagrams, pictures, and explanation texts about wild flowers of Japan",
        "domain": "Botanical Illustration",
        "size": 31,
        "year": 1991,
        "creators": [
          "Yasuhiko Watanabe",
          "Makoto Nagao"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "SemanticAnalysisAccuracy_Classification",
        "entity_type": "Metric",
        "name": "Semantic Analysis Accuracy",
        "description": "Accuracy of classifying words in PBF diagrams into five semantic categories",
        "category": "Classification Evaluation",
        "formula": "Correctly classified words / Total words"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2016_DimensionallyGuidedSynthesis",
        "entity_type": "Algorithm",
        "name": "Dimensionally Guided Synthesis",
        "title": "Dimensionally Guided Synthesis of Mathematical Word Problems",
        "year": 2016,
        "authors": [
          "Ke Wang",
          "Zhendong Su"
        ],
        "task": [
          "Math Word Problem Generation"
        ],
        "datasets": [],
        "metrics": [
          "Statistical Indistinguishability",
          "Error Rate"
        ],
        "architecture": {
          "components": [
            "Equation Generator",
            "Narrative Generator"
          ],
          "connections": [
            "Binary Expression Tree",
            "Dimensional Units"
          ],
          "mechanisms": [
            "Dimensional Consistency",
            "Recursive Story Generation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Random Equation Generation",
            "Variable Unrolling"
          ],
          "parameter_tuning": [
            "Dimensional Unit Assignment",
            "Keyword Selection"
          ]
        },
        "feature_processing": [
          "Dimensional Unit Handling",
          "Keyword Assignment",
          "Value Generation"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "StatisticalIndistinguishability_Authenticity",
        "entity_type": "Metric",
        "name": "Statistical Indistinguishability",
        "description": "Measure of whether generated problems are statistically indistinguishable from textbook problems.",
        "category": "Authenticity Evaluation",
        "formula": "Paired t-test and Chi-square test of independence"
      }
    },
    {
      "metric_entity": {
        "metric_id": "ErrorRate_Difficulty",
        "entity_type": "Metric",
        "name": "Error Rate",
        "description": "Measure of the difficulty level of problems based on student error rates.",
        "category": "Difficulty Evaluation",
        "formula": "Percentage of incorrect answers"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_SkipGram",
        "entity_type": "Algorithm",
        "name": "Skip-gram",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": [
          "Word Representation Learning",
          "Phrase Representation Learning"
        ],
        "datasets": [
          "News Articles Dataset"
        ],
        "metrics": [
          "Accuracy",
          "Syntactic Accuracy",
          "Semantic Accuracy"
        ],
        "architecture": {
          "components": [
            "Input Layer",
            "Hidden Layer",
            "Output Layer"
          ],
          "connections": [
            "Weight Matrices"
          ],
          "mechanisms": [
            "Negative Sampling",
            "Hierarchical Softmax"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Subsampling of Frequent Words",
            "Negative Sampling",
            "Hierarchical Softmax"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Context Window Size",
            "Vector Dimensionality"
          ]
        },
        "feature_processing": [
          "Word Tokenization",
          "Phrase Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_HierarchicalSoftmax",
        "entity_type": "Algorithm",
        "name": "Hierarchical Softmax",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": [
          "Word Representation Learning"
        ],
        "datasets": [
          "News Articles Dataset"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Tree",
            "Inner Nodes",
            "Leaf Nodes"
          ],
          "connections": [
            "Paths in Binary Tree"
          ],
          "mechanisms": [
            "Logarithmic Evaluation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Binary Huffman Tree"
          ],
          "parameter_tuning": [
            "Tree Structure"
          ]
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_NegativeSampling",
        "entity_type": "Algorithm",
        "name": "Negative Sampling",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": [
          "Word Representation Learning"
        ],
        "datasets": [
          "News Articles Dataset"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Noise Distribution",
            "Target Word",
            "Negative Samples"
          ],
          "connections": [
            "Logistic Regression"
          ],
          "mechanisms": [
            "Log-Sigmoid Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Logistic Regression Objective"
          ],
          "parameter_tuning": [
            "Number of Negative Samples"
          ]
        },
        "feature_processing": []
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NewsArticlesDataset_2013",
        "entity_type": "Dataset",
        "name": "News Articles Dataset",
        "description": "A large dataset consisting of various news articles",
        "domain": "Natural Language Processing",
        "size": 1000000000,
        "year": 2013,
        "creators": [
          "Google Inc."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "SyntacticAccuracy_Classification",
        "entity_type": "Metric",
        "name": "Syntactic Accuracy",
        "description": "Accuracy on syntactic analogical reasoning",
        "category": "Classification Evaluation",
        "formula": "Correct Syntactic Analogies / Total Syntactic Analogies"
      }
    },
    {
      "metric_entity": {
        "metric_id": "SemanticAccuracy_Classification",
        "entity_type": "Metric",
        "name": "Semantic Accuracy",
        "description": "Accuracy on semantic analogical reasoning",
        "category": "Classification Evaluation",
        "formula": "Correct Semantic Analogies / Total Semantic Analogies"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Slagle1965_DEDUCOM",
        "entity_type": "Algorithm",
        "name": "DEDUCOM",
        "title": "Experiments with a Deductive Question-Answering Program",
        "year": 1965,
        "authors": [
          "James R. Slagle"
        ],
        "task": [
          "Deductive Question-Answering"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Fact Interpreter",
            "Question Reducer",
            "Search Procedure"
          ],
          "connections": [
            "Depth-First Search",
            "Predicate Calculus"
          ],
          "mechanisms": [
            "Logical Deductions",
            "Simple Program Writing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Heuristic Programming"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Fact Input Processing",
          "Question Parsing"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Yu2016_ImplicitQuantityRelationsExtractor",
        "entity_type": "Algorithm",
        "name": "Implicit Quantity Relations Extractor",
        "title": "Extraction of Implicit Quantity Relations for Arithmetic Word Problems in Chinese",
        "year": 2016,
        "authors": [
          "Xinguo Yu",
          "Pengpeng Jian",
          "Mingshu Wang",
          "Shuang Wu"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Elementary school arithmetic application problem 2011"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Chinese phrase parser",
            "SVM classifier",
            "Semantic models"
          ],
          "connections": [
            "Mapping between classifications and semantic models"
          ],
          "mechanisms": [
            "Sequence alignment",
            "Equation construction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SVM with slack variable"
          ],
          "parameter_tuning": [
            "Lagrange multiplier",
            "Weight number for slack variable"
          ]
        },
        "feature_processing": [
          "Chinese phrase parsing",
          "Bag of words extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ElementarySchoolArithmeticApplicationProblem_2011",
        "entity_type": "Dataset",
        "name": "Elementary school arithmetic application problem",
        "description": "Arithmetic word problems for elementary school students",
        "domain": "Education",
        "size": 627,
        "year": 2011,
        "creators": [
          "People's Education Press"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ma2010_FrameBasedCalculus",
        "entity_type": "Algorithm",
        "name": "Frame-Based Calculus",
        "title": "Frame-Based Calculus of solving Arithmetic Multi-Step Addition and Subtraction word problems",
        "year": 2010,
        "authors": [
          "Ma Yuhui",
          "Zhou Ying",
          "Cui Guangzuo",
          "Ren Yun",
          "Huang Ronghuai"
        ],
        "task": [
          "Solving Multi-Step Addition and Subtraction Word Problems"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "MSWPAS-NP",
            "MSWPAS-CP"
          ],
          "connections": [
            "Natural Language Processing",
            "Frame-Based Calculus"
          ],
          "mechanisms": [
            "Means-end Analysis"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Comprehending Natural Language",
          "Constructing Problem Frames"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ChineseElementarySchoolProblems_2010",
        "entity_type": "Dataset",
        "name": "Chinese Elementary School Word Problems",
        "description": "Word problems gathered from four publishers in China",
        "domain": "Education",
        "size": null,
        "year": 2010,
        "creators": [
          "People’s Education Press",
          "Beijing Normal University Press",
          "DONGBEI Normal University Press"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Correctness_Solving",
        "entity_type": "Metric",
        "name": "Correctness",
        "description": "Whether the system correctly solves multi-step addition and subtraction word problems",
        "category": "Problem Solving Evaluation",
        "formula": "Number of Correctly Solved Problems / Total Number of Problems"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "deMarneffe2006_TypedDependencyExtraction",
        "entity_type": "Algorithm",
        "name": "Typed Dependency Extraction",
        "title": "Generating Typed Dependency Parses from Phrase Structure Parses",
        "year": 2006,
        "authors": [
          "Marie-Catherine de Marneffe",
          "Bill MacCartney",
          "Christopher D. Manning"
        ],
        "task": [
          "Dependency Parsing"
        ],
        "datasets": [
          "Penn Treebank"
        ],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Phrase Structure Parser",
            "Dependency Extractor"
          ],
          "connections": [
            "Rule-based Transformation"
          ],
          "mechanisms": [
            "Collins Head Rules",
            "Semantic Head Identification"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Statistical Parsing"
          ],
          "parameter_tuning": [
            "High-Accuracy Statistical Phrase Structure Parser"
          ]
        },
        "feature_processing": [
          "Head Identification",
          "Dependency Labeling"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PennTreebank_1999",
        "entity_type": "Dataset",
        "name": "Penn Treebank",
        "description": "A widely used corpus for parsing English sentences.",
        "domain": "Natural Language Processing",
        "size": "Over 4.5 million words",
        "year": 1999,
        "creators": [
          "Various contributors"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "DependencyAccuracy_Parsing",
        "entity_type": "Metric",
        "name": "Dependency Accuracy",
        "description": "Measures the accuracy of dependency relations in parsing.",
        "category": "Parsing Evaluation",
        "formula": "Correctly labeled dependencies / Total dependencies"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Collins1999_HeadDrivenStatisticalModels",
        "entity_type": "Algorithm",
        "name": "Head-Driven Statistical Models",
        "title": "Head-Driven Statistical Models for Natural Language Parsing",
        "year": 1999,
        "authors": [
          "Michael Collins"
        ],
        "task": [
          "Natural Language Parsing"
        ],
        "datasets": [
          "Penn Treebank"
        ],
        "metrics": [
          "Parsing Accuracy"
        ],
        "architecture": {
          "components": [
            "Head Rules",
            "Statistical Models"
          ],
          "connections": [
            "Dependency Relations"
          ],
          "mechanisms": [
            "Probabilistic Context-Free Grammar"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Bayesian Smoothing"
          ]
        },
        "feature_processing": [
          "Syntactic Analysis"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Charniak2000_MaximumEntropyParser",
        "entity_type": "Algorithm",
        "name": "Maximum-Entropy-Inspired Parser",
        "title": "A maximum-entropy-inspired parser",
        "year": 2000,
        "authors": [
          "Eugene Charniak"
        ],
        "task": [
          "Natural Language Parsing"
        ],
        "datasets": [
          "Penn Treebank"
        ],
        "metrics": [
          "Parsing Accuracy"
        ],
        "architecture": {
          "components": [
            "Maximum Entropy Model"
          ],
          "connections": [
            "Feature Functions"
          ],
          "mechanisms": [
            "Log-linear Model"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Feature Selection"
          ]
        },
        "feature_processing": [
          "Lexical Features",
          "Syntactic Features"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lin1998_MINIPAR",
        "entity_type": "Algorithm",
        "name": "MINIPAR",
        "title": "Dependency-based evaluation of MINIPAR",
        "year": 1998,
        "authors": [
          "Dekang Lin"
        ],
        "task": [
          "Dependency Parsing"
        ],
        "datasets": [
          "Penn Treebank"
        ],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Dependency Parser"
          ],
          "connections": [
            "Dependency Relations"
          ],
          "mechanisms": [
            "Rule-based Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Rule-based"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Syntactic Analysis"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sleator1993_LinkParser",
        "entity_type": "Algorithm",
        "name": "Link Parser",
        "title": "Parsing English with a link grammar",
        "year": 1993,
        "authors": [
          "Daniel D. Sleator",
          "Davy Temperley"
        ],
        "task": [
          "Dependency Parsing"
        ],
        "datasets": [
          "Penn Treebank"
        ],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Link Grammar"
          ],
          "connections": [
            "Dependency Relations"
          ],
          "mechanisms": [
            "Constraint-based Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Rule-based"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Syntactic Analysis"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Gelernter1959_GeometryMachine",
        "entity_type": "Algorithm",
        "name": "Geometry Machine",
        "year": 1959,
        "authors": [
          "Gelernter"
        ],
        "task": [
          "Geometry Theorem Proving"
        ],
        "methodology": {
          "training_strategy": [
            "Backward Chaining Search Strategy"
          ],
          "parameter_tuning": [
            "Heuristic Knowledge"
          ]
        },
        "feature_processing": [
          "Diagram Analysis"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "CeruttiDavis1969_FORMAC",
        "entity_type": "Algorithm",
        "name": "FORMAC",
        "year": 1969,
        "authors": [
          "Cerutti",
          "Davis"
        ],
        "task": [
          "Elementary Analytic Geometry Theorem Proving"
        ],
        "methodology": {
          "training_strategy": [
            "Symbolic Manipulation"
          ],
          "parameter_tuning": [
            "Descartes' Method"
          ]
        }
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "HighSchoolTextbooks_1959",
        "entity_type": "Dataset",
        "name": "High School Textbooks",
        "year": 1959,
        "domain": "Education",
        "description": "Collection of high school geometry textbooks used for theorem proving."
      }
    },
    {
      "metric_entity": {
        "metric_id": "Efficiency_SearchSpace",
        "entity_type": "Metric",
        "name": "Efficiency",
        "category": "Search Space Evaluation",
        "description": "Measure of the efficiency of search strategies in geometry theorem proving."
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ferguson1999_GeoRep",
        "entity_type": "Algorithm",
        "name": "GeoRep",
        "title": "GeoRep: A Flexible Tool for Spatial Representation of Line Drawings",
        "year": 1999,
        "authors": [
          "Ronald W. Ferguson",
          "Kenneth D. Forbus"
        ],
        "task": [
          "Spatial Representation",
          "Diagrammatic Reasoning"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Low-Level Relational Describer (LLRD)",
            "High-Level Relational Describer (HLRD)"
          ],
          "connections": [
            "Visual Operations Library",
            "Domain-Specific Rules"
          ],
          "mechanisms": [
            "Proximity Detection",
            "Reference Frame Relations",
            "Parallel Lines Detection",
            "Connectivity Analysis",
            "Boundary Description",
            "Interval Relations",
            "Grouping"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Proximity Calculation",
          "Primitive Shape Detection",
          "Visual Relation Detection"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ferguson1994_MAGI",
        "entity_type": "Algorithm",
        "name": "MAGI",
        "title": "MAGI: Analogy-based encoding using symmetry and regularity",
        "year": 1994,
        "authors": [
          "Ronald W. Ferguson"
        ],
        "task": [
          "Symmetry Detection",
          "Analogical Encoding"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Symmetry Detector",
            "Regularity Analyzer"
          ],
          "connections": [
            "Visual Relation Input"
          ],
          "mechanisms": [
            "Symmetry Judgment",
            "Regularity Recognition"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Shape Analysis",
          "Boundary Concavity Detection"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ferguson1995_JUXTA",
        "entity_type": "Algorithm",
        "name": "JUXTA",
        "title": "Understanding illustrations of physical laws by integrating differences in visual and textual representations",
        "year": 1995,
        "authors": [
          "Ronald W. Ferguson",
          "Kenneth D. Forbus"
        ],
        "task": [
          "Diagram Critiquing",
          "Physical Phenomena Understanding"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Visual Level Representation",
            "Physical Level Representation",
            "Process Level Representation"
          ],
          "connections": [
            "Visual Operations Library",
            "Domain-Specific Rules"
          ],
          "mechanisms": [
            "Difference Detection",
            "Caption Interpretation",
            "Process Inference"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Visual Difference Analysis",
          "Caption Parsing"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Forbus1980_MD_PV",
        "entity_type": "Algorithm",
        "name": "MD/PV Model",
        "title": "Spatial and qualitative aspects of reasoning about motion",
        "year": 1980,
        "authors": [
          "Kenneth D. Forbus"
        ],
        "task": [
          "Spatial Reasoning",
          "Qualitative Physics"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Metric Diagram",
            "Place Vocabulary"
          ],
          "connections": [
            "Query Mechanism"
          ],
          "mechanisms": [
            "Quantitative Information Retrieval",
            "Qualitative Spatial Representation"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Spatial Relation Extraction",
          "Conceptual Content Linking"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Pennington2014_GloVe",
        "entity_type": "Algorithm",
        "name": "GloVe",
        "title": "GloVe: Global Vectors for Word Representation",
        "year": 2014,
        "authors": [
          "Jeffrey Pennington",
          "Richard Socher",
          "Christopher D. Manning"
        ],
        "task": [
          "Word Representation",
          "Word Analogy",
          "Word Similarity",
          "Named Entity Recognition"
        ],
        "datasets": [
          "Wikipedia 2010",
          "Wikipedia 2014",
          "Gigaword 5",
          "Gigaword 5 + Wikipedia 2014",
          "Common Crawl"
        ],
        "metrics": [
          "Accuracy",
          "Spearman Rank Correlation"
        ],
        "architecture": {
          "components": [
            "Global Log-Bilinear Regression Model",
            "Weighted Least Squares Model"
          ],
          "connections": [
            "Dot Product",
            "Bias Terms"
          ],
          "mechanisms": [
            "Co-occurrence Matrix Factorization",
            "Weighting Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "AdaGrad",
            "Stochastic Sampling"
          ],
          "parameter_tuning": [
            "xmax=100",
            "α=3/4",
            "Initial Learning Rate=0.05",
            "50 Iterations for <300 Dimensions",
            "100 Iterations for ≥300 Dimensions"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Lowercasing",
          "Context Window Construction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Wikipedia_2010",
        "entity_type": "Dataset",
        "name": "Wikipedia 2010",
        "description": "A dump of Wikipedia with 1 billion tokens",
        "domain": "Natural Language Processing",
        "size": 1000000000,
        "year": 2010,
        "creators": [
          "Stanford University"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Wikipedia_2014",
        "entity_type": "Dataset",
        "name": "Wikipedia 2014",
        "description": "A dump of Wikipedia with 1.6 billion tokens",
        "domain": "Natural Language Processing",
        "size": 1600000000,
        "year": 2014,
        "creators": [
          "Stanford University"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Gigaword_5",
        "entity_type": "Dataset",
        "name": "Gigaword 5",
        "description": "A dataset with 4.3 billion tokens",
        "domain": "Natural Language Processing",
        "size": 4300000000,
        "year": 2014,
        "creators": [
          "Various Contributors"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Gigaword_5_Wikipedia_2014",
        "entity_type": "Dataset",
        "name": "Gigaword 5 + Wikipedia 2014",
        "description": "Combined dataset with 6 billion tokens",
        "domain": "Natural Language Processing",
        "size": 6000000000,
        "year": 2014,
        "creators": [
          "Stanford University"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Common_Crawl",
        "entity_type": "Dataset",
        "name": "Common Crawl",
        "description": "Web data with 42 billion tokens",
        "domain": "Natural Language Processing",
        "size": 42000000000,
        "year": 2014,
        "creators": [
          "Common Crawl Foundation"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_WordAnalogy",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Percentage of correctly answered word analogy questions",
        "category": "Word Analogy Task",
        "formula": "Correct Answers / Total Questions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "SpearmanRankCorrelation_WordSimilarity",
        "entity_type": "Metric",
        "name": "Spearman Rank Correlation",
        "description": "Correlation between predicted and human-rated word similarities",
        "category": "Word Similarity Task",
        "formula": "Spearman's Rank Correlation Coefficient"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_NER",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall for Named Entity Recognition",
        "category": "Named Entity Recognition",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kushman2014_TemplateBasedStatisticalLearning",
        "entity_type": "Algorithm",
        "name": "KAZB",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": 2014,
        "authors": [
          "Kushman",
          "Artzi",
          "Zettlemoyer",
          "Barzilay"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Alg514",
          "Dolphin18K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Template Matching",
            "Equation Template"
          ],
          "connections": [
            "Problem Sentence Mapping"
          ],
          "mechanisms": [
            "Reasoning Across Sentences"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Validation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Lexical Similarity Calculation"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Zhou2015_ImprovedTemplateBasedStatisticalLearning",
        "entity_type": "Algorithm",
        "name": "ZDC",
        "title": "Learn to Solve Algebra Word Problems Using Quadratic Programming",
        "year": 2015,
        "authors": [
          "Zhou",
          "Dai",
          "Chen"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Alg514",
          "Dolphin18K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Template Matching",
            "Equation Template"
          ],
          "connections": [
            "Problem Sentence Mapping"
          ],
          "mechanisms": [
            "Quadratic Programming"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Validation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Lexical Similarity Calculation"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Huang2016_SimpleSimilarityBasedMethod",
        "entity_type": "Algorithm",
        "name": "SIM",
        "title": "How Well Do Computers Solve Math Word Problems?",
        "year": 2016,
        "authors": [
          "Huang",
          "Shi",
          "Lin",
          "Yin",
          "Ma"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Alg514",
          "SingleEQ",
          "Dolphin18K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Lexical Similarity Calculation"
          ],
          "connections": [
            "Template Selection",
            "Template Slot Filling"
          ],
          "mechanisms": [
            "Weighted Jaccard Similarity"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Validation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "TF-IDF Scoring",
          "Word-Level Edit-Distance"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Dolphin1878_2015",
        "entity_type": "Dataset",
        "name": "Dolphin1878",
        "description": "A dataset of 1,878 number word problems",
        "domain": "Mathematics",
        "size": 1878,
        "year": 2015,
        "creators": [
          "Shi",
          "Wang",
          "Lin",
          "Liu",
          "Rui"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Dolphin18K_2016",
        "entity_type": "Dataset",
        "name": "Dolphin18K",
        "description": "A large-scale dataset of 18,460 annotated math word problems",
        "domain": "Mathematics",
        "size": 18460,
        "year": 2016,
        "creators": [
          "Huang",
          "Shi",
          "Lin",
          "Yin",
          "Ma"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SingleEQ_2015",
        "entity_type": "Dataset",
        "name": "SingleEQ",
        "description": "A dataset of 508 problems, each corresponding to one single equation",
        "domain": "Mathematics",
        "size": 508,
        "year": 2015,
        "creators": [
          "Koncel-Kedziorski",
          "Hajishirzi",
          "Sabharwal",
          "Etzioni",
          "Ang"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ImageNet_2009",
        "entity_type": "Dataset",
        "name": "ImageNet",
        "description": "一个大规模层次化的图像数据库，基于WordNet结构构建，旨在为每个同义词集合提供500-1000张高质量、全分辨率图像。",
        "domain": "计算机视觉",
        "size": 3200000,
        "year": 2009,
        "creators": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Precision_Classification",
        "entity_type": "Metric",
        "name": "Precision",
        "description": "精确率",
        "category": "分类评估",
        "formula": "真正例/(真正例 + 假正例)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall_Classification",
        "entity_type": "Metric",
        "name": "Recall",
        "description": "召回率",
        "category": "分类评估",
        "formula": "真正例/(真正例 + 假负例)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "AUC_ROC_Classification",
        "entity_type": "Metric",
        "name": "AUC",
        "description": "ROC曲线下面积",
        "category": "分类评估",
        "formula": "ROC曲线下的面积"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Deng2009_TreeMaxClassifier",
        "entity_type": "Algorithm",
        "name": "Tree-Max Classifier",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "authors": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": [
          "Object Classification"
        ],
        "datasets": [
          "ImageNet"
        ],
        "metrics": [
          "AUC"
        ],
        "architecture": {
          "components": [
            "Synset Node Classifiers"
          ],
          "connections": [
            "Hierarchical Structure"
          ],
          "mechanisms": [
            "AdaBoost-based Classifier"
          ]
        },
        "methodology": {
          "training_strategy": [
            "AdaBoost"
          ],
          "parameter_tuning": [
            "None mentioned"
          ]
        },
        "feature_processing": [
          "None mentioned"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Deng2009_NBNN",
        "entity_type": "Algorithm",
        "name": "NBNN",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "authors": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": [
          "Object Recognition"
        ],
        "datasets": [
          "ImageNet",
          "Caltech256"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Bag-of-Features Representation"
          ],
          "connections": [
            "SIFT Descriptors"
          ],
          "mechanisms": [
            "Naive Bayesian Nearest Neighbor"
          ]
        },
        "methodology": {
          "training_strategy": [
            "None mentioned"
          ],
          "parameter_tuning": [
            "None mentioned"
          ]
        },
        "feature_processing": [
          "SIFT Descriptors"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Deng2009_NN_Voting",
        "entity_type": "Algorithm",
        "name": "NN-Voting",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "authors": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": [
          "Object Recognition"
        ],
        "datasets": [
          "ImageNet",
          "Caltech256"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Nearest Neighbor"
          ],
          "connections": [
            "Pixel Distance"
          ],
          "mechanisms": [
            "Voting Scheme"
          ]
        },
        "methodology": {
          "training_strategy": [
            "None mentioned"
          ],
          "parameter_tuning": [
            "None mentioned"
          ]
        },
        "feature_processing": [
          "Pixel Distance"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Caltech101_2004",
        "entity_type": "Dataset",
        "name": "Caltech101",
        "description": "一个包含101个类别的小规模图像数据集，每个类别大约有40到800张图像。",
        "domain": "计算机视觉",
        "size": 9146,
        "year": 2004,
        "creators": [
          "Fei-Fei, L.",
          "Fergus, R.",
          "Perona, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Caltech256_2007",
        "entity_type": "Dataset",
        "name": "Caltech256",
        "description": "一个包含256个类别的图像数据集，每个类别大约有80张图像。",
        "domain": "计算机视觉",
        "size": 30607,
        "year": 2007,
        "creators": [
          "Griffin, G.",
          "Holub, A.",
          "Perona, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "TinyImage_2008",
        "entity_type": "Dataset",
        "name": "TinyImage",
        "description": "一个包含8000万张32x32低分辨率图像的数据集，通过将WordNet中的所有单词作为查询发送到图像搜索引擎收集。",
        "domain": "计算机视觉",
        "size": 80000000,
        "year": 2008,
        "creators": [
          "Torralba, A.",
          "Fergus, R.",
          "Freeman, W."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ESP_Dataset_2004",
        "entity_type": "Dataset",
        "name": "ESP Dataset",
        "description": "通过在线游戏ESP Game收集的图像数据集，玩家独立为同一张图片提出标签，目标是在限定时间内匹配尽可能多的词汇。",
        "domain": "计算机视觉",
        "size": 60000,
        "year": 2004,
        "creators": [
          "von Ahn, L.",
          "Dabbish, L."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "LabelMe_2008",
        "entity_type": "Dataset",
        "name": "LabelMe",
        "description": "一个包含30,000张标注和分割的图像数据集。",
        "domain": "计算机视觉",
        "size": 30000,
        "year": 2008,
        "creators": [
          "Russell, B.",
          "Torralba, A.",
          "Murphy, K.",
          "Freeman, W."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Lotus_Hill_2007",
        "entity_type": "Dataset",
        "name": "Lotus Hill",
        "description": "一个包含50,000张标注和分割的图像数据集，以及587,000帧视频。",
        "domain": "计算机视觉",
        "size": 50000,
        "year": 2007,
        "creators": [
          "Yao, B.",
          "Yang, X.",
          "Zhu, S."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hajishirzi2013_NECO",
        "entity_type": "Algorithm",
        "name": "NECO",
        "title": "Joint Coreference Resolution and Named-Entity Linking with Multi-pass Sieves",
        "year": 2013,
        "authors": [
          "Hannaneh Hajishirzi",
          "Leila Zilles",
          "Daniel S. Weld",
          "Luke Zettlemoyer"
        ],
        "task": [
          "Coreference Resolution",
          "Named-Entity Linking"
        ],
        "datasets": [
          "ACE 2004 Newswire",
          "CoNLL 2011"
        ],
        "metrics": [
          "MUC",
          "B3",
          "Pairwise",
          "F1"
        ],
        "architecture": {
          "components": [
            "Stanford Sieve Model",
            "NEL-informed sieves",
            "mention detection",
            "mention attributes"
          ],
          "connections": [
            "coreference clusters",
            "NEL constraints"
          ],
          "mechanisms": [
            "mention merging",
            "entity link propagation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "deterministic rules",
            "multi-pass sieves"
          ],
          "parameter_tuning": [
            "confidence thresholds for NEL systems"
          ]
        },
        "feature_processing": [
          "mention detection",
          "entity linking",
          "attribute extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004_NWIRE_2004",
        "entity_type": "Dataset",
        "name": "ACE 2004 Newswire",
        "description": "Newswire subset of the ACE 2004 corpus",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": 2004,
        "creators": [
          "NIST"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CONLL2011_2011",
        "entity_type": "Dataset",
        "name": "CoNLL 2011",
        "description": "Coreference dataset from five different domains",
        "domain": "Natural Language Processing",
        "size": 625,
        "year": 2011,
        "creators": [
          "Pradhan et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Pairwise_Coreference",
        "entity_type": "Metric",
        "name": "Pairwise",
        "description": "Measures the accuracy of pairwise coreference decisions",
        "category": "Coreference Evaluation",
        "formula": "Correct pairwise coreference decisions / Total pairwise decisions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_NamedEntityLinking",
        "entity_type": "Metric",
        "name": "F1",
        "description": "Harmonic mean of precision and recall",
        "category": "Named-Entity Linking Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Zhou2015_QuadraticProgrammingSolver",
        "entity_type": "Algorithm",
        "name": "Quadratic Programming Solver",
        "title": "Learn to Solve Algebra Word Problems Using Quadratic Programming",
        "year": 2015,
        "authors": [
          "Lipu Zhou",
          "Shuaixiang Dai",
          "Liwei Chen"
        ],
        "task": [
          "Algebra Word Problem Solving"
        ],
        "datasets": [
          "Kushman2014_Dataset"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Log-linear Model",
            "Quadratic Programming"
          ],
          "connections": [
            "Max-margin Objective"
          ],
          "mechanisms": [
            "Constraint Generation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-margin Objective",
            "Quadratic Programming"
          ],
          "parameter_tuning": [
            "C Parameter"
          ]
        },
        "feature_processing": [
          "Single Slot Features",
          "Slot Pair Features",
          "Solution Features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Kushman2014_Dataset_2014",
        "entity_type": "Dataset",
        "name": "Kushman2014_Dataset",
        "description": "Benchmark dataset for algebra word problems",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Nate Kushman",
          "Yoav Artzi",
          "Luke Zettlemoyer",
          "Regina Barzilay"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goldwasser2014_CombinedFeedbackPerceptron",
        "entity_type": "Algorithm",
        "name": "Combined Feedback Perceptron",
        "title": "Learning from natural instructions",
        "year": 2014,
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": [
          "Natural Language Instruction Interpretation"
        ],
        "datasets": [
          "Solitaire Card Game",
          "Geoquery"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Perceptron",
            "Structured Perceptron"
          ],
          "connections": [
            "Feature Function",
            "Weight Vector"
          ],
          "mechanisms": [
            "Binary Update",
            "Structural Update"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Online Learning",
            "Error-Driven Updates"
          ],
          "parameter_tuning": [
            "Weight Vector Adjustment"
          ]
        },
        "feature_processing": [
          "Lexical Mapping",
          "Syntactic Information"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goldwasser2014_StructuredPredictor",
        "entity_type": "Algorithm",
        "name": "Structured Predictor",
        "title": "Learning from natural instructions",
        "year": 2014,
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": [
          "Semantic Parsing"
        ],
        "datasets": [
          "Solitaire Card Game",
          "Geoquery"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Feature Function",
            "Weight Vector"
          ],
          "connections": [
            "Logical Interpretation"
          ],
          "mechanisms": [
            "Integer Linear Programming"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Constrained Optimization"
          ],
          "parameter_tuning": [
            "Weight Vector Adjustment"
          ]
        },
        "feature_processing": [
          "Lexical Mapping",
          "Syntactic Information"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Geoquery_2014",
        "entity_type": "Dataset",
        "name": "Geoquery",
        "description": "A dataset of geographical queries and their corresponding logical forms",
        "domain": "Natural Language Processing",
        "size": 500,
        "year": 2014,
        "creators": [
          "Zelle, J.",
          "Mooney, R."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SolitaireCardGame_2014",
        "entity_type": "Dataset",
        "name": "Solitaire Card Game",
        "description": "A dataset of solitaire card game rules and their corresponding logical forms",
        "domain": "Natural Language Processing",
        "size": "Not specified",
        "year": 2014,
        "creators": [
          "Goldwasser, D.",
          "Roth, D."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Cho2014_RNNEncoderDecoder",
        "entity_type": "Algorithm",
        "name": "RNN Encoder–Decoder",
        "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "year": 2014,
        "authors": [
          "Kyunghyun Cho",
          "Bart van Merriënboer",
          "Caglar Gulcehre",
          "Dzmitry Bahdanau",
          "Fethi Bougares",
          "Holger Schwenk",
          "Yoshua Bengio"
        ],
        "task": [
          "Statistical Machine Translation"
        ],
        "datasets": [
          "Europarl",
          "News Commentary",
          "UN",
          "Crawled Corpora"
        ],
        "metrics": [
          "BLEU"
        ],
        "architecture": {
          "components": [
            "Encoder RNN",
            "Decoder RNN"
          ],
          "connections": [
            "Conditional Probability"
          ],
          "mechanisms": [
            "Reset Gate",
            "Update Gate",
            "Hidden Unit"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-based Algorithm",
            "Joint Training"
          ],
          "parameter_tuning": [
            "Adadelta",
            "Stochastic Gradient Descent"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Phrase Pair Scoring"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Europarl_2014",
        "entity_type": "Dataset",
        "name": "Europarl",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 61000000,
        "year": 2014,
        "creators": [
          "Philipp Koehn"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NewsCommentary_2014",
        "entity_type": "Dataset",
        "name": "News Commentary",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 5500000,
        "year": 2014,
        "creators": [
          "Philipp Koehn"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "UN_2014",
        "entity_type": "Dataset",
        "name": "UN",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 421000000,
        "year": 2014,
        "creators": [
          "Philipp Koehn"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CrawledCorpora_2014",
        "entity_type": "Dataset",
        "name": "Crawled Corpora",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 870000000,
        "year": 2014,
        "creators": [
          "Philipp Koehn"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "BLEU_Translation",
        "entity_type": "Metric",
        "name": "BLEU",
        "description": "Bilingual Evaluation Understudy",
        "category": "Machine Translation Evaluation",
        "formula": "Exponential averaging of modified n-gram precision"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kushman2014_EquationSystemSolver",
        "entity_type": "Algorithm",
        "name": "Equation System Solver",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": 2014,
        "authors": [
          "Nate Kushman",
          "Yoav Artzi",
          "Luke Zettlemoyer",
          "Regina Barzilay"
        ],
        "task": [
          "Algebra Word Problem Solving"
        ],
        "datasets": [
          "Algebra.com Dataset"
        ],
        "metrics": [
          "Equation Accuracy",
          "Answer Accuracy"
        ],
        "architecture": {
          "components": [
            "Template Selection",
            "Slot Filling",
            "Alignment"
          ],
          "connections": [
            "Log-linear Distribution",
            "Beam Search"
          ],
          "mechanisms": [
            "Canonicalization",
            "Dependency Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Semi-supervised Learning"
          ],
          "parameter_tuning": [
            "L-BFGS",
            "L2 Regularization"
          ]
        },
        "feature_processing": [
          "Part-of-Speech Tagging",
          "Lematization",
          "Dependency Parsing"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Algebra_com_Dataset_2014",
        "entity_type": "Dataset",
        "name": "Algebra.com Dataset",
        "description": "A dataset of algebra word problems collected from Algebra.com",
        "domain": "Mathematics",
        "size": 514,
        "year": 2014,
        "creators": [
          "Nate Kushman",
          "Yoav Artzi",
          "Luke Zettlemoyer",
          "Regina Barzilay"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Equation_Accuracy_Algebra",
        "entity_type": "Metric",
        "name": "Equation Accuracy",
        "description": "Measures how often the system generates the correct equation system",
        "category": "Algebra Problem Solving",
        "formula": "Number of correct equation systems / Total number of equation systems"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Answer_Accuracy_Algebra",
        "entity_type": "Metric",
        "name": "Answer Accuracy",
        "description": "Evaluates how often the generated numerical answer is correct",
        "category": "Algebra Problem Solving",
        "formula": "Number of correct answers / Total number of answers"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hosseini2014_ARIS",
        "entity_type": "Algorithm",
        "name": "ARIS",
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "year": 2014,
        "authors": [
          "Mohammad Javad Hosseini",
          "Hannaneh Hajishirzi",
          "Oren Etzioni",
          "Nate Kushman"
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "MA1_2014",
          "IXL_2014",
          "MA2_2014"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Entity Recognition",
            "Container Identification",
            "Verb Categorization",
            "State Transition Modeling"
          ],
          "connections": [
            "Dependency Parsing",
            "Coreference Resolution"
          ],
          "mechanisms": [
            "Support Vector Machines",
            "Circumscription Assumption"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-validation",
            "Feature Extraction"
          ],
          "parameter_tuning": [
            "Regularization",
            "Similarity-based Feature Selection"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Coreference Resolution",
          "Named Entity Recognition"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mitra2016_FormulaBasedSolver",
        "entity_type": "Algorithm",
        "name": "Formula-Based Solver",
        "title": "Learning To Use Formulas To Solve Simple Arithmetic Problems",
        "year": 2016,
        "authors": [
          "Arindam Mitra",
          "Chitta Baral"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "AddSub_2014"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Part Whole",
            "Change",
            "Comparison"
          ],
          "connections": [
            "Equation Generation"
          ],
          "mechanisms": [
            "Log-linear Model",
            "Feature Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Parameter Vector θ"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Machine Readable Dictionaries",
          "Semantic Relations Extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AddSub_2014",
        "entity_type": "Dataset",
        "name": "AddSub",
        "description": "A dataset of simple addition-subtraction arithmetic problems for third, fourth, and fifth graders.",
        "domain": "Natural Language Processing",
        "size": 395,
        "year": 2014,
        "creators": [
          "Mohammad Javad Hosseini",
          "Hannaneh Hajishirzi",
          "Oren Etzioni",
          "Nate Kushman"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hochreiter1997_LongShortTermMemory",
        "entity_type": "Algorithm",
        "name": "Long Short-Term Memory (LSTM)",
        "title": "Long Short-Term Memory",
        "year": 1997,
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": [
          "Sequence Modeling",
          "Long Time Lag Problems"
        ],
        "datasets": [],
        "metrics": [
          "Success Rate",
          "Training Time"
        ],
        "architecture": {
          "components": [
            "Memory Cells",
            "Input Gates",
            "Output Gates",
            "Constant Error Carousels (CECs)"
          ],
          "connections": [
            "Fully Connected Hidden Layer",
            "Self-Connections"
          ],
          "mechanisms": [
            "Multiplicative Gates",
            "Truncated Backpropagation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning",
            "Real-Time Recurrent Learning (RTRL)",
            "Back-Propagation Through Time (BPTT)"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Bias Initialization"
          ]
        },
        "feature_processing": [
          "Local Input/Output Representations",
          "Distributed Representations"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hochreiter1991_ConstantErrorBackprop",
        "entity_type": "Algorithm",
        "name": "Constant Error Backpropagation",
        "title": "Long Short-Term Memory",
        "year": 1991,
        "authors": [
          "Sepp Hochreiter"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Single Unit",
            "Constant Error Carousels (CECs)"
          ],
          "connections": [
            "Self-Connections"
          ],
          "mechanisms": [
            "Linear Activation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mozer1992_FocusedRecurrentBackprop",
        "entity_type": "Algorithm",
        "name": "Focused Recurrent Backpropagation",
        "title": "Long Short-Term Memory",
        "year": 1992,
        "authors": [
          "Michael C. Mozer"
        ],
        "task": [
          "Temporal Sequence Recognition"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Adaptive Thresholds"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Elman1988_RecurrentCascadeCorrelation",
        "entity_type": "Algorithm",
        "name": "Recurrent Cascade-Correlation",
        "title": "Long Short-Term Memory",
        "year": 1988,
        "authors": [
          "Jeffrey L. Elman"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Hidden Units",
            "Gate Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": [
            "Incremental Network Growth"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Schmidhuber1992_HierarchicalChunker",
        "entity_type": "Algorithm",
        "name": "Hierarchical Chunker",
        "title": "Long Short-Term Memory",
        "year": 1992,
        "authors": [
          "Jürgen Schmidhuber"
        ],
        "task": [
          "Grammar Learning"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Hierarchical Recurrent Nets"
          ],
          "connections": [
            "Recurrent Connections"
          ],
          "mechanisms": [
            "Chunking"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bengio1994_Backpropagation",
        "entity_type": "Algorithm",
        "name": "Backpropagation",
        "title": "Long Short-Term Memory",
        "year": 1994,
        "authors": [
          "Yoshua Bengio",
          "Patrice Simard",
          "Pascal Frasconi"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Robinson1987_UtilityDrivenDynamicErrorPropagationNetwork",
        "entity_type": "Algorithm",
        "name": "Utility-Driven Dynamic Error Propagation Network",
        "title": "Long Short-Term Memory",
        "year": 1987,
        "authors": [
          "Anthony J. Robinson",
          "Firoz Fallside"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Williams1992_BackPropagationThroughTime",
        "entity_type": "Algorithm",
        "name": "Back-Propagation Through Time (BPTT)",
        "title": "Long Short-Term Memory",
        "year": 1992,
        "authors": [
          "Ronald J. Williams",
          "David Zipser"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Pineda1987_RealTimeRecurrentLearning",
        "entity_type": "Algorithm",
        "name": "Real-Time Recurrent Learning (RTRL)",
        "title": "Long Short-Term Memory",
        "year": 1987,
        "authors": [
          "Frank J. Pineda"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Fahlman1991_RecurrentCascadeCorrelation",
        "entity_type": "Algorithm",
        "name": "Recurrent Cascade-Correlation",
        "title": "Long Short-Term Memory",
        "year": 1991,
        "authors": [
          "Scott E. Fahlman"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Hidden Units",
            "Gate Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": [
            "Incremental Network Growth"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Pearlmutter1989_LearningStateSpaceTrajectories",
        "entity_type": "Algorithm",
        "name": "Learning State Space Trajectories",
        "title": "Long Short-Term Memory",
        "year": 1989,
        "authors": [
          "Barak A. Pearlmutter"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Puskorius1994_KalmanFilterTrainedRecurrentNetworks",
        "entity_type": "Algorithm",
        "name": "Kalman Filter Trained Recurrent Networks",
        "title": "Long Short-Term Memory",
        "year": 1994,
        "authors": [
          "Gail V. Puskorius",
          "Larry A. Feldkamp"
        ],
        "task": [
          "Nonlinear Dynamical Systems Control"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Kalman Filter"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ring1993_IncrementalHigherOrderAddition",
        "entity_type": "Algorithm",
        "name": "Incremental Higher Order Addition",
        "title": "Long Short-Term Memory",
        "year": 1993,
        "authors": [
          "Mark B. Ring"
        ],
        "task": [
          "Sequence Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Higher Order Units"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Watrous1992_SecondOrderRecurrentNetworks",
        "entity_type": "Algorithm",
        "name": "Second Order Recurrent Networks",
        "title": "Long Short-Term Memory",
        "year": 1992,
        "authors": [
          "Robert L. Watrous",
          "Gregory M. Kuhn"
        ],
        "task": [
          "Finite-State Language Induction"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Recurrent Units"
          ],
          "connections": [
            "Feedback Connections"
          ],
          "mechanisms": [
            "Second-Order Sigma-Pi Units"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "EmbeddedReberGrammar_1989",
        "entity_type": "Dataset",
        "name": "Embedded Reber Grammar",
        "description": "A synthetic dataset used for evaluating sequence modeling algorithms.",
        "domain": "Sequence Modeling",
        "size": null,
        "year": 1989,
        "creators": [
          "Smith",
          "Zipser"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "SuccessRate_Classification",
        "entity_type": "Metric",
        "name": "Success Rate",
        "description": "Percentage of successful trials where the model correctly predicts the sequence.",
        "category": "Classification Evaluation",
        "formula": "Number of successful trials / Total number of trials"
      }
    },
    {
      "metric_entity": {
        "metric_id": "TrainingTime_Performance",
        "entity_type": "Metric",
        "name": "Training Time",
        "description": "Time required to train the model to convergence.",
        "category": "Performance Evaluation",
        "formula": "Total time taken for training"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Antol2015_VQA",
        "entity_type": "Algorithm",
        "name": "VQA",
        "title": "VQA: Visual Question Answering",
        "year": 2015,
        "authors": [
          "Antol",
          "Agrawal",
          "Lu",
          "Mitchell",
          "Batra",
          "Zitnick",
          "Parikh"
        ],
        "task": "Visual Question Answering",
        "datasets": [
          "VQA_2015"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "CNN",
            "LSTM"
          ],
          "connections": [
            "Point-wise Multiplication"
          ],
          "mechanisms": [
            "Multi-layer Perceptron Classifier"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss"
          ],
          "parameter_tuning": [
            "Adam"
          ]
        },
        "feature_processing": [
          "Image Embedding",
          "Question Embedding"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goyal2016_BalancedVQA",
        "entity_type": "Algorithm",
        "name": "Balanced VQA",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "year": 2016,
        "authors": [
          "Goyal",
          "Khot",
          "Summers-Stay",
          "Batra",
          "Parikh"
        ],
        "task": "Visual Question Answering",
        "datasets": [
          "VQA_v2.0_2016"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "CNN",
            "LSTM",
            "Attention Mechanism"
          ],
          "connections": [
            "Point-wise Multiplication"
          ],
          "mechanisms": [
            "Hierarchical Co-attention",
            "Multimodal Compact Bilinear Pooling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Pairwise Hinge Ranking Loss"
          ],
          "parameter_tuning": [
            "Adam",
            "Normalization"
          ]
        },
        "feature_processing": [
          "Image Embedding",
          "Question Embedding",
          "Counter-example Retrieval"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "VQA_2015",
        "entity_type": "Dataset",
        "name": "VQA",
        "description": "Visual Question Answering dataset with 204K images, 614K questions, and 6M answers",
        "domain": "Computer Vision and Natural Language Processing",
        "size": 614000,
        "year": 2015,
        "creators": [
          "Antol",
          "Agrawal",
          "Lu",
          "Mitchell",
          "Batra",
          "Zitnick",
          "Parikh"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "VQA_v2.0_2016",
        "entity_type": "Dataset",
        "name": "VQA v2.0",
        "description": "Balanced Visual Question Answering dataset with 1.1M image-question pairs and 13M answers",
        "domain": "Computer Vision and Natural Language Processing",
        "size": 1100000,
        "year": 2016,
        "creators": [
          "Goyal",
          "Khot",
          "Summers-Stay",
          "Batra",
          "Parikh"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall@5",
        "entity_type": "Metric",
        "name": "Recall@5",
        "description": "Proportion of times the human-picked counter-example is among the top-5 ranked images",
        "category": "Ranking Evaluation",
        "formula": "Number of times human-picked image is in top-5 / Total queries"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2018_KNOWLEDGE",
        "entity_type": "Algorithm",
        "name": "KNOWLEDGE",
        "title": "Mapping to Declarative Knowledge for Word Problem Solving",
        "year": 2018,
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "AllArith",
          "AllArithLex",
          "AllArithTmpl",
          "Perturb",
          "Aggregate"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Transfer",
            "Dimensional Analysis",
            "Part-Whole Relation",
            "Explicit Math"
          ],
          "connections": [
            "Concept Selection",
            "Declarative Rule Selection"
          ],
          "mechanisms": [
            "Latent Variable Modeling",
            "Beam Search"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Latent Structured SVM",
            "Two Stage Learning"
          ],
          "parameter_tuning": [
            "Regularization Parameter C"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Coreference Resolution",
          "Verb Classification",
          "Rate Component Detection"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArith_2018",
        "entity_type": "Dataset",
        "name": "AllArith",
        "description": "A dataset of arithmetic word problems",
        "domain": "Mathematics",
        "size": 831,
        "year": 2018,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArithLex_2018",
        "entity_type": "Dataset",
        "name": "AllArithLex",
        "description": "A subset of AllArith to test robustness to new vocabulary",
        "domain": "Mathematics",
        "size": 831,
        "year": 2018,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArithTmpl_2018",
        "entity_type": "Dataset",
        "name": "AllArithTmpl",
        "description": "A subset of AllArith to test robustness to new equation forms",
        "domain": "Mathematics",
        "size": 831,
        "year": 2018,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Perturb_2018",
        "entity_type": "Dataset",
        "name": "Perturb",
        "description": "A dataset created by perturbing AllArith problems to minimize bias",
        "domain": "Mathematics",
        "size": 661,
        "year": 2018,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Aggregate_2018",
        "entity_type": "Dataset",
        "name": "Aggregate",
        "description": "A dataset combining AllArith and Perturb",
        "domain": "Mathematics",
        "size": 1492,
        "year": 2018,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_MathDQN",
        "entity_type": "Algorithm",
        "name": "MathDQN",
        "title": "MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "AI2",
          "IL",
          "CC",
          "ArithS",
          "ArithM"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Deep Q-Network",
            "Feed-forward Neural Network"
          ],
          "connections": [
            "Two-layer Feed-forward Neural Network"
          ],
          "mechanisms": [
            "Reinforcement Learning",
            "Q-learning",
            "Experience Replay"
          ]
        },
        "methodology": {
          "training_strategy": [
            "ε-greedy Strategy",
            "Mini-batch Gradient Descent"
          ],
          "parameter_tuning": [
            "Learning Rate = 0.0001",
            "Discount Factor γ = 0.9",
            "Replay Memory Size = 15,000"
          ]
        },
        "feature_processing": [
          "Quantity Schema Extraction",
          "Verb Categorization",
          "Rate Unit Detection",
          "Context Feature Extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AI2_2014",
        "entity_type": "Dataset",
        "name": "AI2",
        "description": "Single-step and multi-step arithmetic word problems involving only addition and subtraction",
        "domain": "Arithmetic Word Problems",
        "size": 395,
        "year": 2014,
        "creators": [
          "Hosseini et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IL_2015",
        "entity_type": "Dataset",
        "name": "IL",
        "description": "Single-step word problems with one operator, including addition, subtraction, multiplication, and division",
        "domain": "Arithmetic Word Problems",
        "size": 562,
        "year": 2015,
        "creators": [
          "Roy, Vieira, and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CC_2015",
        "entity_type": "Dataset",
        "name": "CC",
        "description": "Multi-step problems without irrelevant quantities, involving combinations of four types of operators",
        "domain": "Arithmetic Word Problems",
        "size": 600,
        "year": 2015,
        "creators": [
          "Roy and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ArithS_2018",
        "entity_type": "Dataset",
        "name": "ArithS",
        "description": "Subset of single-step arithmetic problems involving only one operator",
        "domain": "Arithmetic Word Problems",
        "size": 890,
        "year": 2018,
        "creators": [
          "Wang et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ArithM_2018",
        "entity_type": "Dataset",
        "name": "ArithM",
        "description": "Collected from AI2 ∪ IL ∪ CC, multi-step arithmetic problems involving at least two operators",
        "domain": "Arithmetic Word Problems",
        "size": 667,
        "year": 2018,
        "creators": [
          "Wang et al."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_ExpTree",
        "entity_type": "Algorithm",
        "name": "ExpTree",
        "title": "Solving General Arithmetic Word Problems",
        "year": 2015,
        "authors": [
          "Roy",
          "Roth"
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "AI2",
          "IL",
          "CC"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Tree",
            "Classifiers"
          ],
          "connections": [
            "Leaf Node Classification",
            "Operator Selection"
          ],
          "mechanisms": [
            "Scoring Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Quantity Schema Extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Koncel-Kedziorski2015_ALGES",
        "entity_type": "Algorithm",
        "name": "ALGES",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": 2015,
        "authors": [
          "Koncel-Kedziorski",
          "Hajishirzi",
          "Sabharwal",
          "Etzioni",
          "Ang"
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "AI2",
          "IL",
          "CC"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Integer Linear Programming",
            "Expression Trees"
          ],
          "connections": [
            "Candidate Tree Ranking"
          ],
          "mechanisms": [
            "Scoring Function"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2017_UnitDep",
        "entity_type": "Algorithm",
        "name": "UnitDep",
        "title": "Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving",
        "year": 2017,
        "authors": [
          "Roy",
          "Roth"
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "AI2",
          "IL",
          "CC"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Unit Dependency Graph",
            "Expression Trees"
          ],
          "connections": [
            "Rate Unit Consistency"
          ],
          "mechanisms": [
            "Scoring Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Rate Unit Detection"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Luong2016_MultiTaskSeq2Seq",
        "entity_type": "Algorithm",
        "name": "Multi-task Sequence to Sequence Learning",
        "title": "MULTI-TASK SEQUENCE TO SEQUENCE LEARNING",
        "year": 2016,
        "authors": [
          "Minh-Thang Luong",
          "Quoc V. Le",
          "Ilya Sutskever",
          "Oriol Vinyals",
          "Lukasz Kaiser"
        ],
        "task": [
          "Machine Translation",
          "Constituency Parsing",
          "Image Caption Generation"
        ],
        "datasets": [
          "WMT'15 English-German",
          "Penn Tree Bank",
          "High-Confidence Corpus",
          "Image Captioning Dataset"
        ],
        "metrics": [
          "BLEU",
          "F1",
          "Perplexity"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "One-to-many",
            "Many-to-one",
            "Many-to-many"
          ],
          "mechanisms": [
            "Recurrent Neural Networks",
            "Long Short-Term Memory",
            "Gated Recurrent Unit",
            "Attention Mechanism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Alternating Training",
            "Parameter Updates Allocation"
          ],
          "parameter_tuning": [
            "SGD",
            "Learning Rate Halving"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Embeddings"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WMT15_English_German_2015",
        "entity_type": "Dataset",
        "name": "WMT'15 English-German",
        "description": "Parallel corpus for English-German translation",
        "domain": "Natural Language Processing",
        "size": 4500000,
        "year": 2015,
        "creators": [
          "Bojar et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Penn_Tree_Bank_1993",
        "entity_type": "Dataset",
        "name": "Penn Tree Bank",
        "description": "Corpus for syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 40000,
        "year": 1993,
        "creators": [
          "Marcus, Mitchell P.",
          "Marcinkiewicz, Mary Ann",
          "Santorini, Beatrice"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "High_Confidence_Corpus_2015",
        "entity_type": "Dataset",
        "name": "High-Confidence Corpus",
        "description": "Large corpus for syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 11000000,
        "year": 2015,
        "creators": [
          "Vinyals et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Image_Captioning_Dataset_2015",
        "entity_type": "Dataset",
        "name": "Image Captioning Dataset",
        "description": "Dataset of image and caption pairs",
        "domain": "Computer Vision",
        "size": 596000,
        "year": 2015,
        "creators": [
          "Vinyals et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Parsing",
        "entity_type": "Metric",
        "name": "F1",
        "description": "Evaluation metric for parsing",
        "category": "Parsing Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Perplexity_Language_Modeling",
        "entity_type": "Metric",
        "name": "Perplexity",
        "description": "Evaluation metric for language modeling",
        "category": "Language Modeling Evaluation",
        "formula": "Exp of the negative log-likelihood"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sundaram2015_WordProblemSolver",
        "entity_type": "Algorithm",
        "name": "Word Problem Solver",
        "title": "Natural Language Processing for Solving Simple Word Problems",
        "year": 2015,
        "authors": [
          "Sundaram, S. S.",
          "Khemani, D."
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "DS1_2014",
          "DS2_2014",
          "DS3_2014"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Simplification Module",
            "Analysis Module",
            "Knowledge Representation Module",
            "Temporal Schema Module"
          ],
          "connections": [
            "Dependency Parsing",
            "Temporal Ordering"
          ],
          "mechanisms": [
            "Stanford CoreNLP Suite",
            "Schema Matching",
            "Heuristics"
          ]
        },
        "methodology": {
          "training_strategy": [
            "None"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Conjunction Resolution",
          "Currency Preprocessing",
          "Co-reference Resolution",
          "Sentence Simplification",
          "Entity Resolution"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DS1_2014",
        "entity_type": "Dataset",
        "name": "DS1",
        "description": "Dataset for simple word problems",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DS2_2014",
        "entity_type": "Dataset",
        "name": "DS2",
        "description": "Dataset for word problems with irrelevant information",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DS3_2014",
        "entity_type": "Dataset",
        "name": "DS3",
        "description": "Dataset for complex word problems",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Huang2018_CASS",
        "entity_type": "Algorithm",
        "name": "CASS",
        "title": "Neural Math Word Problem Solver with Reinforcement Learning",
        "year": 2018,
        "authors": [
          "Danqing Huang",
          "Jing Liu",
          "Chin-Yew Lin",
          "Jian Yin"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Alg514",
          "NumWord",
          "Dolphin18K"
        ],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Copy Mechanism",
            "Alignment Mechanism"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "Gated Recurrent Unit",
            "Policy Gradient"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Reinforcement Learning",
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "SGD Optimizer",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Equation Normalization"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NumWord_2015",
        "entity_type": "Dataset",
        "name": "NumWord",
        "description": "Contains 2,871 number word problems",
        "domain": "Mathematics",
        "size": 2871,
        "year": 2015,
        "creators": [
          "Shi et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Solution_Accuracy",
        "entity_type": "Metric",
        "name": "Solution Accuracy",
        "description": "The proportion of correctly solved math word problems",
        "category": "Math Word Problem Solving Evaluation",
        "formula": "Correctly solved problems / Total problems"
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SINGLEEQ_2015",
        "entity_type": "Dataset",
        "name": "SINGLEEQ",
        "description": "Grade-school algebra word problems that map to single equations",
        "domain": "Natural Language Processing",
        "size": 508,
        "year": 2015,
        "creators": [
          "Koncel-Kedziorski, R.",
          "Hajishirzi, H.",
          "Sabharwal, A.",
          "Etzioni, O.",
          "Ang, S. D."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hosseini2014_VerbCategorization",
        "entity_type": "Algorithm",
        "name": "Verb Categorization",
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "year": 2014,
        "authors": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "ADDSUB"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Verb Categories",
            "Semantic Parsing"
          ],
          "connections": [
            "Equation Tree Generation"
          ],
          "mechanisms": [
            "Rule-Based Filtering"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Rule-Based"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Verb Category Extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ADDSUB_2014",
        "entity_type": "Dataset",
        "name": "ADDSUB",
        "description": "Addition and subtraction word problems with irrelevant distractor quantities",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kushman2014_TemplateBased",
        "entity_type": "Algorithm",
        "name": "Template-Based Method",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": 2014,
        "authors": [
          "Kushman, N.",
          "Artzi, Y.",
          "Zettlemoyer, L.",
          "Barzilay, R."
        ],
        "task": [
          "Algebraic Word Problem Solving"
        ],
        "datasets": [
          "SINGLEEQ"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Templates",
            "Global and Local Features"
          ],
          "connections": [
            "Equation Tree Mapping"
          ],
          "mechanisms": [
            "Template Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Fully Supervised Learning"
          ],
          "parameter_tuning": [
            "Template-Based"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Template Extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Socher2013_CompositionalVectorGrammar",
        "entity_type": "Algorithm",
        "name": "Compositional Vector Grammar (CVG)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": 2013,
        "authors": [
          "Richard Socher",
          "John Bauer",
          "Christopher D. Manning",
          "Andrew Y. Ng"
        ],
        "task": [
          "Syntactic Parsing"
        ],
        "datasets": [
          "Penn Treebank WSJ"
        ],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Probabilistic Context-Free Grammar (PCFG)",
            "Syntactically Untied Recursive Neural Network (SU-RNN)"
          ],
          "connections": [
            "Composition Function"
          ],
          "mechanisms": [
            "Continuous Vector Representations",
            "Max-Margin Training Objective"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CKY Dynamic Programming",
            "Beam Search",
            "AdaGrad"
          ],
          "parameter_tuning": [
            "Regularization",
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Vector Representations",
          "POS Tags"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Socher2013_SyntacticallyUntiedRecursiveNeuralNetwork",
        "entity_type": "Algorithm",
        "name": "Syntactically Untied Recursive Neural Network (SU-RNN)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": 2013,
        "authors": [
          "Richard Socher",
          "John Bauer",
          "Christopher D. Manning",
          "Andrew Y. Ng"
        ],
        "task": [
          "Syntactic Parsing"
        ],
        "datasets": [
          "Penn Treebank WSJ"
        ],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Recursive Neural Network"
          ],
          "connections": [
            "Composition Function"
          ],
          "mechanisms": [
            "Syntactically Untied Weights"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Backpropagation Through Structure (BTS)"
          ],
          "parameter_tuning": [
            "Weight Matrices Initialization"
          ]
        },
        "feature_processing": [
          "Word Vector Representations"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PennTreebank_WSJ_2013",
        "entity_type": "Dataset",
        "name": "Penn Treebank WSJ",
        "description": "Wall Street Journal section of the Penn Treebank",
        "domain": "Natural Language Processing",
        "size": "Varies by section",
        "year": 2013,
        "creators": [
          "Various contributors"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Parsing",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Parsing Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_QuantityExtraction",
        "entity_type": "Algorithm",
        "name": "QuantityExtraction",
        "title": "Reasoning about Quantities in Natural Language",
        "year": 2015,
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": [
          "Quantity Extraction",
          "Standardization"
        ],
        "datasets": [
          "RTE Datasets",
          "Newswire Text"
        ],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Segmentation",
            "Standardization"
          ],
          "connections": [
            "Semi-CRF",
            "Bank of Classifiers"
          ],
          "mechanisms": [
            "Parameter Averaging",
            "Perceptron"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Structured Perceptron",
            "Parameter Averaging"
          ],
          "parameter_tuning": [
            "Semi-CRF",
            "Bank of Classifiers"
          ]
        },
        "feature_processing": [
          "Word class features",
          "Character-based features",
          "Part of speech tags"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_QuantityEntailment",
        "entity_type": "Algorithm",
        "name": "QuantityEntailment",
        "title": "Reasoning about Quantities in Natural Language",
        "year": 2015,
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": [
          "Quantity Entailment"
        ],
        "datasets": [
          "RTE Datasets"
        ],
        "metrics": [
          "Precision",
          "Recall",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Extraction Phase",
            "Reasoning Phase"
          ],
          "connections": [
            "Implicit Quantity Productions",
            "Quantity Comparisons"
          ],
          "mechanisms": [
            "Monotonicity Verification",
            "Coreference Resolution",
            "Semantic Role Labeling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Rule-based",
            "Logical Inference"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Coreference Resolution",
          "Semantic Role Labeling"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_MathWordProblemSolver",
        "entity_type": "Algorithm",
        "name": "MathWordProblemSolver",
        "title": "Reasoning about Quantities in Natural Language",
        "year": 2015,
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": [
          "Solving Math Word Problems"
        ],
        "datasets": [
          "Elementary Math Word Problems"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Quantity Pair Classifier",
            "Operation Classifier",
            "Order Classifier"
          ],
          "connections": [
            "Cascade of Classifiers"
          ],
          "mechanisms": [
            "Sparse Averaged Perceptron"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gold Annotations"
          ],
          "parameter_tuning": [
            "SNOW Framework"
          ]
        },
        "feature_processing": [
          "Unigrams",
          "Bigrams",
          "POS Tags",
          "Relevant Quantities"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "RTE_Datasets_2006",
        "entity_type": "Dataset",
        "name": "RTE Datasets",
        "description": "Recognizing Textual Entailment datasets",
        "domain": "Natural Language Processing",
        "size": 384,
        "year": 2006,
        "creators": [
          "Dagan, I.",
          "Glickman, O.",
          "Magnini, B."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Newswire_Text_2015",
        "entity_type": "Dataset",
        "name": "Newswire Text",
        "description": "Sentences of newswire text containing quantity mentions",
        "domain": "Natural Language Processing",
        "size": 600,
        "year": 2015,
        "creators": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Elementary_Math_Word_Problems_2015",
        "entity_type": "Dataset",
        "name": "Elementary Math Word Problems",
        "description": "Elementary school math word problems",
        "domain": "Education",
        "size": 869,
        "year": 2015,
        "creators": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Quantity_Segmentation",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Segmentation Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Precision_Entailment",
        "entity_type": "Metric",
        "name": "Precision",
        "description": "Proportion of true positive predictions",
        "category": "Entailment Evaluation",
        "formula": "True Positives / (True Positives + False Positives)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall_Entailment",
        "entity_type": "Metric",
        "name": "Recall",
        "description": "Proportion of actual positives predicted correctly",
        "category": "Entailment Evaluation",
        "formula": "True Positives / (True Positives + False Negatives)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Entailment",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Entailment Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Math_Word_Problems",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Proportion of correct answers",
        "category": "Math Word Problem Solving",
        "formula": "Correct Answers / Total Questions"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Jung2004_WindowedHoughTransform",
        "entity_type": "Algorithm",
        "name": "Windowed Hough Transform",
        "title": "Rectangle Detection based on a Windowed Hough Transform",
        "year": 2004,
        "authors": [
          "Claudio Rosito Jung",
          "Rodrigo Schramm"
        ],
        "task": [
          "Rectangle Detection"
        ],
        "datasets": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Sliding Window",
            "Hough Transform",
            "Peak Extraction"
          ],
          "connections": [
            "Geometric Constraints"
          ],
          "mechanisms": [
            "Line Segment Detection",
            "Rectangle Pattern Matching"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": [
            "Tθ",
            "Tρ",
            "TL",
            "Tα"
          ]
        },
        "feature_processing": [
          "Edge Detection"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lagunovsky1999_StraightLineBasedPrimitiveExtraction",
        "entity_type": "Algorithm",
        "name": "Straight-Line-Based Primitive Extraction",
        "title": "Straight-line-based primitive extraction in grey-scale object recognition",
        "year": 1999,
        "authors": [
          "Dmitry Lagunovsky",
          "Sergey Ablameyko"
        ],
        "task": [
          "Primitive Extraction"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Linear Primitives",
            "Grouping"
          ],
          "connections": [
            "Line Segments"
          ],
          "mechanisms": [
            "Quadrangle Approximation"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lin1998_BuildingDetectionAndDescription",
        "entity_type": "Algorithm",
        "name": "Building Detection and Description",
        "title": "Building detection and description from a single intensity image",
        "year": 1998,
        "authors": [
          "Changjiang Lin",
          "Ram Nevatia"
        ],
        "task": [
          "Building Detection",
          "Building Description"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Line Detection",
            "Anti-Parallel Lines"
          ],
          "connections": [
            "Search Region"
          ],
          "mechanisms": [
            "Rectangle Formation"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Tao2002_RectangleBuildingExtraction",
        "entity_type": "Algorithm",
        "name": "Rectangle Building Extraction",
        "title": "A new approach to extract rectangle building from aerial urban images",
        "year": 2002,
        "authors": [
          "Wei-Bin Tao",
          "Jian-Wu Tian",
          "Jian Liu"
        ],
        "task": [
          "Building Extraction"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Edge Elements",
            "Linear Elements"
          ],
          "connections": [
            "Parallel Lines"
          ],
          "mechanisms": [
            "Rectangular Primitive Structures"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Zhu2003_AutomaticParticleDetection",
        "entity_type": "Algorithm",
        "name": "Automatic Particle Detection",
        "title": "Automatic particle detection through efficient hough transforms",
        "year": 2003,
        "authors": [
          "Yuhui Zhu",
          "Barry Carragher",
          "F. Mouche",
          "Carolyn S. Potter"
        ],
        "task": [
          "Particle Detection"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Rectangular Hough Transform"
          ],
          "connections": [
            "Accumulator Array"
          ],
          "mechanisms": [
            "Center and Orientation Detection"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Detection",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Detection accuracy",
        "category": "Detection Evaluation",
        "formula": "Correct detections / Total detections"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bakman2023_ROBUST",
        "entity_type": "Algorithm",
        "name": "ROBUST",
        "title": "ROBUST UNDERSTANDING OF WORD PROBLEMS WITH EXTRANEOUS INFORMATION",
        "year": 2023,
        "authors": [
          "Bakman, Y."
        ],
        "task": [
          "Understanding Arithmetic Word Problems with Extraneous Information"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Change Schema Recognition",
            "Formula Matching",
            "Schema Instantiation"
          ],
          "connections": [
            "Change Verbs Categorization",
            "Natural Language Parsing"
          ],
          "mechanisms": [
            "Cautious Strategy for Schema Instantiation Creation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "None"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Sentence Splitting",
          "Variable Substitution"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WordProblems_2023",
        "entity_type": "Dataset",
        "name": "Word Problems",
        "description": "A collection of multi-step arithmetic word problems with extraneous information.",
        "domain": "Arithmetic Problem Solving",
        "size": "Not specified",
        "year": 2023,
        "creators": [
          "Bakman, Y."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Correctness_Solution",
        "entity_type": "Metric",
        "name": "Correctness",
        "description": "The correctness of the solution to word problems.",
        "category": "Solution Evaluation",
        "formula": "Number of Correct Solutions / Total Number of Problems"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Relevance_Schema",
        "entity_type": "Metric",
        "name": "Relevance",
        "description": "The relevance of schema instantiations to the problem solution.",
        "category": "Schema Evaluation",
        "formula": "Number of Relevant Schema Instantiations / Total Number of Schema Instantiations"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kwiatkowski2013_SemanticParser",
        "entity_type": "Algorithm",
        "name": "Semantic Parser with On-the-fly Ontology Matching",
        "title": "Scaling Semantic Parsers with On-the-fly Ontology Matching",
        "year": 2013,
        "authors": [
          "Tom Kwiatkowski",
          "Eunsol Choi",
          "Yoav Artzi",
          "Luke Zettlemoyer"
        ],
        "task": [
          "Question Answering",
          "Semantic Parsing"
        ],
        "datasets": [
          "GeoQuery_1996",
          "FreebaseQA_2013"
        ],
        "metrics": [
          "Recall",
          "Precision",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Probabilistic CCG",
            "Ontology Matching Model"
          ],
          "connections": [
            "CCG Parsing",
            "Logical Form Transformation"
          ],
          "mechanisms": [
            "Domain-independent Parsing",
            "Structure Matching",
            "Constant Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Perceptron",
            "Latent Variable Learning"
          ],
          "parameter_tuning": [
            "Feature Weights",
            "Margin-based Loss"
          ]
        },
        "feature_processing": [
          "Word Class Information",
          "Lexical Features",
          "Structural Features",
          "Knowledge Base Features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "GeoQuery_1996",
        "entity_type": "Dataset",
        "name": "GeoQuery",
        "description": "Geography database with a small ontology and questions with relatively complex, compositional structure.",
        "domain": "Geography",
        "size": null,
        "year": 1996,
        "creators": [
          "Zelle, J.",
          "Mooney, R."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "FreebaseQA_2013",
        "entity_type": "Dataset",
        "name": "Freebase QA",
        "description": "Questions to Freebase, a large community-authored database that spans many sub-domains.",
        "domain": "General Knowledge",
        "size": 917,
        "year": 2013,
        "creators": [
          "Cai, Q.",
          "Yates, A."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall_QA",
        "entity_type": "Metric",
        "name": "Recall",
        "description": "Percentage of total questions answered correctly.",
        "category": "Question Answering Evaluation",
        "formula": "Correctly Answered Questions / Total Questions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Precision_QA",
        "entity_type": "Metric",
        "name": "Precision",
        "description": "Percentage of produced queries with correct answers.",
        "category": "Question Answering Evaluation",
        "formula": "Correct Queries / Total Produced Queries"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_QA",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of Precision and Recall.",
        "category": "Question Answering Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chiang2019_SemanticallyAlignedEquationGenerator",
        "entity_type": "Algorithm",
        "name": "Semantically-Aligned Equation Generator",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": 2019,
        "authors": [
          "Ting-Rui Chiang",
          "Yun-Nung Chen"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Stack",
            "Semantic Transformer"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "Gated Recurrent Unit",
            "Stack Actions",
            "Semantic Representation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Semantic Representation Extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Knuth1968_ContextFreeGrammarAlgorithm",
        "entity_type": "Algorithm",
        "name": "Context-Free Grammar Algorithm",
        "title": "Semantics of Context-Free Languages",
        "year": 1968,
        "authors": [
          "Donald E. Knuth"
        ],
        "task": [
          "Context-Free Grammar Parsing"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Directed Graph Construction"
          ],
          "connections": [
            "Graph Pasting"
          ],
          "mechanisms": [
            "Cycle Detection"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sutskever2014_SequenceToSequenceLearning",
        "entity_type": "Algorithm",
        "name": "Sequence to Sequence Learning with Neural Networks",
        "title": "Sequence to Sequence Learning with Neural Networks",
        "year": 2014,
        "authors": [
          "Sutskever, I.",
          "Vinyals, O.",
          "Le, Q. V."
        ],
        "task": [
          "Machine Translation"
        ],
        "datasets": [
          "WMT'14 English to French"
        ],
        "metrics": [
          "BLEU score"
        ],
        "architecture": {
          "components": [
            "Multilayered LSTM",
            "Deep LSTM"
          ],
          "connections": [
            "Fixed-dimensional vector representation"
          ],
          "mechanisms": [
            "Long Short-Term Memory"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised backpropagation",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Learning rate decay",
            "Gradient clipping"
          ]
        },
        "feature_processing": [
          "Word embeddings",
          "Reversing source sentences"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WMT14_EnglishToFrench",
        "entity_type": "Dataset",
        "name": "WMT'14 English to French",
        "description": "A large-scale dataset for English to French translation",
        "domain": "Natural Language Processing",
        "size": 12000000,
        "year": 2014,
        "creators": [
          "Various contributors"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "BLEU_TranslationQuality",
        "entity_type": "Metric",
        "name": "BLEU score",
        "description": "A metric for evaluating the quality of machine translation",
        "category": "Translation Evaluation",
        "formula": "Exponential averaging of modified n-gram precision"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kalchbrenner2013_RecurrentContinuousTranslationModels",
        "entity_type": "Algorithm",
        "name": "Recurrent Continuous Translation Models",
        "title": "Recurrent Continuous Translation Models",
        "year": 2013,
        "authors": [
          "Kalchbrenner, N.",
          "Blunsom, P."
        ],
        "task": [
          "Machine Translation"
        ],
        "datasets": [
          "Not specified"
        ],
        "metrics": [
          "Not specified"
        ],
        "architecture": {
          "components": [
            "Convolutional Neural Networks"
          ],
          "connections": [
            "Vector representation"
          ],
          "mechanisms": [
            "Not specified"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Not specified"
          ],
          "parameter_tuning": [
            "Not specified"
          ]
        },
        "feature_processing": [
          "Not specified"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bahdanau2014_NeuralMachineTranslation",
        "entity_type": "Algorithm",
        "name": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "year": 2014,
        "authors": [
          "Bahdanau, D.",
          "Cho, K.",
          "Bengio, Y."
        ],
        "task": [
          "Machine Translation"
        ],
        "datasets": [
          "Not specified"
        ],
        "metrics": [
          "Not specified"
        ],
        "architecture": {
          "components": [
            "Attention mechanism"
          ],
          "connections": [
            "Not specified"
          ],
          "mechanisms": [
            "Not specified"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Not specified"
          ],
          "parameter_tuning": [
            "Not specified"
          ]
        },
        "feature_processing": [
          "Not specified"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wiseman2016_BSO",
        "entity_type": "Algorithm",
        "name": "Beam Search Optimization (BSO)",
        "title": "Sequence-to-Sequence Learning as Beam-Search Optimization",
        "year": 2016,
        "authors": [
          "Sam Wiseman",
          "Alexander M. Rush"
        ],
        "task": [
          "Word Ordering",
          "Dependency Parsing",
          "Machine Translation"
        ],
        "datasets": [
          "PTB",
          "Penn Treebank",
          "IWSLT 2014 German-to-English"
        ],
        "metrics": [
          "BLEU",
          "UAS",
          "LAS"
        ],
        "architecture": {
          "components": [
            "LSTM Encoder",
            "LSTM Decoder",
            "Global Attention Model"
          ],
          "connections": [
            "Attention Mechanism",
            "Input Feeding"
          ],
          "mechanisms": [
            "Beam Search",
            "LaSO Framework"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search Training",
            "Curriculum Beam Strategy",
            "Pre-training with Cross-Entropy Loss"
          ],
          "parameter_tuning": [
            "Mini-batch Adagrad",
            "Gradient Renormalization",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embeddings Initialization",
          "Singleton Words Replacement",
          "Digit Normalization"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Luong2015_GlobalAttention",
        "entity_type": "Algorithm",
        "name": "Global Attention Model",
        "title": "Effective Approaches to Attention-based Neural Machine Translation",
        "year": 2015,
        "authors": [
          "Thang Luong",
          "Hieu Pham",
          "Christopher D. Manning"
        ],
        "task": [
          "Machine Translation"
        ],
        "datasets": [
          "IWSLT 2014 German-to-English"
        ],
        "metrics": [
          "BLEU"
        ],
        "architecture": {
          "components": [
            "LSTM Encoder",
            "LSTM Decoder"
          ],
          "connections": [
            "Attention Mechanism"
          ],
          "mechanisms": [
            "Scheduled Sampling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Entropy Loss",
            "Scheduled Sampling"
          ],
          "parameter_tuning": [
            "Mini-batch Adagrad"
          ]
        },
        "feature_processing": [
          "Word Embeddings Initialization"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PTB_2015",
        "entity_type": "Dataset",
        "name": "Penn Treebank (PTB)",
        "description": "A widely used dataset for parsing and language modeling",
        "domain": "Natural Language Processing",
        "size": "Varies depending on splits",
        "year": 2015,
        "creators": [
          "Various contributors"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IWSLT2014_GermanToEnglish",
        "entity_type": "Dataset",
        "name": "IWSLT 2014 German-to-English",
        "description": "Dataset from the IWSLT 2014 machine translation evaluation campaign",
        "domain": "Machine Translation",
        "size": "153K training sentences, 7K development sentences, 7K test sentences",
        "year": 2014,
        "creators": [
          "Mauro Cettolo",
          "Jan Niehues",
          "Sebastian Stüker",
          "Luisa Bentivogli",
          "Marcello Federico"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "UAS_Parsing",
        "entity_type": "Metric",
        "name": "UAS",
        "description": "Unlabeled Attachment Score for dependency parsing",
        "category": "Dependency Parsing Evaluation",
        "formula": "Percentage of words with correct head"
      }
    },
    {
      "metric_entity": {
        "metric_id": "LAS_Parsing",
        "entity_type": "Metric",
        "name": "LAS",
        "description": "Labeled Attachment Score for dependency parsing",
        "category": "Dependency Parsing Evaluation",
        "formula": "Percentage of words with correct head and label"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Haghighi2009_SimpleCoreferenceResolution",
        "entity_type": "Algorithm",
        "name": "Simple Coreference Resolution",
        "title": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
        "year": 2009,
        "authors": [
          "Aria Haghighi",
          "Dan Klein"
        ],
        "task": [
          "Coreference Resolution"
        ],
        "datasets": [
          "ACE2004-ROTH-DEV",
          "ACE2004-CULOTTA-TEST",
          "ACE2004-NWIRE",
          "MUC-6-TEST"
        ],
        "metrics": [
          "Pairwise F1",
          "b3",
          "MUC",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Syntactic Module",
            "Semantic Module",
            "Selection Module"
          ],
          "connections": [
            "Syntactic Paths",
            "Compatibility Lists"
          ],
          "mechanisms": [
            "Syntactic Constraints",
            "Semantic Compatibility",
            "Tree Distance"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Unsupervised Learning",
            "Treebank Parsing"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Syntactic Parsing",
          "Semantic Compatibility Mining",
          "Appositive Annotation",
          "Predicate Nominative Annotation"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-ROTH-DEV_2009",
        "entity_type": "Dataset",
        "name": "ACE2004-ROTH-DEV",
        "description": "Development set split of the ACE 2004 training set",
        "domain": "Natural Language Processing",
        "size": 68,
        "year": 2009,
        "creators": [
          "Bengston and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-CULOTTA-TEST_2009",
        "entity_type": "Dataset",
        "name": "ACE2004-CULOTTA-TEST",
        "description": "Test set split of the ACE 2004 training set",
        "domain": "Natural Language Processing",
        "size": 107,
        "year": 2009,
        "creators": [
          "Culotta et al.",
          "Bengston and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-NWIRE_2009",
        "entity_type": "Dataset",
        "name": "ACE2004-NWIRE",
        "description": "ACE 2004 Newswire set",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": 2009,
        "creators": [
          "Poon and Domingos"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MUC-6-TEST_2009",
        "entity_type": "Dataset",
        "name": "MUC-6-TEST",
        "description": "MUC6 formal evaluation set",
        "domain": "Natural Language Processing",
        "size": 30,
        "year": 2009,
        "creators": []
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "BLIPP_2009",
        "entity_type": "Dataset",
        "name": "BLIPP",
        "description": "1.8 million sentences of newswire parsed with the Charniak parser",
        "domain": "Natural Language Processing",
        "size": 1800000,
        "year": 2009,
        "creators": [
          "Charniak"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WIKI_2009",
        "entity_type": "Dataset",
        "name": "WIKI",
        "description": "25k articles of English Wikipedia abstracts parsed by the Klein and Manning parser",
        "domain": "Natural Language Processing",
        "size": 25000,
        "year": 2009,
        "creators": [
          "Klein and Manning"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "b3_Coreference",
        "entity_type": "Metric",
        "name": "b3",
        "description": "For each mention, form the intersection between the predicted cluster and the true cluster for that mention",
        "category": "Coreference Evaluation",
        "formula": "F1 = 2 * (precision * recall) / (precision + recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2016_ExpressionTreeBasedSolver",
        "entity_type": "Algorithm",
        "name": "Expression Tree Based Solver",
        "title": "Solving General Arithmetic Word Problems",
        "year": 2016,
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "AI2 Dataset",
          "IL Dataset",
          "Commoncore Dataset"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Tree",
            "Monotonic Expression Tree",
            "Quantity Schema"
          ],
          "connections": [
            "LCA Operations",
            "Relevance Classification"
          ],
          "mechanisms": [
            "Constrained Inference Framework",
            "Multiclass SVM Classifier"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search",
            "Pairwise Conjunction of Features"
          ],
          "parameter_tuning": [
            "wIRR"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Shallow Parsing",
          "Unit Extraction",
          "NP Extraction",
          "Rate Detection"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AI2_Dataset_2014",
        "entity_type": "Dataset",
        "name": "AI2 Dataset",
        "description": "Collection of 395 addition and subtraction problems",
        "domain": "Arithmetic Word Problems",
        "size": 395,
        "year": 2014,
        "creators": [
          "M. J. Hosseini",
          "H. Hajishirzi",
          "O. Etzioni",
          "N. Kushman"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IL_Dataset_2015",
        "entity_type": "Dataset",
        "name": "IL Dataset",
        "description": "Collection of arithmetic problems with one operation",
        "domain": "Arithmetic Word Problems",
        "size": 562,
        "year": 2015,
        "creators": [
          "S. Roy",
          "T. Vieira",
          "D. Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Commoncore_Dataset_2016",
        "entity_type": "Dataset",
        "name": "Commoncore Dataset",
        "description": "New dataset of multi-step arithmetic problems",
        "domain": "Arithmetic Word Problems",
        "size": 600,
        "year": 2016,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Relax_Accuracy",
        "entity_type": "Metric",
        "name": "Relax Accuracy",
        "description": "Fraction of quantities or quantity pairs correctly predicted",
        "category": "Partial Evaluation",
        "formula": "Number of Correct Predictions / Total Number of Predictions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Strict_Accuracy",
        "entity_type": "Metric",
        "name": "Strict Accuracy",
        "description": "Fraction of problems where all quantities or quantity pairs are correctly classified",
        "category": "Complete Evaluation",
        "formula": "Number of Fully Correct Problems / Total Number of Problems"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Seo2015_GEOS",
        "entity_type": "Algorithm",
        "name": "GEOS",
        "title": "Solving Geometry Problems: Combining Text and Diagram Interpretation",
        "year": 2015,
        "authors": [
          "Minjoon Seo",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Oren Etzioni",
          "Clint Malcolm"
        ],
        "task": [
          "Geometry Problem Solving"
        ],
        "datasets": [
          "SAT Geometry Questions"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Text Parser",
            "Diagram Parser",
            "Optimization Module",
            "Solver"
          ],
          "connections": [
            "Submodular Optimization",
            "Greedy Maximization"
          ],
          "mechanisms": [
            "Hypergraph Representation",
            "Log-linear Model",
            "Bridging Relations",
            "Coordinating Conjunctions"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Logistic Regression",
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "L2 Regularization",
            "Trade-off Parameter λ"
          ]
        },
        "feature_processing": [
          "Concept Identification",
          "Relation Identification",
          "Relation Completion",
          "Equation Analyzer"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SAT_Geometry_Questions_2015",
        "entity_type": "Dataset",
        "name": "SAT Geometry Questions",
        "description": "A dataset of SAT plane geometry questions with textual descriptions and diagrams.",
        "domain": "Educational Testing",
        "size": 186,
        "year": 2015,
        "creators": [
          "College Board",
          "Allen Institute for AI"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Text_Interpretation",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall for text interpretation.",
        "category": "Text Interpretation Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Dependency_Parsing_Accuracy",
        "entity_type": "Metric",
        "name": "Dependency Parsing Accuracy",
        "description": "Accuracy of dependency parsing structures matching the ground truth annotations.",
        "category": "Dependency Parsing Evaluation",
        "formula": "Number of correctly parsed dependencies / Total dependencies"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2016_RelationKnowledgePoweredModel",
        "entity_type": "Algorithm",
        "name": "Relation Knowledge Powered Model (RK)",
        "title": "Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding",
        "year": 2016,
        "authors": [
          "Huazheng Wang",
          "Fei Tian",
          "Bin Gao",
          "Chengjieren Zhu",
          "Jiang Bian",
          "Tie-Yan Liu"
        ],
        "task": [
          "Verbal Comprehension Questions in IQ Tests"
        ],
        "datasets": [
          "wiki2014",
          "VerbalQuestions"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Question Classifier",
            "Word-Sense Pair Embedding",
            "Relation Embedding"
          ],
          "connections": [
            "Multi-Sense Clustering",
            "Relational Knowledge Integration"
          ],
          "mechanisms": [
            "Spherical k-means",
            "Skip-gram",
            "Soft Norm Constraint"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Back Propagation",
            "Negative Sampling"
          ],
          "parameter_tuning": [
            "Window Size",
            "Embedding Dimension",
            "Epoch Number"
          ]
        },
        "feature_processing": [
          "TF-IDF",
          "Context Window Representation",
          "Dictionary Matching"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "wiki2014_2016",
        "entity_type": "Dataset",
        "name": "wiki2014",
        "description": "A large text snapshot from Wikipedia",
        "domain": "Natural Language Processing",
        "size": 3400000000,
        "year": 2016,
        "creators": [
          "Wikipedia Contributors"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "VerbalQuestions_2016",
        "entity_type": "Dataset",
        "name": "VerbalQuestions",
        "description": "A collection of verbal comprehension questions from published IQ test books",
        "domain": "Intelligence Testing",
        "size": 232,
        "year": 2016,
        "creators": [
          "Philip Carter",
          "Dan Pape",
          "Ken Russell"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bengio2003_NeuralProbabilisticLanguageModel",
        "entity_type": "Algorithm",
        "name": "Neural Probabilistic Language Model",
        "title": "A Neural Probabilistic Language Model",
        "year": 2003,
        "authors": [
          "Yoshua Bengio",
          "Rejean Ducharme",
          "Pascal Vincent",
          "Christian Jauvin"
        ],
        "task": [
          "Language Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {},
        "methodology": {},
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Blei2003_LatentDirichletAllocation",
        "entity_type": "Algorithm",
        "name": "Latent Dirichlet Allocation (LDA)",
        "title": "Latent Dirichlet Allocation",
        "year": 2003,
        "authors": [
          "David M Blei",
          "Andrew Y Ng",
          "Michael I Jordan"
        ],
        "task": [
          "Topic Modeling"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {},
        "methodology": {},
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Collobert2008_UnifiedArchitecture",
        "entity_type": "Algorithm",
        "name": "Unified Architecture for Natural Language Processing",
        "title": "A Unified Architecture for Natural Language Processing",
        "year": 2008,
        "authors": [
          "Ronan Collobert",
          "Jason Weston"
        ],
        "task": [
          "Natural Language Processing"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {},
        "methodology": {},
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Pennington2014_Glove",
        "entity_type": "Algorithm",
        "name": "Glove",
        "title": "Global Vectors for Word Representation",
        "year": 2014,
        "authors": [
          "Jeffrey Pennington",
          "Richard Socher",
          "Christopher D Manning"
        ],
        "task": [
          "Word Embedding"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {},
        "methodology": {},
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Huang2012_MultiSenseWordEmbedding",
        "entity_type": "Algorithm",
        "name": "Multi-Sense Word Embedding",
        "title": "Improving Word Representations via Global Context and Multiple Word Prototypes",
        "year": 2012,
        "authors": [
          "Eric H Huang",
          "Richard Socher",
          "Christopher D Manning",
          "Andrew Y Ng"
        ],
        "task": [
          "Word Embedding"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {},
        "methodology": {},
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Tian2014_ProbabilisticModel",
        "entity_type": "Algorithm",
        "name": "Probabilistic Model for Learning Multi-Prototype Word Embeddings",
        "title": "A Probabilistic Model for Learning Multi-Prototype Word Embeddings",
        "year": 2014,
        "authors": [
          "Fei Tian",
          "Hanjun Dai",
          "Jiang Bian",
          "Bin Gao",
          "Rui Zhang",
          "Enhong Chen",
          "Tie-Yan Liu"
        ],
        "task": [
          "Word Embedding"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {},
        "methodology": {},
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Neelakantan2014_EfficientNonParametricEstimation",
        "entity_type": "Algorithm",
        "name": "Efficient Non-Parametric Estimation of Multiple Embeddings per Word",
        "title": "Efficient Non-Parametric Estimation of Multiple Embeddings per Word",
        "year": 2014,
        "authors": [
          "Arvind Neelakantan",
          "Jeevan Shankar",
          "Alexandre Passos",
          "Andrew McCallum"
        ],
        "task": [
          "Word Embedding"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {},
        "methodology": {},
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Alvin2014_GenProblem",
        "entity_type": "Algorithm",
        "name": "GenProblem",
        "title": "Synthesis of Geometry Proof Problems",
        "year": 2014,
        "authors": [
          "Alvin, C.",
          "Gulwani, S.",
          "Majumdar, R.",
          "Mukhopadhyay, S."
        ],
        "task": [
          "Geometry Proof Problem Generation"
        ],
        "datasets": [
          "Figures from Geometry Textbooks"
        ],
        "metrics": [
          "Number of Generated Problems",
          "Time Taken to Generate Problems"
        ],
        "architecture": {
          "components": [
            "Hypergraph Construction",
            "Minimal Assumption Generation",
            "Strictly Interesting Problem Synthesis"
          ],
          "connections": [
            "Derive Function",
            "Choose Operator"
          ],
          "mechanisms": [
            "Fixed-Point Procedure",
            "Non-Deterministic Choices"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Hypergraph Reachability"
          ],
          "parameter_tuning": [
            "Minimal Sets Enumeration",
            "Goal Set Generation"
          ]
        },
        "feature_processing": [
          "Implicit and Explicit Facts Extraction",
          "Predicate Handling"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Figures_from_Textbooks_2014",
        "entity_type": "Dataset",
        "name": "Figures from Geometry Textbooks",
        "description": "A set of 110 geometric figures taken from standard mathematics textbooks in India and the United States.",
        "domain": "High School Geometry",
        "size": 110,
        "year": 2014,
        "creators": [
          "Alvin, C.",
          "Gulwani, S.",
          "Majumdar, R.",
          "Mukhopadhyay, S."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Number_of_Generated_Problems_Generation",
        "entity_type": "Metric",
        "name": "Number of Generated Problems",
        "description": "The number of geometry proof problems generated per figure.",
        "category": "Problem Generation Evaluation",
        "formula": "Total number of problems generated / Number of figures"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Time_Taken_to_Generate_Problems_Generation",
        "entity_type": "Metric",
        "name": "Time Taken to Generate Problems",
        "description": "The average time taken to generate problems per figure.",
        "category": "Efficiency Evaluation",
        "formula": "Total time taken / Number of figures"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Strictly_Interesting_Problems_Generation",
        "entity_type": "Metric",
        "name": "Strictly Interesting Problems",
        "description": "The number of strictly interesting problems generated.",
        "category": "Problem Quality Evaluation",
        "formula": "Number of strictly interesting problems"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Strictly_Complete_Problems_Generation",
        "entity_type": "Metric",
        "name": "Strictly Complete Problems",
        "description": "The number of strictly complete problems generated.",
        "category": "Problem Completeness Evaluation",
        "formula": "Number of strictly complete problems"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2019_TemplateBasedSolver",
        "entity_type": "Algorithm",
        "name": "Template-Based Solver with Recursive Neural Networks",
        "title": "Template-Based Math Word Problem Solvers with Recursive Neural Networks",
        "year": 2019,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Jipeng Zhang",
          "Xing Xu",
          "Lianli Gao",
          "Bing Tian Dai",
          "Heng Tao Shen"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K",
          "MAWPS"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Bi-LSTM",
            "Self Attention",
            "Recursive Neural Network"
          ],
          "connections": [
            "Attention Layer",
            "Bottom-Up Inference"
          ],
          "mechanisms": [
            "Expression Tree Construction",
            "Operator Encapsulation",
            "Equation Normalization"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Adam Optimizer",
            "SGD Optimizer",
            "Dropout"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Batch Size",
            "Epochs"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Quantity Extraction",
          "Suffix Expression Serialization"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_MathProblemSolving",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Proportion of correctly solved math word problems",
        "category": "Math Word Problem Solving Evaluation",
        "formula": "Correctly solved problems / Total problems"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_EquationNormalization",
        "entity_type": "Algorithm",
        "name": "Equation Normalization",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Tree"
          ],
          "connections": [
            "Normalization Rules"
          ],
          "mechanisms": [
            "Order Duplication Handling",
            "Bracket Duplication Handling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Number Tokenization",
          "Equation Template Transformation"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_BiLSTM",
        "entity_type": "Algorithm",
        "name": "BiLSTM",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM"
          ],
          "connections": [
            "Global Attention Mechanism"
          ],
          "mechanisms": [
            "LSTM Cells"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Adam Optimizer"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Beam Search"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_ConvS2S",
        "entity_type": "Algorithm",
        "name": "ConvS2S",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Convolutional Layers"
          ],
          "connections": [
            "Gate Linear Units"
          ],
          "mechanisms": [
            "Convolutional Structure"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Early Stopping",
            "Learning Rate Annealing"
          ],
          "parameter_tuning": [
            "Max-Epochs",
            "Hidden Size"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Beam Search"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_Transformer",
        "entity_type": "Algorithm",
        "name": "Transformer",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Multi-Head Self-Attention",
            "Position-Wise Fully-Connected Feed-Forward Network"
          ],
          "connections": [
            "Self-Attention"
          ],
          "mechanisms": [
            "Attention Mechanism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Adam Optimizer"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Beam Search"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_EnsembleModel",
        "entity_type": "Algorithm",
        "name": "Ensemble Model",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": [
          "Math Word Problem Solving"
        ],
        "datasets": [
          "Math23K"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "BiLSTM",
            "ConvS2S",
            "Transformer"
          ],
          "connections": [
            "Generation Probability Selection"
          ],
          "mechanisms": [
            "Model Combination"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Number Tokenization",
          "Beam Search"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Fletcher1985_WORDPRO",
        "entity_type": "Algorithm",
        "name": "WORDPRO",
        "title": "Understanding and solving arithmetic word problems: A computer simulation",
        "year": 1985,
        "authors": [
          "Fletcher, C. R."
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [],
        "metrics": [
          "Run-Time Statistics"
        ],
        "architecture": {
          "components": [
            "Production Rules",
            "Set Schema",
            "Transfer Schema",
            "Superset Schema",
            "More-Than/Less-Than Schema"
          ],
          "connections": [
            "Short-Term Memory (STM)",
            "Long-Term Memory (LTM)"
          ],
          "mechanisms": [
            "Meaning Postulates",
            "Arithmetic Strategies",
            "Problem-Solving Procedures"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Read-Purge Loop"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Propositional Representation",
          "Bilevel Representation"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "RunTimeStatistics_Performance",
        "entity_type": "Metric",
        "name": "Run-Time Statistics",
        "description": "Statistics collected during program execution to evaluate performance",
        "category": "Program Performance Evaluation",
        "formula": "Number of production rules fired, number of conversions, number of LTM searches, maximum number of chunks held over"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Novak1990_BEATRIX",
        "entity_type": "Algorithm",
        "name": "BEATRIX",
        "title": "Understanding Natural Language with Diagrams",
        "year": 1990,
        "authors": [
          "Novak, G. S.",
          "Bulko, W."
        ],
        "task": [
          "Physics Problem Solving"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "English Parser",
            "Diagram Parser",
            "Coreference Resolver"
          ],
          "connections": [
            "Blackboard Architecture"
          ],
          "mechanisms": [
            "Opportunistic Co-parsers",
            "Knowledge Sources"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Syntactic Recognition",
          "Semantic Processing"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PhysicsProblems_1990",
        "entity_type": "Dataset",
        "name": "Physics Problems",
        "description": "Textbook physics problems specified by a combination of English text and a diagram.",
        "domain": "Physics Education",
        "size": null,
        "year": 1990,
        "creators": [
          "Novak, G. S.",
          "Bulko, W."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "UnifiedModel_Quality",
        "entity_type": "Metric",
        "name": "Unified Model Quality",
        "description": "Quality of the unified internal model that represents the problem, including information derived from both the English text and the diagram.",
        "category": "Problem Understanding",
        "formula": null
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bulko1988_BEATRIX",
        "entity_type": "Algorithm",
        "name": "BEATRIX",
        "title": "Understanding Text With an Accompanying Diagram",
        "year": 1988,
        "authors": [
          "William C. Bulko"
        ],
        "task": [
          "Physics Problem Solving",
          "Text and Diagram Understanding"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Graphic Interface",
            "Blackboard System",
            "Parsing Module"
          ],
          "connections": [
            "Coreference Resolution",
            "Object Identification"
          ],
          "mechanisms": [
            "Blackboard Control Structure",
            "Knowledge Sources"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Text Parsing",
          "Diagram Parsing",
          "Coreference Resolution"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CollegeLevelPhysicsTextbooks_1988",
        "entity_type": "Dataset",
        "name": "College-Level Physics Textbooks",
        "description": "Collections of ready-made test cases in the form of college-level textbooks",
        "domain": "Physics Education",
        "size": null,
        "year": 1988,
        "creators": []
      }
    },
    {
      "metric_entity": {
        "metric_id": "CorrectnessAndCompleteness_PhysicsModel",
        "entity_type": "Metric",
        "name": "Correctness and Completeness",
        "description": "Validation of the correctness and completeness of the model produced by BEATRIX",
        "category": "Physics Problem Solving",
        "formula": null
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bengtson2008_PairwiseCoreferenceModel",
        "entity_type": "Algorithm",
        "name": "Pairwise Coreference Model",
        "title": "Understanding the Value of Features for Coreference Resolution",
        "year": 2008,
        "authors": [
          "Eric Bengtson",
          "Dan Roth"
        ],
        "task": [
          "Coreference Resolution"
        ],
        "datasets": [
          "ACE 2004 English"
        ],
        "metrics": [
          "B-Cubed F-Score",
          "MUC F-Score"
        ],
        "architecture": {
          "components": [
            "Pairwise Coreference Function",
            "Best-Link Decision Model"
          ],
          "connections": [
            "Graph-based Representation"
          ],
          "mechanisms": [
            "Averaged Perceptron Learning Algorithm"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Regularized Averaged Perceptron"
          ],
          "parameter_tuning": [
            "Learning Rate = 0.1",
            "Regularization Parameter = 3.5"
          ]
        },
        "feature_processing": [
          "Mention Types",
          "String Relation Features",
          "Semantic Features",
          "Relative Location Features",
          "Learned Features",
          "Aligned Modifiers",
          "Memorization Features",
          "Predicted Entity Types"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004_English_2004",
        "entity_type": "Dataset",
        "name": "ACE 2004 English",
        "description": "Official ACE 2004 English training data for coreference resolution",
        "domain": "Natural Language Processing",
        "size": 336,
        "year": 2004,
        "creators": [
          "NIST"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "B-Cubed_F-Score_Coreference",
        "entity_type": "Metric",
        "name": "B-Cubed F-Score",
        "description": "Measure of the overlap of predicted clusters and true clusters",
        "category": "Coreference Evaluation",
        "formula": "Harmonic mean of precision and recall"
      }
    },
    {
      "metric_entity": {
        "metric_id": "MUC_F-Score_Coreference",
        "entity_type": "Metric",
        "name": "MUC F-Score",
        "description": "Official MUC scoring algorithm for coreference resolution",
        "category": "Coreference Evaluation",
        "formula": "Harmonic mean of precision and recall"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2017_UNITDEP",
        "entity_type": "Algorithm",
        "name": "UNITDEP",
        "title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
        "year": 2017,
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": [
          "Arithmetic Word Problem Solving"
        ],
        "datasets": [
          "AllArith",
          "AllArithLex",
          "AllArithTmpl"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Vertex Classifier",
            "Edge Classifier",
            "Constrained Inference Module"
          ],
          "connections": [
            "Joint Inference with Arithmetic Solver"
          ],
          "mechanisms": [
            "Decomposed Model",
            "Monotonic Expression Tree"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search",
            "Cross Validation"
          ],
          "parameter_tuning": [
            "Scaling Parameters"
          ]
        },
        "feature_processing": [
          "Context Features",
          "Rule Based Extraction Features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArith_2017",
        "entity_type": "Dataset",
        "name": "AllArith",
        "description": "A dataset of arithmetic word problems pooled from multiple sources and normalized.",
        "domain": "Arithmetic Word Problem Solving",
        "size": 831,
        "year": 2017,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArithLex_2017",
        "entity_type": "Dataset",
        "name": "AllArithLex",
        "description": "A subset of AllArith with low lexical overlap.",
        "domain": "Arithmetic Word Problem Solving",
        "size": 415,
        "year": 2017,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArithTmpl_2017",
        "entity_type": "Dataset",
        "name": "AllArithTmpl",
        "description": "A subset of AllArith with low template overlap.",
        "domain": "Arithmetic Word Problem Solving",
        "size": 415,
        "year": 2017,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Duda1972_HoughTransformation",
        "entity_type": "Algorithm",
        "name": "Hough Transformation",
        "title": "Use of the Hough Transformation To Detect Lines and Curves in Pictures",
        "year": 1972,
        "authors": [
          "Richard O. Duda",
          "Peter E. Hart"
        ],
        "task": [
          "Line Detection",
          "Curve Detection"
        ],
        "datasets": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Point-Line Transformation",
            "Accumulator Array"
          ],
          "connections": [
            "Parameter Space Mapping"
          ],
          "mechanisms": [
            "Normal Parameterization"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Quantization",
          "Intensity Change Detection"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "CoincidentPoints_Detection",
        "entity_type": "Metric",
        "name": "Coincident Points",
        "description": "Detection of colinear or nearly colinear points",
        "category": "Line Detection",
        "formula": "Number of points lying on the same line"
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MS_COCO_2014",
        "entity_type": "Dataset",
        "name": "MS COCO",
        "description": "Microsoft Common Objects in Context dataset",
        "domain": "Computer Vision",
        "size": 204721,
        "year": 2014,
        "creators": [
          "Lin, Tsung-Yi",
          "Maire, Michael",
          "Belongie, Serge",
          "Hays, James",
          "Perona, Pietro",
          "Ramanan, Deva",
          "Dollar, Piotr",
          "Zitnick, C. Lawrence"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Abstract_Scenes_2015",
        "entity_type": "Dataset",
        "name": "Abstract Scenes",
        "description": "Dataset of abstract scenes for VQA",
        "domain": "Computer Vision",
        "size": 50000,
        "year": 2015,
        "creators": [
          "Stanislaw Antol",
          "Aishwarya Agrawal",
          "Jiasen Lu",
          "Margaret Mitchell",
          "Dhruv Batra",
          "C. Lawrence Zitnick",
          "Devi Parikh"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Open-Answer",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Accuracy for open-answer task",
        "category": "Classification Evaluation",
        "formula": "min(# humans that provided that answer / 3, 1)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Multiple-Choice",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Accuracy for multiple-choice task",
        "category": "Classification Evaluation",
        "formula": "Number of correct answers / Total number of questions"
      }
    }
  ],
  "is_complete": true,
  "extraction_time": 1749097677.028818
}