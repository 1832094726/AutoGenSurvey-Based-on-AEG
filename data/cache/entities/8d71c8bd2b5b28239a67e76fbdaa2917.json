{
  "entities": [
    {
      "algorithm_entity": {
        "algorithm_id": "Chang2013_LatentLeftLinkingModel",
        "entity_type": "Algorithm",
        "name": "Latent Left-Linking Model (L3M)",
        "year": 2013,
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE 2004",
          "Ontonotes-5.0"
        ],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Pairwise Scorer",
            "Left-Linking Model"
          ],
          "connections": [
            "Pairwise Compatibility Scores",
            "Best-Left-Link Inference"
          ],
          "mechanisms": [
            "Efficient Inference",
            "Constraint-Augmented Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Approach",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Pairwise Weight Vector",
            "Threshold"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Features"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chang2013_ConstrainedLatentLeftLinkingModel",
        "entity_type": "Algorithm",
        "name": "Constrained Latent Left-Linking Model (CL3M)",
        "year": 2013,
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE 2004",
          "Ontonotes-5.0"
        ],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Pairwise Scorer",
            "Left-Linking Model",
            "Domain Knowledge-Based Constraints"
          ],
          "connections": [
            "Pairwise Compatibility Scores",
            "Best-Left-Link Inference",
            "Constraint-Augmented Scoring Function"
          ],
          "mechanisms": [
            "Efficient Inference",
            "Constraint-Augmented Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Approach",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Pairwise Weight Vector",
            "Threshold",
            "Constraint Scores"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Features",
          "Constraint Features"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chang2013_ProbabilisticLatentLeftLinkingModel",
        "entity_type": "Algorithm",
        "name": "Probabilistic Latent Left-Linking Model (PL3M)",
        "year": 2013,
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE 2004",
          "Ontonotes-5.0"
        ],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Pairwise Scorer",
            "Left-Linking Model",
            "Probability Distribution"
          ],
          "connections": [
            "Pairwise Compatibility Scores",
            "Best-Left-Link Inference",
            "Probability-Based Clustering"
          ],
          "mechanisms": [
            "Efficient Inference",
            "Constraint-Augmented Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Likelihood-Based Approach",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Pairwise Weight Vector",
            "Temperature Parameter γ"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE_2004",
        "entity_type": "Dataset",
        "name": "ACE 2004",
        "year": 2004,
        "creators": [
          "NIST"
        ],
        "description": "A dataset containing 443 documents used for coreference resolution.",
        "domain": "Natural Language Processing"
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Ontonotes_2012",
        "entity_type": "Dataset",
        "name": "Ontonotes-5.0",
        "year": 2012,
        "creators": [
          "Pradhan et al."
        ],
        "description": "A large annotated corpus for coreference resolution containing 3,145 documents from various sources.",
        "domain": "Natural Language Processing"
      }
    },
    {
      "metric_entity": {
        "metric_id": "MUC_Coreference",
        "entity_type": "Metric",
        "name": "MUC",
        "description": "Metric for evaluating coreference resolution performance.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper."
      }
    },
    {
      "metric_entity": {
        "metric_id": "BCUB_Coreference",
        "entity_type": "Metric",
        "name": "BCUB",
        "description": "Metric for evaluating coreference resolution performance.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper."
      }
    },
    {
      "metric_entity": {
        "metric_id": "CEAF_EntityBased",
        "entity_type": "Metric",
        "name": "Entity-based CEAF",
        "description": "Metric for evaluating coreference resolution performance.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper."
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1Score_Average",
        "entity_type": "Metric",
        "name": "Average F1 Score",
        "description": "Average of F1 scores across multiple metrics.",
        "category": "Coreference Evaluation",
        "formula": "Average of MUC, BCUB, and CEAF F1 scores."
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chen2014_NeuralNetworkDependencyParser",
        "entity_type": "Algorithm",
        "name": "Neural Network Dependency Parser",
        "title": "A Fast and Accurate Dependency Parser using Neural Networks",
        "year": 2014,
        "authors": [
          "Danqi Chen",
          "Christopher D. Manning"
        ],
        "task": "Dependency Parsing",
        "dataset": [
          "PTB_2014",
          "CTB_2014"
        ],
        "metrics": [
          "UAS_Parsing",
          "LAS_Parsing",
          "Parsing_Speed"
        ],
        "architecture": {
          "components": [
            "Word Embeddings",
            "POS Tag Embeddings",
            "Arc Label Embeddings",
            "Hidden Layer",
            "Softmax Layer"
          ],
          "connections": [
            "Input Layer -> Hidden Layer",
            "Hidden Layer -> Softmax Layer"
          ],
          "mechanisms": [
            "Cube Activation Function",
            "Pre-trained Word Embeddings Initialization",
            "POS and Arc Label Embeddings"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Mini-batched AdaGrad",
            "Dropout"
          ],
          "parameter_tuning": [
            "Embedding Size",
            "Hidden Layer Size",
            "Regularization Parameter",
            "Initial Learning Rate"
          ]
        },
        "feature_processing": [
          "Dense Feature Representation",
          "Pre-computation Trick"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PTB_2014",
        "entity_type": "Dataset",
        "name": "English Penn Treebank",
        "description": "Standard dataset for English syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 39832,
        "year": 2014,
        "creators": [
          "Johansson, R.",
          "Nugues, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CTB_2014",
        "entity_type": "Dataset",
        "name": "Chinese Penn Treebank",
        "description": "Standard dataset for Chinese syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 16091,
        "year": 2014,
        "creators": [
          "Zhang, Y.",
          "Clark, S."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "UAS_Parsing",
        "entity_type": "Metric",
        "name": "Unlabeled Attachment Score",
        "description": "Percentage of words with correct head",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correctly attached words / Total words"
      }
    },
    {
      "metric_entity": {
        "metric_id": "LAS_Parsing",
        "entity_type": "Metric",
        "name": "Labeled Attachment Score",
        "description": "Percentage of words with correct head and label",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correctly attached and labeled words / Total words"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Parsing_Speed",
        "entity_type": "Metric",
        "name": "Parsing Speed",
        "description": "Number of sentences parsed per second",
        "category": "Dependency Parsing Efficiency",
        "formula": "Sentences parsed / Time taken"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Raghunathan2010_MultiPassSieve",
        "entity_type": "Algorithm",
        "name": "Multi-Pass Sieve",
        "title": "A Multi-Pass Sieve for Coreference Resolution",
        "year": 2010,
        "authors": [
          "Karthik Raghunathan",
          "Heeyoung Lee",
          "Sudarshan Rangarajan",
          "Nathanael Chambers",
          "Mihai Surdeanu",
          "Dan Jurafsky",
          "Christopher Manning"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE2004-ROTH-DEV2",
          "ACE2004-CULOTTA-TEST",
          "ACE2004-NWIRE",
          "MUC6-TEST"
        ],
        "metrics": [
          "Pairwise F1",
          "MUC",
          "B3"
        ],
        "architecture": {
          "components": [
            "Pass 1- Exact Match",
            "Pass 2- Precise Constructs",
            "Pass 3- Strict Head Matching",
            "Pass 4- Variants of Strict Head",
            "Pass 5- Variants of Strict Head",
            "Pass 6- Relaxed Head Matching",
            "Pass 7- Pronouns"
          ],
          "connections": [
            "Each pass builds on the previous pass's entity cluster output"
          ],
          "mechanisms": [
            "Attribute sharing",
            "Mention selection",
            "Search pruning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Unsupervised",
            "Deterministic"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Syntactic information",
          "Attribute sharing",
          "Cluster-level features",
          "Acronym detection",
          "Demonym detection",
          "Animacy detection"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-ROTH-DEV2_2004",
        "entity_type": "Dataset",
        "name": "ACE2004-ROTH-DEV2",
        "description": "Development split of Bengston and Roth(2008) from the 2004 Automatic Content Extraction (ACE) evaluation",
        "domain": "Natural Language Processing",
        "size": 68,
        "year": 2004,
        "creators": [
          "Bengston and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-CULOTTA-TEST_2004",
        "entity_type": "Dataset",
        "name": "ACE2004-CULOTTA-TEST",
        "description": "Partition of ACE 2004 corpus reserved for testing by several previous works",
        "domain": "Natural Language Processing",
        "size": 107,
        "year": 2004,
        "creators": [
          "Culotta et al.",
          "Bengston and Roth",
          "Haghighi and Klein"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-NWIRE_2004",
        "entity_type": "Dataset",
        "name": "ACE2004-NWIRE",
        "description": "Newswire subset of the ACE 2004 corpus",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": 2004,
        "creators": [
          "Poon and Domingos",
          "Haghighi and Klein"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MUC6-TEST_1995",
        "entity_type": "Dataset",
        "name": "MUC6-TEST",
        "description": "Test corpus from the sixth Message Understanding Conference (MUC-6) evaluation",
        "domain": "Natural Language Processing",
        "size": 30,
        "year": 1995,
        "creators": [
          "Vilain et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Pairwise_F1",
        "entity_type": "Metric",
        "name": "Pairwise F1",
        "description": "Computed over mention pairs in the same entity cluster",
        "category": "Coreference Resolution",
        "formula": "2 * (precision * recall) / (precision + recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "MUC_Clustering",
        "entity_type": "Metric",
        "name": "MUC",
        "description": "Measures how many predicted clusters need to be merged to cover the gold clusters",
        "category": "Coreference Resolution",
        "formula": "Not specified in the paper"
      }
    },
    {
      "metric_entity": {
        "metric_id": "B3_Clustering",
        "entity_type": "Metric",
        "name": "B3",
        "description": "Uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the predicted and gold clusters as denominators for precision and recall",
        "category": "Coreference Resolution",
        "formula": "Not specified in the paper"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
        "entity_type": "Algorithm",
        "name": "Structured Self-attentive Sentence Embedding",
        "year": 2017,
        "authors": [
          "Zhouhan Lin",
          "Minwei Feng",
          "Cicero Nogueira dos Santos",
          "Mo Yu",
          "Bing Xiang",
          "Bowen Zhou",
          "Yoshua Bengio"
        ],
        "task": "Sentence Representation",
        "dataset": [
          "Age dataset",
          "Yelp dataset",
          "SNLI Corpus"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM",
            "Self-Attention Mechanism",
            "Penalization Term"
          ],
          "connections": [
            "LSTM hidden states to attention mechanism",
            "Attention mechanism to sentence embedding"
          ],
          "mechanisms": [
            "Self-attention",
            "Penalization for diversity"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent",
            "AdaGrad"
          ],
          "parameter_tuning": [
            "Learning rate",
            "Batch size",
            "Dropout",
            "L2 regularization",
            "Penalization term coefficient"
          ]
        },
        "feature_processing": [
          "Word embeddings",
          "Max pooling",
          "Averaging"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Age_dataset_2017",
        "entity_type": "Dataset",
        "name": "Age dataset",
        "description": "Twitter tweets in English, Spanish, and Dutch with age and gender labels",
        "domain": "Social Media",
        "size": 76485,
        "year": 2017,
        "creators": [
          "PAN 2016 Author Profiling Task"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Yelp_dataset_2017",
        "entity_type": "Dataset",
        "name": "Yelp dataset",
        "description": "2.7M Yelp reviews with star ratings",
        "domain": "Sentiment Analysis",
        "size": 2700000,
        "year": 2017,
        "creators": [
          "Yelp"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SNLI_Corpus_2015",
        "entity_type": "Dataset",
        "name": "SNLI Corpus",
        "description": "570k human-written English sentence pairs manually labeled for entailment, contradiction, and neutral",
        "domain": "Textual Entailment",
        "size": 570000,
        "year": 2015,
        "creators": [
          "Bowman et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Classification",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "分类准确率",
        "category": "分类评估",
        "formula": "正确分类样本数/总样本数"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "BiLSTM_MaxPooling_MLP",
        "entity_type": "Algorithm",
        "name": "BiLSTM with Max Pooling and MLP",
        "year": 2017,
        "authors": [
          "Zhouhan Lin",
          "Minwei Feng",
          "Cicero Nogueira dos Santos",
          "Mo Yu",
          "Bing Xiang",
          "Bowen Zhou",
          "Yoshua Bengio"
        ],
        "task": "Sentence Representation",
        "dataset": [
          "Age dataset",
          "Yelp dataset"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM",
            "Max Pooling",
            "Multilayer Perceptron"
          ],
          "connections": [
            "LSTM hidden states to max pooling",
            "Max pooling to MLP"
          ],
          "mechanisms": [
            "Max pooling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Learning rate",
            "Batch size",
            "Dropout",
            "L2 regularization"
          ]
        },
        "feature_processing": [
          "Word embeddings"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "CNN_MaxPooling_MLP",
        "entity_type": "Algorithm",
        "name": "CNN with Max Pooling and MLP",
        "year": 2017,
        "authors": [
          "Zhouhan Lin",
          "Minwei Feng",
          "Cicero Nogueira dos Santos",
          "Mo Yu",
          "Bing Xiang",
          "Bowen Zhou",
          "Yoshua Bengio"
        ],
        "task": "Sentence Representation",
        "dataset": [
          "Age dataset",
          "Yelp dataset"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Convolutional Neural Network",
            "Max Pooling",
            "Multilayer Perceptron"
          ],
          "connections": [
            "CNN hidden states to max pooling",
            "Max pooling to MLP"
          ],
          "mechanisms": [
            "Max pooling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Learning rate",
            "Batch size",
            "Dropout",
            "L2 regularization"
          ]
        },
        "feature_processing": [
          "Word embeddings"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Liang2016_TagBasedSolver",
        "entity_type": "Algorithm",
        "name": "Tag-based statistical math word problem solver",
        "title": "A Tag-based English Math Word Problem Solver with Understanding, Reasoning and Explanation",
        "year": 2016,
        "authors": [
          "Chao-Chun Liang",
          "Kuang-Yi Hsu",
          "Chien-Tsung Huang",
          "Chung-Min Li",
          "Shen-Yu Miao",
          "Keh-Yih Su"
        ],
        "task": "Solving math word problems",
        "dataset": [
          "MA1_2014",
          "MA2_2014",
          "IXL_2014"
        ],
        "metrics": [
          "Accuracy_Classification",
          "Solution_Type_Accuracy"
        ],
        "architecture": {
          "components": [
            "Language Analyzer",
            "Solution Type Classifier",
            "Logic Form Converter",
            "Inference Engine",
            "Explanation Generator"
          ],
          "connections": [
            "LA -> STC",
            "STC -> LFC",
            "LFC -> IE",
            "IE -> EG"
          ],
          "mechanisms": [
            "Tag-based annotation",
            "First-order logic predicates",
            "Logic inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised learning with SVM classifier"
          ],
          "parameter_tuning": [
            "Linear kernel functions"
          ]
        },
        "feature_processing": [
          "Verb category features",
          "Keyword indicators",
          "Pattern-matching indicators"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MA1_2014",
        "entity_type": "Dataset",
        "name": "MA1",
        "description": "Simple math word problems on addition and subtraction for third, fourth, and fifth graders",
        "domain": "Mathematics education",
        "size": 395,
        "year": 2014,
        "creators": [
          "M. J. Hosseini",
          "H. Hajishirzi",
          "O. Etzioni",
          "N. Kushman"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MA2_2014",
        "entity_type": "Dataset",
        "name": "MA2",
        "description": "Math word problems with more irrelevant information",
        "domain": "Mathematics education",
        "size": 395,
        "year": 2014,
        "creators": [
          "M. J. Hosseini",
          "H. Hajishirzi",
          "O. Etzioni",
          "N. Kushman"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IXL_2014",
        "entity_type": "Dataset",
        "name": "IXL",
        "description": "Math word problems with more information gaps",
        "domain": "Mathematics education",
        "size": 395,
        "year": 2014,
        "creators": [
          "M. J. Hosseini",
          "H. Hajishirzi",
          "O. Etzioni",
          "N. Kushman"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Solution_Type_Accuracy",
        "entity_type": "Metric",
        "name": "Solution Type Accuracy",
        "description": "Accuracy of identifying the correct solution type",
        "category": "Solution type classification",
        "formula": "Correct solution types / Total solution types"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goldberg2010_EasyFirstNonDirectionalParser",
        "entity_type": "Algorithm",
        "name": "Easy-First Non-Directional Dependency Parsing",
        "title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing",
        "year": 2010,
        "authors": [
          "Yoav Goldberg",
          "Michael Elhadad"
        ],
        "task": "Dependency Parsing",
        "dataset": [
          "WSJ Treebank",
          "CoNLL 2007 English dataset"
        ],
        "metrics": [
          "Accuracy",
          "Root",
          "Complete"
        ],
        "architecture": {
          "components": [
            "ATTACHLEFT",
            "ATTACHRIGHT"
          ],
          "connections": [
            "Dependency edges"
          ],
          "mechanisms": [
            "Non-directional parsing",
            "Best-first parsing",
            "Greedy algorithm"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Structured perceptron"
          ],
          "parameter_tuning": [
            "Feature representation",
            "Weight vector updates"
          ]
        },
        "feature_processing": [
          "Binary valued features",
          "POS tags",
          "Head word forms",
          "Left-most and right-most children POS tags",
          "Preposition and coordinator lexicalization",
          "Structural features",
          "Unigram features",
          "Bigram features",
          "PP-attachment features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WSJ_Treebank_2010",
        "entity_type": "Dataset",
        "name": "WSJ Treebank",
        "description": "Wall Street Journal corpus converted to dependency structures",
        "domain": "Natural Language Processing",
        "size": "Sections 2-21 for training, Section 22 for development, Section 23 for testing",
        "year": 2010,
        "creators": [
          "Yamada and Matsumoto"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CoNLL_2007_English_dataset_2007",
        "entity_type": "Dataset",
        "name": "CoNLL 2007 English dataset",
        "description": "English dataset derived from WSJ Treebank with a different conversion procedure",
        "domain": "Natural Language Processing",
        "size": "Smaller in size compared to WSJ Treebank",
        "year": 2007,
        "creators": [
          "CoNLL 2007 Shared Task"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Root_Prediction",
        "entity_type": "Metric",
        "name": "Root",
        "description": "Percentage of sentences in which the ROOT attachment is correct",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correct ROOT attachments / Total sentences"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Complete_Parsing",
        "entity_type": "Metric",
        "name": "Complete",
        "description": "Percentage of sentences in which all tokens were assigned their correct parent",
        "category": "Dependency Parsing Evaluation",
        "formula": "Sentences with all correct token assignments / Total sentences"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Earley1970_EfficientParsingAlgorithm",
        "entity_type": "Algorithm",
        "name": "Efficient Context-Free Parsing Algorithm",
        "title": "An Efficient Context-Free Parsing Algorithm",
        "year": 1970,
        "authors": [
          "Jay Earley"
        ],
        "task": "Parsing context-free grammars",
        "dataset": [],
        "metrics": [
          "Time complexity",
          "Space complexity"
        ],
        "architecture": {
          "components": [
            "Predictor",
            "Completer",
            "Scanner"
          ],
          "connections": [
            "State transitions",
            "Look-ahead"
          ],
          "mechanisms": [
            "State sets",
            "Derivation trees"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Look-ahead strings",
          "State representation"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "TimeComplexity_Parsing",
        "entity_type": "Metric",
        "name": "Time complexity",
        "description": "Time required to parse a string",
        "category": "Algorithm performance",
        "formula": "O(n^3) for general context-free grammars, O(n^2) for unambiguous grammars, O(n) for bounded state grammars"
      }
    },
    {
      "metric_entity": {
        "metric_id": "SpaceComplexity_Parsing",
        "entity_type": "Metric",
        "name": "Space complexity",
        "description": "Memory required to parse a string",
        "category": "Algorithm performance",
        "formula": "O(n^2)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Alvin2015_GeoTutor",
        "entity_type": "Algorithm",
        "name": "GeoTutor",
        "title": "Automatic Synthesis of Geometry Problems for an Intelligent Tutoring System",
        "year": 2015,
        "authors": [
          "Alvin, C.",
          "Gulwani, S.",
          "Majumdar, R.",
          "Mukhopadhyay, S."
        ],
        "task": "Euclidean Geometry Problem Synthesis",
        "dataset": [
          "High School Geometry Problems_2015"
        ],
        "metrics": [
          "Interesting_Problems",
          "Strictly_Interesting_Problems",
          "Converse_Problems",
          "Proof_Width",
          "Proof_Length",
          "Deductive_Steps"
        ],
        "architecture": {
          "components": [
            "Hypergraph Construction",
            "Pebbling Algorithm",
            "Problem Synthesis"
          ],
          "connections": [
            "Forward Edges",
            "Back-Edges"
          ],
          "mechanisms": [
            "Traversal Algorithm",
            "Coarse Problem Homomorphism",
            "Goal Analogous Problems"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Breadth-First Pebbling",
            "User Query-Based Restrictions"
          ],
          "parameter_tuning": [
            "Deductive Steps Range",
            "Proof Width Range",
            "Proof Length Range"
          ]
        },
        "feature_processing": [
          "Coordinate-Based Computation",
          "Assumption Filtering"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "High_School_Geometry_Problems_2015",
        "entity_type": "Dataset",
        "name": "High School Geometry Problems",
        "description": "A corpus of high school geometry problems from standard geometry textbooks",
        "domain": "Education",
        "size": 155,
        "year": 2015,
        "creators": [
          "Sinclair et al.",
          "Boyd et al.",
          "Larson et al.",
          "Jurgensen et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Interesting_Problems",
        "entity_type": "Metric",
        "name": "Interesting Problems",
        "description": "Problems that require at least one or more of the assumptions and are minimal with respect to the goal",
        "category": "Problem Synthesis Evaluation",
        "formula": "Number of problems that meet the criteria for interestingness"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Strictly_Interesting_Problems",
        "entity_type": "Metric",
        "name": "Strictly Interesting Problems",
        "description": "Problems that use all the assumptions in the statement for their solution",
        "category": "Problem Synthesis Evaluation",
        "formula": "Number of problems that strictly use all assumptions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Converse_Problems",
        "entity_type": "Metric",
        "name": "Converse Problems",
        "description": "Problems that reverse the goal and assumptions of the original problem",
        "category": "Problem Synthesis Evaluation",
        "formula": "Number of converse problems generated"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Proof_Width",
        "entity_type": "Metric",
        "name": "Proof Width",
        "description": "Width of the problem hypergraph",
        "category": "Problem Complexity",
        "formula": "Maximum number of nodes at any level of the hypergraph"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Proof_Length",
        "entity_type": "Metric",
        "name": "Proof Length",
        "description": "Diameter of the problem hypergraph",
        "category": "Problem Complexity",
        "formula": "Longest path from source to goal in the hypergraph"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Deductive_Steps",
        "entity_type": "Metric",
        "name": "Deductive Steps",
        "description": "Number of hyperedges in the problem hypergraph",
        "category": "Problem Complexity",
        "formula": "Total number of deductive steps required to solve the problem"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Shi2015_SigmaDolphin",
        "entity_type": "Algorithm",
        "name": "SigmaDolphin",
        "title": "Automatically Solving Number Word Problems by Semantic Parsing and Reasoning",
        "year": 2015,
        "authors": [
          "Shi, Shuming",
          "Wang, Yuehui",
          "Lin, Chin-Yew",
          "Liu, Xiaojiang",
          "Rui, Yong"
        ],
        "task": "自动求解数学文字题",
        "dataset": [
          "Algebra.com_2015",
          "YahooAnswers.com_2015"
        ],
        "metrics": [
          "Precision_Classification",
          "Recall_Classification",
          "F1_Classification"
        ],
        "architecture": {
          "components": [
            "CFG Parser",
            "Reasoning Module",
            "Semantic Representation Language DOL"
          ],
          "connections": [
            "NL Text -> DOL Trees -> Math Expressions"
          ],
          "mechanisms": [
            "Context-Free Grammar Parsing",
            "Semantic Interpretation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CFG规则创建",
            "半自动化语法构建"
          ],
          "parameter_tuning": [
            "无"
          ]
        },
        "feature_processing": [
          "自然语言文本解析",
          "数学表达式推导"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Algebra.com_2015",
        "entity_type": "Dataset",
        "name": "Algebra.com",
        "description": "一个用户发布数学问题并获得导师帮助的网站",
        "domain": "数学教育",
        "size": 1878,
        "year": 2015,
        "creators": [
          "Algebra.com"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "YahooAnswers.com_2015",
        "entity_type": "Dataset",
        "name": "Yahoo Answers",
        "description": "一个问答网站，其中包含数学问题",
        "domain": "数学教育",
        "size": 1878,
        "year": 2015,
        "creators": [
          "Yahoo Answers"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Precision_Classification",
        "entity_type": "Metric",
        "name": "Precision",
        "description": "精确率",
        "category": "分类评估",
        "formula": "正确分类样本数 / 总分类样本数"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall_Classification",
        "entity_type": "Metric",
        "name": "Recall",
        "description": "召回率",
        "category": "分类评估",
        "formula": "正确分类样本数 / 总实际正样本数"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Classification",
        "entity_type": "Metric",
        "name": "F1",
        "description": "F1分数",
        "category": "分类评估",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kushman2014_KAZB",
        "entity_type": "Algorithm",
        "name": "KAZB",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": 2014,
        "authors": [
          "Kushman, N.",
          "Artzi, Y.",
          "Zettlemoyer, L.",
          "Barzilay, R."
        ],
        "task": "自动求解代数文字题",
        "dataset": [
          "LinearT2_2015",
          "LinearT6_2015"
        ],
        "metrics": [
          "Precision_Classification",
          "Recall_Classification",
          "F1_Classification"
        ],
        "architecture": {
          "components": [
            "Equation Templates",
            "Learning-Based Statistical Method"
          ],
          "connections": [
            "问题映射到方程模板"
          ],
          "mechanisms": [
            "方程模板匹配"
          ]
        },
        "methodology": {
          "training_strategy": [
            "方程模板确定"
          ],
          "parameter_tuning": [
            "无"
          ]
        },
        "feature_processing": [
          "问题特征提取"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hosseini2014_VerbCategorization",
        "entity_type": "Algorithm",
        "name": "Verb Categorization",
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "year": 2014,
        "authors": [
          "Hosseini, M.J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ],
        "task": "自动求解算术文字题",
        "dataset": [
          "未指定"
        ],
        "metrics": [
          "未指定"
        ],
        "architecture": {
          "components": [
            "动词分类"
          ],
          "connections": [
            "问题映射到动词分类"
          ],
          "mechanisms": [
            "动词分类学习"
          ]
        },
        "methodology": {
          "training_strategy": [
            "动词分类学习"
          ],
          "parameter_tuning": [
            "无"
          ]
        },
        "feature_processing": [
          "问题特征提取"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_Aristo",
        "entity_type": "Algorithm",
        "name": "Aristo",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "Elementary Science Question Answering",
        "dataset": [
          "NY Regents Science Exam_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "IR Solver",
            "PMI Solver",
            "SVM Solver",
            "RULE Solver",
            "ILP Solver"
          ],
          "connections": [
            "Logistic Regression Combiner"
          ],
          "mechanisms": [
            "Information Retrieval",
            "Pointwise Mutual Information",
            "Support Vector Machine",
            "Rule-based Reasoning",
            "Integer Linear Programming"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Logistic Regression Calibration"
          ],
          "parameter_tuning": [
            "Solver-specific Feature Weights"
          ]
        },
        "feature_processing": [
          "TF-IDF Scoring",
          "Lexical Chunk Matching",
          "Syntactic Pattern Extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NY_Regents_Science_Exam_2016",
        "entity_type": "Dataset",
        "name": "NY Regents Science Exam",
        "description": "Standardized science exams for 4th grade students",
        "domain": "Elementary Education",
        "size": 237,
        "year": 2016,
        "creators": [
          "New York State Education Department"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_IRSolver",
        "entity_type": "Algorithm",
        "name": "IR Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "Elementary Science Question Answering",
        "dataset": [
          "NY Regents Science Exam_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Lucene Search Engine"
          ],
          "connections": [],
          "mechanisms": [
            "Information Retrieval"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Non-stopword Overlap"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_PMISolver",
        "entity_type": "Algorithm",
        "name": "PMI Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "Elementary Science Question Answering",
        "dataset": [
          "NY Regents Science Exam_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Pointwise Mutual Information Calculation"
          ],
          "connections": [],
          "mechanisms": [
            "Statistical Association"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Unigrams, Bigrams, Trigrams, Skip-Bigrams"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_SVMSolver",
        "entity_type": "Algorithm",
        "name": "SVM Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "Elementary Science Question Answering",
        "dataset": [
          "NY Regents Science Exam_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Support Vector Machine"
          ],
          "connections": [],
          "mechanisms": [
            "Word Embeddings",
            "Cosine Similarity"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SVM Ranker"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Domain-appropriate Embeddings"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_RULESolver",
        "entity_type": "Algorithm",
        "name": "RULE Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "Elementary Science Question Answering",
        "dataset": [
          "NY Regents Science Exam_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Probabilistic First-Order Logic Rules"
          ],
          "connections": [],
          "mechanisms": [
            "Rule-based Reasoning",
            "Textual Entailment"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Syntactic Structure Mapping"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Clark2016_ILPSolver",
        "entity_type": "Algorithm",
        "name": "ILP Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": 2016,
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "Elementary Science Question Answering",
        "dataset": [
          "NY Regents Science Exam_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Integer Linear Programming"
          ],
          "connections": [],
          "mechanisms": [
            "Structured Knowledge Representation",
            "Proof Graph Construction"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Lexical Chunk Matching",
          "Table Joining"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Khot2015_Praline",
        "entity_type": "Algorithm",
        "name": "Praline",
        "title": "Exploring Markov Logic Networks for Question Answering",
        "year": 2015,
        "authors": [
          "Tushar Khot",
          "Niranjan Balasubramanian",
          "Erik Gribkoff",
          "Ashish Sabharwal",
          "Peter Clark",
          "Oren Etzioni"
        ],
        "task": "Elementary Science Question Answering",
        "dataset": [
          "NY Regents Science Exam_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Markov Logic Networks"
          ],
          "connections": [],
          "mechanisms": [
            "Lexical Alignment",
            "Controlled Inference"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Robaidek2018_BiLSTMClassifier",
        "entity_type": "Algorithm",
        "name": "BiLSTM Classifier",
        "year": 2018,
        "authors": [
          "Robaidek, B.",
          "Koncel-Kedziorski, R.",
          "Hajishirzi, H."
        ],
        "task": "Solving Algebra Word Problems",
        "dataset": [
          "DRAW_2016",
          "MAWPS_2016",
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM",
            "Softmax Layer"
          ],
          "connections": [
            "Word Embedding -> BiLSTM -> Softmax"
          ],
          "mechanisms": [
            "Cross Entropy Loss"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-end Training"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Hidden State Size"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lin2017_StructuredSelfAttention",
        "entity_type": "Algorithm",
        "name": "Structured Self-Attention",
        "year": 2017,
        "authors": [
          "Lin, Z.",
          "Feng, M.",
          "Santos, C.N.d.",
          "Yu, M.",
          "Xiang, B.",
          "Zhou, B.",
          "Bengio, Y."
        ],
        "task": "Solving Algebra Word Problems",
        "dataset": [
          "DRAW_2016",
          "MAWPS_2016",
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM",
            "Multi-hop Self-Attention"
          ],
          "connections": [
            "Word Embedding -> BiLSTM -> Self-Attention -> Fixed Sized Embedding"
          ],
          "mechanisms": [
            "Redundancy Reduction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-end Training"
          ],
          "parameter_tuning": [
            "Attention Hop Constraints"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sutskever2014_Seq2Seq",
        "entity_type": "Algorithm",
        "name": "Seq2Seq Model",
        "year": 2014,
        "authors": [
          "Sutskever, I.",
          "Vinyals, O.",
          "Le, Q.V."
        ],
        "task": "Solving Algebra Word Problems",
        "dataset": [
          "DRAW_2016",
          "MAWPS_2016",
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Generation"
        ],
        "architecture": {
          "components": [
            "Encoder LSTM",
            "Decoder LSTM",
            "Attention Mechanism"
          ],
          "connections": [
            "Word Embedding -> Encoder LSTM -> Attention -> Decoder LSTM"
          ],
          "mechanisms": [
            "Attention Mechanism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Sequence Prediction"
          ],
          "parameter_tuning": [
            "Hidden State Size",
            "Dropout Rate"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DRAW_2016",
        "entity_type": "Dataset",
        "name": "DRAW",
        "year": 2016,
        "creators": [
          "Upadhyay, S.",
          "Chang, M.W."
        ],
        "domain": "Algebra Word Problems",
        "size": 1000,
        "description": "A challenging and diverse algebra word problem set"
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MAWPS_2016",
        "entity_type": "Dataset",
        "name": "MAWPS",
        "year": 2016,
        "creators": [
          "Koncel-Kedziorski, R.",
          "Roy, S.",
          "Amini, A.",
          "Kushman, N.",
          "Hajishirzi, H."
        ],
        "domain": "Algebra Word Problems",
        "size": 2373,
        "description": "A math word problem repository"
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Math23K_2017",
        "entity_type": "Dataset",
        "name": "Math23K",
        "year": 2017,
        "creators": [
          "Wang, Y.",
          "Liu, X.",
          "Shi, S."
        ],
        "domain": "Algebra Word Problems",
        "size": 23164,
        "description": "A large dataset of Chinese algebra word problems"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Generation",
        "entity_type": "Metric",
        "name": "Accuracy",
        "category": "Generation",
        "description": "Proportion of correctly generated equation templates",
        "formula": "Correct Generated Templates / Total Templates"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Oracle_Accuracy",
        "entity_type": "Metric",
        "name": "Oracle Accuracy",
        "category": "Upper Bound",
        "description": "Number of test equation templates which appear in the training data",
        "formula": "Test Templates in Training Data / Total Test Templates"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_RNNBasedSeq2SeqModel",
        "entity_type": "Algorithm",
        "name": "RNN-based Seq2Seq Model",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": 2017,
        "authors": [
          "Wang, Y.",
          "Liu, X.",
          "Shi, S."
        ],
        "task": "自动求解数学文字题",
        "dataset": [
          "Math23K_2017",
          "Alg514_2014"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "GRU",
            "LSTM"
          ],
          "connections": [
            "编码器-解码器"
          ],
          "mechanisms": [
            "门控循环单元",
            "长短期记忆网络"
          ]
        },
        "methodology": {
          "training_strategy": [
            "dropout",
            "mini-batch"
          ],
          "parameter_tuning": [
            "学习率",
            "dropout概率"
          ]
        },
        "feature_processing": [
          "显著数字识别"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_HybridModel",
        "entity_type": "Algorithm",
        "name": "Hybrid Model",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": 2017,
        "authors": [
          "Wang, Y.",
          "Liu, X.",
          "Shi, S."
        ],
        "task": "自动求解数学文字题",
        "dataset": [
          "Math23K_2017",
          "Alg514_2014"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "RNN-based Seq2Seq Model",
            "检索模型"
          ],
          "connections": [
            "基于相似度的选择机制"
          ],
          "mechanisms": [
            "Jaccard相似度"
          ]
        },
        "methodology": {
          "training_strategy": [
            "阈值设置"
          ],
          "parameter_tuning": [
            "相似度阈值"
          ]
        },
        "feature_processing": [
          "显著数字识别"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_SignificantNumberIdentification",
        "entity_type": "Algorithm",
        "name": "Significant Number Identification",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": 2017,
        "authors": [
          "Wang, Y.",
          "Liu, X.",
          "Shi, S."
        ],
        "task": "识别数学文字题中的重要数字",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "LSTM"
          ],
          "connections": [
            "单层LSTM"
          ],
          "mechanisms": [
            "二元分类模型"
          ]
        },
        "methodology": {
          "training_strategy": [
            "上下文窗口"
          ],
          "parameter_tuning": [
            "节点数量"
          ]
        },
        "feature_processing": [
          "数字及其上下文"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Alg514_2014",
        "entity_type": "Dataset",
        "name": "Alg514",
        "description": "包含514个线性代数问题的数据集",
        "domain": "自然语言处理",
        "size": 514,
        "year": 2014,
        "creators": [
          "Kushman, N.",
          "Artzi, Y.",
          "Zettlemoyer, L.",
          "Barzilay, R."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Seo2014_G-ALIGNER",
        "entity_type": "Algorithm",
        "name": "G-ALIGNER",
        "title": "Diagram Understanding in Geometry Questions",
        "year": 2014,
        "authors": [
          "Min Joon Seo",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Oren Etzioni"
        ],
        "task": "Diagram Understanding",
        "dataset": [
          "Geometry Questions Dataset_2014"
        ],
        "metrics": [
          "F1 Score_Identifying Primitives",
          "F1 Score_Aligning Visual Elements"
        ],
        "architecture": {
          "components": [
            "Primitive Detection",
            "Textual Mention Extraction",
            "Alignment Constraint Function"
          ],
          "connections": [
            "Coupling Textual and Visual Information",
            "Submodular Optimization"
          ],
          "mechanisms": [
            "Hough Transform",
            "Submodular Objective Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Iterative Optimization",
            "Greedy Approximation"
          ],
          "parameter_tuning": [
            "Not Required"
          ]
        },
        "feature_processing": [
          "OCR for Label Positioning",
          "Harris Corner Detector"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Geometry Questions Dataset_2014",
        "entity_type": "Dataset",
        "name": "Geometry Questions Dataset",
        "description": "Dataset of geometry questions including textual descriptions and diagrams",
        "domain": "Geometry",
        "size": 100,
        "year": 2014,
        "creators": [
          "Min Joon Seo",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Oren Etzioni"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1 Score_Identifying Primitives",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall for identifying primitives",
        "category": "Identifying Primitives",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1 Score_Aligning Visual Elements",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall for aligning visual elements with textual mentions",
        "category": "Aligning Visual Elements",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Watanabe1991_IntegrationFramework",
        "entity_type": "Algorithm",
        "name": "Integration Framework",
        "title": "Diagram Understanding Using Integration of Layout Information and Textual Information",
        "year": 1991,
        "authors": [
          "Watanabe, Yasuhiko",
          "Nagao, Makoto"
        ],
        "task": "Diagram Understanding",
        "dataset": [
          "PBF Diagrams"
        ],
        "metrics": [
          "Semantic Analysis Success Rate"
        ],
        "architecture": {
          "components": [
            "Layout Information",
            "Natural Language Information"
          ],
          "connections": [
            "Connection",
            "Adjacency"
          ],
          "mechanisms": [
            "Symbol-based Classification",
            "Expression Pattern Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Manual Annotation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Japanese Morphological Analysis",
          "Pattern Matching"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PBF_Diagrams_1991",
        "entity_type": "Dataset",
        "name": "PBF Diagrams",
        "description": "Diagrams from Pictorial Books of Flora",
        "domain": "Botanical Illustrations",
        "size": 31,
        "year": 1991,
        "creators": [
          "Watanabe, Yasuhiko",
          "Nagao, Makoto"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "SemanticAnalysisSuccessRate_Classification",
        "entity_type": "Metric",
        "name": "Semantic Analysis Success Rate",
        "description": "Rate of successful semantic interpretation of diagram elements",
        "category": "Classification Evaluation",
        "formula": "Number of correctly interpreted elements / Total number of elements"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2016_DimensionallyGuidedSynthesis",
        "entity_type": "Algorithm",
        "name": "Dimensionally Guided Synthesis",
        "title": "Dimensionally Guided Synthesis of Mathematical Word Problems",
        "year": 2016,
        "authors": [
          "Ke Wang",
          "Zhendong Su"
        ],
        "task": "Mathematical Word Problem Generation",
        "dataset": [],
        "metrics": [
          "Statistical Indistinguishability",
          "Error Rate"
        ],
        "architecture": {
          "components": [
            "Equation Generator",
            "Narrative Generator"
          ],
          "connections": [
            "Equation Generation Procedure",
            "Binary Expression Tree Representation"
          ],
          "mechanisms": [
            "Dimensional Unit Assignment",
            "Recursive Narrative Construction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Random Equation Generation",
            "Variable Unrolling"
          ],
          "parameter_tuning": [
            "Dimensional Units",
            "Operational Rules"
          ]
        },
        "feature_processing": [
          "Dimensional Unit Classification",
          "Keyword Assignment",
          "Sub-story Generation"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "StatisticalIndistinguishability_Authenticity",
        "entity_type": "Metric",
        "name": "Statistical Indistinguishability",
        "description": "衡量生成的问题与教科书问题在统计上是否无法区分",
        "category": "问题真实性评估",
        "formula": "通过配对T检验和卡方独立性检验进行评估"
      }
    },
    {
      "metric_entity": {
        "metric_id": "ErrorRate_Difficulty",
        "entity_type": "Metric",
        "name": "Error Rate",
        "description": "学生解答问题时的错误率",
        "category": "问题难度评估",
        "formula": "错误答案数量 / 总答案数量"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_SkipGram",
        "entity_type": "Algorithm",
        "name": "Skip-gram",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "Learning high-quality distributed vector representations",
        "dataset": [
          "News articles dataset"
        ],
        "metrics": [
          "Accuracy",
          "Syntactic accuracy",
          "Semantic accuracy"
        ],
        "architecture": {
          "components": [
            "Input layer",
            "Hidden layer",
            "Output layer"
          ],
          "connections": [
            "Input to hidden connections",
            "Hidden to output connections"
          ],
          "mechanisms": [
            "Negative Sampling",
            "Hierarchical Softmax",
            "Subsampling of frequent words"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Negative Sampling",
            "Hierarchical Softmax",
            "Subsampling of frequent words"
          ],
          "parameter_tuning": [
            "Dimensionality",
            "Context size",
            "Number of negative samples",
            "Subsampling rate"
          ]
        },
        "feature_processing": [
          "Word tokenization",
          "Phrase identification"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NewsArticles_2013",
        "entity_type": "Dataset",
        "name": "News articles dataset",
        "description": "A large dataset consisting of various news articles",
        "domain": "Natural Language Processing",
        "size": "One billion words",
        "year": 2013,
        "creators": [
          "Google Inc."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_AnalogicalReasoning",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Accuracy on the analogical reasoning task",
        "category": "Analogical reasoning evaluation",
        "formula": "Correct answers / Total questions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "SyntacticAccuracy_AnalogicalReasoning",
        "entity_type": "Metric",
        "name": "Syntactic accuracy",
        "description": "Accuracy on syntactic analogical reasoning",
        "category": "Analogical reasoning evaluation",
        "formula": "Correct syntactic analogies / Total syntactic questions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "SemanticAccuracy_AnalogicalReasoning",
        "entity_type": "Metric",
        "name": "Semantic accuracy",
        "description": "Accuracy on semantic analogical reasoning",
        "category": "Analogical reasoning evaluation",
        "formula": "Correct semantic analogies / Total semantic questions"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_HierarchicalSoftmax",
        "entity_type": "Algorithm",
        "name": "Hierarchical Softmax",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "Efficient computation of word probabilities",
        "dataset": [
          "News articles dataset"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary tree",
            "Inner nodes",
            "Leaf nodes"
          ],
          "connections": [
            "Path from root to leaf"
          ],
          "mechanisms": [
            "Logarithmic evaluation of nodes"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Binary Huffman tree"
          ],
          "parameter_tuning": [
            "Tree structure"
          ]
        },
        "feature_processing": [
          "Word frequency grouping"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_NegativeSampling",
        "entity_type": "Algorithm",
        "name": "Negative Sampling",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "Efficient training of word representations",
        "dataset": [
          "News articles dataset"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Target word",
            "Noise distribution"
          ],
          "connections": [
            "Logistic regression"
          ],
          "mechanisms": [
            "Logistic loss function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Noise distribution Pn(w)",
            "Unigram distribution U(w)^(3/4)"
          ],
          "parameter_tuning": [
            "Number of negative samples k"
          ]
        },
        "feature_processing": [
          "Word frequency grouping"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_PhraseSkipGram",
        "entity_type": "Algorithm",
        "name": "Phrase Skip-gram",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "Learning vector representations for phrases",
        "dataset": [
          "News articles dataset"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Input layer",
            "Hidden layer",
            "Output layer"
          ],
          "connections": [
            "Input to hidden connections",
            "Hidden to output connections"
          ],
          "mechanisms": [
            "Phrase identification",
            "Negative Sampling",
            "Hierarchical Softmax"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Phrase identification based on unigram and bigram counts"
          ],
          "parameter_tuning": [
            "Score threshold",
            "Vector dimensionality",
            "Context size"
          ]
        },
        "feature_processing": [
          "Phrase tokenization"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Slagle1965_DEDUCOM",
        "entity_type": "Algorithm",
        "name": "DEDUCOM",
        "title": "Experiments with a Deductive Question-Answering Program",
        "year": 1965,
        "authors": [
          "Slagle, J.R."
        ],
        "task": "Deductive Question-Answering",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Fact Interpreter",
            "Question Reducer",
            "Search Procedure"
          ],
          "connections": [
            "Fact Interpreter -> Question Reducer",
            "Question Reducer -> Search Procedure"
          ],
          "mechanisms": [
            "Depth-First Search",
            "Logical Deduction in Predicate Calculus"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Providing facts and rules"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Fact Representation",
          "Question Parsing"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Time_Efficiency",
        "entity_type": "Metric",
        "name": "Time Efficiency",
        "description": "Time taken by DEDUCOM to answer each question",
        "category": "Performance Evaluation",
        "formula": "Time taken to answer a question"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Yu2016_ImplicitQuantityRelationExtractor",
        "entity_type": "Algorithm",
        "name": "Implicit Quantity Relation Extractor",
        "title": "Extraction of Implicit Quantity Relations for Arithmetic Word Problems in Chinese",
        "year": 2016,
        "authors": [
          "Yu, Xinguo",
          "Jian, Pengpeng",
          "Wang, Mingshu",
          "Wu, Shuang"
        ],
        "task": "Extracting implicit quantity relations in arithmetic word problems",
        "dataset": [
          "Elementary school arithmetic application problem_2011",
          "Suzhou Education Publishing House_dataset"
        ],
        "metrics": [
          "Classification Accuracy"
        ],
        "architecture": {
          "components": [
            "Chinese phrase parse",
            "SVM classification",
            "Instantiation method of required general implicit quantity relations with semantic models"
          ],
          "connections": [
            "Chinese phrase parse -> SVM classification -> Instantiation method"
          ],
          "mechanisms": [
            "Semantic models",
            "Sequence alignment"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SVM with slack variable",
            "Bag of words feature extraction"
          ],
          "parameter_tuning": [
            "C value for slack variable"
          ]
        },
        "feature_processing": [
          "Normalization of common units",
          "Chinese phrase parsing",
          "Bag of words extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Elementary_school_arithmetic_application_problem_2011",
        "entity_type": "Dataset",
        "name": "Elementary school arithmetic application problem",
        "description": "Arithmetic word problems for elementary school students",
        "domain": "Education",
        "size": 627,
        "year": 2011,
        "creators": [
          "People's Education Press"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Suzhou_Education_Publishing_House_dataset",
        "entity_type": "Dataset",
        "name": "Suzhou Education Publishing House dataset",
        "description": "Arithmetic word problems for training",
        "domain": "Education",
        "year": 2016,
        "creators": [
          "Suzhou Education Publishing House"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Classification_Accuracy",
        "entity_type": "Metric",
        "name": "Classification Accuracy",
        "description": "Accuracy of classifying arithmetic word problems into different types",
        "category": "Classification evaluation",
        "formula": "Number of correctly classified samples / Total number of samples"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ma2010_FrameBasedCalculus",
        "entity_type": "Algorithm",
        "name": "Frame-Based Calculus",
        "title": "Frame-Based Calculus of solving Arithmetic Multi-Step Addition and Subtraction word problems",
        "year": 2010,
        "authors": [
          "Ma Yuhui",
          "Zhou Ying",
          "Cui Guangzuo",
          "Ren Yun",
          "Huang Ronghuai"
        ],
        "task": "Solving multi-step addition and subtraction word problems",
        "dataset": [],
        "metrics": [
          "Effectiveness of solving multi-step word problems"
        ],
        "architecture": {
          "components": [
            "MSWPAS-NP",
            "MSWPAS-CP"
          ],
          "connections": [
            "Natural language processing and frame construction",
            "Frame-based calculus"
          ],
          "mechanisms": [
            "Means-end Analysis",
            "Production rules"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Not applicable"
          ],
          "parameter_tuning": [
            "Not applicable"
          ]
        },
        "feature_processing": [
          "Comprehension and representation of word problems",
          "Problem-solving planning",
          "Trying to solve and assessment"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ChineseElementarySchoolWordProblems_2010",
        "entity_type": "Dataset",
        "name": "Chinese Elementary School Word Problems",
        "description": "Word problems gathered from four publishers in Chinese (one book from People’s Education Press, one book from Beijing Normal University Press, and two books from DONGBEI Normal University Press)",
        "domain": "Mathematics education",
        "size": "Not specified",
        "year": 2010,
        "creators": [
          "Various publishers"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Effectiveness_SolvingMultiStepWordProblems",
        "entity_type": "Metric",
        "name": "Effectiveness",
        "description": "Effectiveness of solving multi-step word problems",
        "category": "Problem-solving performance",
        "formula": "Not specified"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "deMarneffe2006_TypedDependencyExtraction",
        "entity_type": "Algorithm",
        "name": "Typed Dependency Extraction",
        "title": "Generating Typed Dependency Parses from Phrase Structure Parses",
        "year": 2006,
        "authors": [
          "Marie-Catherine de Marneffe",
          "Bill MacCartney",
          "Christopher D. Manning"
        ],
        "task": "Natural Language Processing",
        "dataset": [
          "Penn Treebank",
          "Brown Corpus"
        ],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Phrase Structure Parser",
            "Dependency Extraction Rules",
            "Dependency Typing Rules"
          ],
          "connections": [
            "Phrase Structure Trees -> Dependency Extraction -> Dependency Typing"
          ],
          "mechanisms": [
            "Head Identification",
            "Pattern Matching",
            "Collapsing Dependencies"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Treebank-trained Statistical Parsers"
          ],
          "parameter_tuning": [
            "Collins Head Rules",
            "Tregex Patterns"
          ]
        },
        "feature_processing": [
          "Semantic Head Retrieval",
          "Preposition and Conjunction Collapsing"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PennTreebank_1999",
        "entity_type": "Dataset",
        "name": "Penn Treebank",
        "description": "A widely used corpus for English syntactic parsing",
        "domain": "Natural Language Processing",
        "size": "Over 4.5 million words",
        "year": 1999,
        "creators": [
          "University of Pennsylvania"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "BrownCorpus_1964",
        "entity_type": "Dataset",
        "name": "Brown Corpus",
        "description": "A standard corpus of present-day edited American English",
        "domain": "Natural Language Processing",
        "size": "1 million words",
        "year": 1964,
        "creators": [
          "Henry Kucera",
          "W. Nelson Francis"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "DependencyAccuracy_Classification",
        "entity_type": "Metric",
        "name": "Dependency Accuracy",
        "description": "Accuracy of dependency relations in a sentence",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correctly identified dependencies / Total dependencies"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Collins1999_HeadDrivenStatisticalModels",
        "entity_type": "Algorithm",
        "name": "Head-Driven Statistical Models",
        "title": "Head-Driven Statistical Models for Natural Language Parsing",
        "year": 1999,
        "authors": [
          "Michael Collins"
        ],
        "task": "Natural Language Parsing",
        "dataset": [
          "Penn Treebank"
        ],
        "metrics": [
          "Parsing Accuracy"
        ],
        "architecture": {
          "components": [
            "Statistical Models",
            "Head Rules"
          ],
          "connections": [
            "Phrase Structure Trees -> Head Identification"
          ],
          "mechanisms": [
            "Statistical Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Treebank-trained Statistical Parsers"
          ],
          "parameter_tuning": [
            "Head Rules"
          ]
        },
        "feature_processing": [
          "Syntactic Head Retrieval"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Klein2003_AccurateUnlexicalizedParsing",
        "entity_type": "Algorithm",
        "name": "Accurate Unlexicalized Parsing",
        "title": "Accurate Unlexicalized Parsing",
        "year": 2003,
        "authors": [
          "Dan Klein",
          "Christopher D. Manning"
        ],
        "task": "Natural Language Parsing",
        "dataset": [
          "Penn Treebank"
        ],
        "metrics": [
          "Parsing Accuracy"
        ],
        "architecture": {
          "components": [
            "Unlexicalized Parsing Model"
          ],
          "connections": [
            "Phrase Structure Trees -> Parsing"
          ],
          "mechanisms": [
            "Unlexicalized Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Treebank-trained Statistical Parsers"
          ],
          "parameter_tuning": [
            "Unlexicalized Features"
          ]
        },
        "feature_processing": [
          "Syntactic Feature Extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lin1998_MINIPAR",
        "entity_type": "Algorithm",
        "name": "MINIPAR",
        "title": "Dependency-based evaluation of MINIPAR",
        "year": 1998,
        "authors": [
          "Dekang Lin"
        ],
        "task": "Dependency Parsing",
        "dataset": [
          "Penn Treebank"
        ],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Dependency Parser"
          ],
          "connections": [
            "Sentence -> Dependency Parse"
          ],
          "mechanisms": [
            "Dependency Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Treebank-trained Statistical Parsers"
          ],
          "parameter_tuning": [
            "Dependency Parsing Rules"
          ]
        },
        "feature_processing": [
          "Dependency Relation Extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sleator1993_LinkParser",
        "entity_type": "Algorithm",
        "name": "Link Parser",
        "title": "Parsing English with a Link Grammar",
        "year": 1993,
        "authors": [
          "Daniel D. Sleator",
          "Davy Temperley"
        ],
        "task": "Dependency Parsing",
        "dataset": [
          "Penn Treebank"
        ],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Link Grammar"
          ],
          "connections": [
            "Sentence -> Dependency Parse"
          ],
          "mechanisms": [
            "Link Grammar Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Treebank-trained Statistical Parsers"
          ],
          "parameter_tuning": [
            "Link Grammar Rules"
          ]
        },
        "feature_processing": [
          "Dependency Relation Extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Gelernter1959_GeometryMachine",
        "entity_type": "Algorithm",
        "name": "Geometry Machine",
        "year": 1959,
        "authors": [
          "Gelernter"
        ],
        "task": "Geometry Theorem Proving",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Heuristic Knowledge",
            "Backward Chaining Search Strategy"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Diagram-based Heuristic"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "CeruttiDavis1969_FORMAC",
        "entity_type": "Algorithm",
        "name": "FORMAC",
        "year": 1969,
        "authors": [
          "Cerutti",
          "Davis"
        ],
        "task": "Elementary Analytic Geometry Theorem Proving",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Symbolic Manipulation",
            "Descartes' Method"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Coordinate Assignment to Points"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ferguson1999_GeoRep",
        "entity_type": "Algorithm",
        "name": "GeoRep",
        "title": "GeoRep: A Flexible Tool for Spatial Representation of Line Drawings",
        "year": 1999,
        "authors": [
          "Ronald W. Ferguson",
          "Kenneth D. Forbus"
        ],
        "task": "Spatial Representation and Reasoning",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Low-Level Relational Describer (LLRD)",
            "High-Level Relational Describer (HLRD)"
          ],
          "connections": [
            "LLRD feeds into HLRD",
            "HLRD uses LLRD's output for domain-specific reasoning"
          ],
          "mechanisms": [
            "Visual operations library",
            "Rule engine (LTRE)",
            "Proximity detection",
            "Reference frame relations",
            "Parallel lines detection",
            "Interval relations",
            "Connectivity detection",
            "Polygon and polyline detection",
            "Boundary description",
            "Grouping"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Proximity detection",
          "Reference frame adaptation",
          "Visual operations"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PolygonFigures_1996",
        "entity_type": "Dataset",
        "name": "Polygon Figures",
        "description": "Set of randomly-generated polygons used in symmetry detection experiments",
        "domain": "Symmetry Detection",
        "size": 240,
        "year": 1996,
        "creators": [
          "Ferguson, R. W.",
          "Aminoff, A.",
          "Gentner, D."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_SymmetryJudgment",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Accuracy of symmetry judgment",
        "category": "Symmetry Detection",
        "formula": "Correct judgments / Total judgments"
      }
    },
    {
      "metric_entity": {
        "metric_id": "QualitativeFactors_SymmetryJudgment",
        "entity_type": "Metric",
        "name": "Qualitative Factors",
        "description": "Effect of qualitative visual structure on symmetry judgment",
        "category": "Symmetry Detection",
        "formula": "Significant effect on accuracy even after accounting for metric measures of asymmetry"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Pennington2014_GloVe",
        "entity_type": "Algorithm",
        "name": "GloVe",
        "title": "GloVe: Global Vectors for Word Representation",
        "year": 2014,
        "authors": [
          "Jeffrey Pennington",
          "Richard Socher",
          "Christopher D. Manning"
        ],
        "task": "Word Representation",
        "dataset": [
          "Wikipedia_2010",
          "Wikipedia_2014",
          "Gigaword_5",
          "Gigaword5_Wikipedia2014",
          "Common_Crawl_2014"
        ],
        "metrics": [
          "Accuracy_Analogy",
          "Spearman_Rank_Correlation_Similarity",
          "F1_NER"
        ],
        "architecture": {
          "components": [
            "Global Log-Bilinear Regression Model",
            "Weighted Least Squares Model"
          ],
          "connections": [
            "Word-Word Co-occurrence Matrix",
            "Dot Product of Word Vectors"
          ],
          "mechanisms": [
            "Logarithmic Transformation",
            "Bias Terms"
          ]
        },
        "methodology": {
          "training_strategy": [
            "AdaGrad",
            "Stochastic Sampling"
          ],
          "parameter_tuning": [
            "xmax=100",
            "α=3/4",
            "Initial Learning Rate=0.05",
            "50 Iterations for <300 Dimensions",
            "100 Iterations for ≥300 Dimensions"
          ]
        },
        "feature_processing": [
          "Symmetric Context Window",
          "Decreasing Weighting Function"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Wikipedia_2010",
        "entity_type": "Dataset",
        "name": "Wikipedia",
        "description": "2010 Wikipedia Dump",
        "domain": "Natural Language Processing",
        "size": 1000000000,
        "year": 2010,
        "creators": [
          "Stanford University"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Wikipedia_2014",
        "entity_type": "Dataset",
        "name": "Wikipedia",
        "description": "2014 Wikipedia Dump",
        "domain": "Natural Language Processing",
        "size": 1600000000,
        "year": 2014,
        "creators": [
          "Stanford University"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Gigaword_5",
        "entity_type": "Dataset",
        "name": "Gigaword 5",
        "description": "Gigaword 5 Corpus",
        "domain": "Natural Language Processing",
        "size": 4300000000,
        "year": 2014,
        "creators": [
          "Various Contributors"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Gigaword5_Wikipedia2014",
        "entity_type": "Dataset",
        "name": "Gigaword5 + Wikipedia2014",
        "description": "Combined Gigaword 5 and 2014 Wikipedia Dump",
        "domain": "Natural Language Processing",
        "size": 6000000000,
        "year": 2014,
        "creators": [
          "Stanford University"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Common_Crawl_2014",
        "entity_type": "Dataset",
        "name": "Common Crawl",
        "description": "Web Data from Common Crawl",
        "domain": "Natural Language Processing",
        "size": 42000000000,
        "year": 2014,
        "creators": [
          "Common Crawl Foundation"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Analogy",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Accuracy on Word Analogy Task",
        "category": "Word Analogy Evaluation",
        "formula": "Number of Correct Answers / Total Number of Questions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Spearman_Rank_Correlation_Similarity",
        "entity_type": "Metric",
        "name": "Spearman Rank Correlation",
        "description": "Correlation between Human Judgments and Cosine Similarity of Word Vectors",
        "category": "Word Similarity Evaluation",
        "formula": "Spearman's Rank Correlation Coefficient"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_NER",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "F1 Score on Named Entity Recognition Task",
        "category": "Named Entity Recognition Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mikolov2013_CBOW",
        "entity_type": "Algorithm",
        "name": "Continuous Bag-of-Words (CBOW)",
        "title": "Efficient Estimation of Word Representations in Vector Space",
        "year": 2013,
        "authors": [
          "Tomas Mikolov",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "Word Representation",
        "dataset": [
          "Wikipedia_2014",
          "Gigaword_5"
        ],
        "metrics": [
          "Accuracy_Analogy",
          "Spearman_Rank_Correlation_Similarity"
        ],
        "architecture": {
          "components": [
            "Neural Network",
            "Softmax Layer"
          ],
          "connections": [
            "Inner Product of Word Vectors"
          ],
          "mechanisms": [
            "Hierarchical Softmax",
            "Negative Sampling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Number of Negative Samples"
          ]
        },
        "feature_processing": [
          "Context Window"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lebret2014_HPCA",
        "entity_type": "Algorithm",
        "name": "Hellinger PCA",
        "title": "Word Embeddings through Hellinger PCA",
        "year": 2014,
        "authors": [
          "Rémi Lebret",
          "Ronan Collobert"
        ],
        "task": "Word Representation",
        "dataset": [
          "Wikipedia_2014",
          "Gigaword_5"
        ],
        "metrics": [
          "Accuracy_Analogy",
          "Spearman_Rank_Correlation_Similarity"
        ],
        "architecture": {
          "components": [
            "Principal Component Analysis"
          ],
          "connections": [
            "Hellinger Distance"
          ],
          "mechanisms": [
            "Square Root Transformation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Matrix Factorization"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Co-occurrence Matrix"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bengio2003_NeuralLM",
        "entity_type": "Algorithm",
        "name": "Neural Probabilistic Language Model",
        "title": "A Neural Probabilistic Language Model",
        "year": 2003,
        "authors": [
          "Yoshua Bengio",
          "Réjean Ducharme",
          "Pascal Vincent",
          "Christian Janvin"
        ],
        "task": "Language Modeling",
        "dataset": [
          "Various Text Corpora"
        ],
        "metrics": [
          "Perplexity"
        ],
        "architecture": {
          "components": [
            "Neural Network"
          ],
          "connections": [
            "Hidden Layers"
          ],
          "mechanisms": [
            "Probabilistic Output"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Word Sequences"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Zhou2015_ZDC",
        "entity_type": "Algorithm",
        "name": "ZDC",
        "title": "How Well Do Computers Solve Math Word Problems?",
        "year": 2016,
        "authors": [
          "Danqing Huang",
          "Shuming Shi",
          "Chin-Yew Lin",
          "Jian Yin",
          "Wei-Ying Ma"
        ],
        "task": "自动解数学应用题",
        "dataset": [
          "Alg514_2014",
          "Dolphin18K_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "改进的KAZB"
          ],
          "connections": [
            "减少名词短语与变量之间的对齐"
          ],
          "mechanisms": [
            "搜索空间缩小"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Huang2016_SIM",
        "entity_type": "Algorithm",
        "name": "SIM",
        "title": "How Well Do Computers Solve Math Word Problems?",
        "year": 2016,
        "authors": [
          "Danqing Huang",
          "Shuming Shi",
          "Chin-Yew Lin",
          "Jian Yin",
          "Wei-Ying Ma"
        ],
        "task": "自动解数学应用题",
        "dataset": [
          "Alg514_2014",
          "SingleEQ_2015",
          "Dolphin18K_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "简单相似性方法"
          ],
          "connections": [
            "问题句子到方程模板映射"
          ],
          "mechanisms": [
            "词向量TF-IDF",
            "加权Jaccard相似度"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "词向量建模"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Verb395_2014",
        "entity_type": "Dataset",
        "name": "Verb395",
        "description": "包含395个加减法应用题的数据集",
        "domain": "数学应用题求解",
        "size": 395,
        "year": 2014,
        "creators": [
          "Mohammad Javad Hosseini",
          "Hannaneh Hajishirzi",
          "Oren Etzioni",
          "Nate Kushman"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Dolphin1878_2015",
        "entity_type": "Dataset",
        "name": "Dolphin1878",
        "description": "包含1878个数字应用题的数据集",
        "domain": "数学应用题求解",
        "size": 1878,
        "year": 2015,
        "creators": [
          "Shuming Shi",
          "Yuehui Wang",
          "Chin-Yew Lin",
          "Xiaojiang Liu",
          "Yong Rui"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DRAW_2015",
        "entity_type": "Dataset",
        "name": "DRAW",
        "description": "包含1000个代数应用题的数据集",
        "domain": "数学应用题求解",
        "size": 1000,
        "year": 2015,
        "creators": [
          "Shyam Upadhyay",
          "Ming-Wei Chang"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SingleEQ_2015",
        "entity_type": "Dataset",
        "name": "SingleEQ",
        "description": "包含508个单一线性方程应用题的数据集",
        "domain": "数学应用题求解",
        "size": 508,
        "year": 2015,
        "creators": [
          "Rik Koncel-Kedziorski",
          "Hannaneh Hajishirzi",
          "Ashish Sabharwal",
          "Oren Etzioni",
          "Siena Dumas Ang"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Dolphin18K_2016",
        "entity_type": "Dataset",
        "name": "Dolphin18K",
        "description": "包含超过18000个标注的数学应用题的数据集",
        "domain": "数学应用题求解",
        "size": 18460,
        "year": 2016,
        "creators": [
          "Danqing Huang",
          "Shuming Shi",
          "Chin-Yew Lin",
          "Jian Yin",
          "Wei-Ying Ma"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ImageNet_2009",
        "entity_type": "Dataset",
        "name": "ImageNet",
        "description": "一个大规模层次化的图像数据库，基于WordNet结构构建。",
        "domain": "计算机视觉",
        "size": 3200000,
        "year": 2009,
        "creators": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Caltech101_2004",
        "entity_type": "Dataset",
        "name": "Caltech101",
        "description": "一个包含101个类别的小规模图像数据集。",
        "domain": "计算机视觉",
        "size": 9146,
        "year": 2004,
        "creators": [
          "Fei-Fei, L.",
          "Fergus, R.",
          "Perona, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Caltech256_2007",
        "entity_type": "Dataset",
        "name": "Caltech256",
        "description": "一个包含256个类别的图像数据集。",
        "domain": "计算机视觉",
        "size": 30607,
        "year": 2007,
        "creators": [
          "Griffin, G.",
          "Holub, A.",
          "Perona, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "TinyImage_2008",
        "entity_type": "Dataset",
        "name": "TinyImage",
        "description": "一个包含8000万张32x32低分辨率图像的数据集。",
        "domain": "计算机视觉",
        "size": 80000000,
        "year": 2008,
        "creators": [
          "Torralba, A.",
          "Fergus, R.",
          "Freeman, W."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ESP_dataset_2004",
        "entity_type": "Dataset",
        "name": "ESP dataset",
        "description": "通过在线游戏收集的图像数据集。",
        "domain": "计算机视觉",
        "size": 60000,
        "year": 2004,
        "creators": [
          "von Ahn, L.",
          "Dabbish, L."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "LabelMe_2008",
        "entity_type": "Dataset",
        "name": "LabelMe",
        "description": "一个包含30000张标注和分割的图像数据集。",
        "domain": "计算机视觉",
        "size": 30000,
        "year": 2008,
        "creators": [
          "Russell, B.",
          "Torralba, A.",
          "Murphy, K.",
          "Freeman, W."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Lotus_Hill_2007",
        "entity_type": "Dataset",
        "name": "Lotus Hill",
        "description": "一个包含50000张标注和分割的图像数据集。",
        "domain": "计算机视觉",
        "size": 50000,
        "year": 2007,
        "creators": [
          "Yao, B.",
          "Yang, X.",
          "Zhu, S."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "AUC_Classification",
        "entity_type": "Metric",
        "name": "AUC",
        "description": "ROC曲线下面积",
        "category": "分类评估",
        "formula": "ROC曲线下的面积"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Collins2008_ScalableDatasetConstruction",
        "entity_type": "Algorithm",
        "name": "Scalable Dataset Construction",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "authors": [
          "Collins, B.",
          "Deng, J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": "数据集构建",
        "dataset": [
          "ImageNet"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Active Learning Approach"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Active Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Boiman2008_NearestNeighborBasedImageClassification",
        "entity_type": "Algorithm",
        "name": "Nearest Neighbor Based Image Classification",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "authors": [
          "Boiman, O.",
          "Shechtman, E.",
          "Irani, M."
        ],
        "task": "图像分类",
        "dataset": [
          "ImageNet"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Bag-of-Features Representation"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Non-parametric"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "SIFT Descriptors"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Torralba2008_TinyImages",
        "entity_type": "Algorithm",
        "name": "Tiny Images",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "authors": [
          "Torralba, A.",
          "Fergus, R.",
          "Freeman, W."
        ],
        "task": "非参数对象和场景识别",
        "dataset": [
          "TinyImage"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Nearest Neighbor Methods"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Non-parametric"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Deng2009_ImageNetTreeMaxClassifier",
        "entity_type": "Algorithm",
        "name": "Tree-Max Classifier",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": 2009,
        "authors": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": "图像分类",
        "dataset": [
          "ImageNet"
        ],
        "metrics": [
          "AUC"
        ],
        "architecture": {
          "components": [
            "Hierarchical Structure"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "AdaBoost-based Classifier"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hajishirzi2013_NECO",
        "entity_type": "Algorithm",
        "name": "NECO",
        "title": "Joint Coreference Resolution and Named-Entity Linking with Multi-pass Sieves",
        "year": 2013,
        "authors": [
          "Hannaneh Hajishirzi",
          "Leila Zilles",
          "Daniel S. Weld",
          "Luke Zettlemoyer"
        ],
        "task": "Coreference Resolution and Named-Entity Linking",
        "dataset": [
          "ACE2004-NWIRE",
          "CONLL2011"
        ],
        "metrics": [
          "MUC",
          "B3",
          "Pairwise",
          "F1"
        ],
        "architecture": {
          "components": [
            "Stanford sieve-based model",
            "NEL-informed sieves",
            "mention detection",
            "mention pruning",
            "attribute assignment"
          ],
          "connections": [
            "NEL constraints",
            "mention clustering",
            "entity link propagation"
          ],
          "mechanisms": [
            "multi-pass sieves",
            "exact and relaxed NEL sieves",
            "fine-grained attributes"
          ]
        },
        "methodology": {
          "training_strategy": [
            "deterministic rules",
            "no learning phase"
          ],
          "parameter_tuning": [
            "confidence thresholds for NEL systems"
          ]
        },
        "feature_processing": [
          "mention detection",
          "NEL constraints",
          "attribute assignment"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CONLL2011_2011",
        "entity_type": "Dataset",
        "name": "CONLL2011",
        "description": "Coreference dataset with text from five different domains",
        "domain": "Natural Language Processing",
        "size": 625,
        "year": 2011,
        "creators": [
          "Pradhan et al."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "B3_Coreference",
        "entity_type": "Metric",
        "name": "B3",
        "description": "Computes the proportion of intersection between predicted and gold clusters for every mention",
        "category": "Coreference Evaluation",
        "formula": "Intersection proportion between predicted and gold clusters"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Pairwise_Coreference",
        "entity_type": "Metric",
        "name": "Pairwise",
        "description": "Measures the pairwise agreement between predicted and gold clusters",
        "category": "Coreference Evaluation",
        "formula": "Pairwise agreement between predicted and gold clusters"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_NEL",
        "entity_type": "Metric",
        "name": "F1",
        "description": "Harmonic mean of precision and recall",
        "category": "Named-Entity Linking Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Zhou2015_QuadraticProgrammingSolver",
        "entity_type": "Algorithm",
        "name": "Quadratic Programming Solver",
        "title": "Learn to Solve Algebra Word Problems Using Quadratic Programming",
        "year": 2015,
        "authors": [
          "Lipu Zhou",
          "Shuaixiang Dai",
          "Liwei Chen"
        ],
        "task": "自动求解代数文字题",
        "dataset": [
          "Kushman2014_Dataset"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Log-linear Model",
            "Quadratic Programming"
          ],
          "connections": [
            "Max-margin Objective",
            "Constraint Generation"
          ],
          "mechanisms": [
            "Feature Extraction",
            "Template Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-margin Objective",
            "Constraint Generation"
          ],
          "parameter_tuning": [
            "Parameter C"
          ]
        },
        "feature_processing": [
          "单槽特征",
          "槽对特征",
          "解特征"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Kushman2014_Dataset_2014",
        "entity_type": "Dataset",
        "name": "Kushman2014_Dataset",
        "description": "由Kushman等人提供的基准数据集，用于评估代数文字题求解器",
        "domain": "自然语言处理",
        "year": 2014,
        "creators": [
          "Nate Kushman",
          "Yoav Artzi",
          "Luke Zettlemoyer",
          "Regina Barzilay"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goldwasser2014_CombinedFeedbackPerceptron",
        "entity_type": "Algorithm",
        "name": "Combined Feedback Perceptron",
        "title": "Learning from natural instructions",
        "year": 2014,
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": "Semantic Parsing",
        "dataset": [
          "Solitaire Card Game",
          "Geoquery"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Update",
            "Structural Update"
          ],
          "connections": [
            "Combines binary and structured learning principles"
          ],
          "mechanisms": [
            "Uses binary feedback as coarse supervision",
            "Approximates structural loss"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Iterative training with feedback"
          ],
          "parameter_tuning": [
            "Weight updates based on feedback"
          ]
        },
        "feature_processing": [
          "Lexical and syntactic features"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goldwasser2014_StructuredPerceptron",
        "entity_type": "Algorithm",
        "name": "Structured Perceptron",
        "title": "Learning from natural instructions",
        "year": 2014,
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": "Semantic Parsing",
        "dataset": [
          "Solitaire Card Game",
          "Geoquery"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Feature mapping",
            "Decision variables"
          ],
          "connections": [
            "Maps input sentences to logical formulas"
          ],
          "mechanisms": [
            "Optimizes joint objective function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised learning"
          ],
          "parameter_tuning": [
            "Weight updates based on labeled examples"
          ]
        },
        "feature_processing": [
          "Lexical and syntactic features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Geoquery_1996",
        "entity_type": "Dataset",
        "name": "Geoquery",
        "description": "Geographical database queries",
        "domain": "Natural Language Processing",
        "size": 500,
        "year": 1996,
        "creators": [
          "Zelle, J.",
          "Mooney, R."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "LossApproximation_SemanticParsing",
        "entity_type": "Metric",
        "name": "Loss Approximation",
        "description": "Approximated structural loss",
        "category": "Semantic Parsing",
        "formula": "Assigns penalties based on confidence scores of substructures"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Cho2014_RNNEncoderDecoder",
        "entity_type": "Algorithm",
        "name": "RNN Encoder–Decoder",
        "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "year": 2014,
        "authors": [
          "Kyunghyun Cho",
          "Bart van Merriënboer",
          "Caglar Gulcehre",
          "Dzmitry Bahdanau",
          "Fethi Bougares",
          "Holger Schwenk",
          "Yoshua Bengio"
        ],
        "task": "Statistical Machine Translation",
        "dataset": [
          "Europarl_2014",
          "News Commentary_2014",
          "UN_2014",
          "Crawled Corpora_2014"
        ],
        "metrics": [
          "BLEU_score_Translation",
          "Perplexity_LanguageModel"
        ],
        "architecture": {
          "components": [
            "Recurrent Neural Network (RNN)",
            "Hidden Unit with Reset and Update Gates"
          ],
          "connections": [
            "Encoder RNN to Fixed-Length Vector",
            "Fixed-Length Vector to Decoder RNN"
          ],
          "mechanisms": [
            "Conditional Probability Maximization",
            "Adaptive Memory Control"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gradient-Based Algorithm",
            "Joint Training of Encoder and Decoder"
          ],
          "parameter_tuning": [
            "Adadelta",
            "Stochastic Gradient Descent",
            "Hyperparameters: ε=10^-6, ρ=0.95"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Phrase Pair Scoring"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Europarl_2014",
        "entity_type": "Dataset",
        "name": "Europarl",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Machine Translation",
        "size": 61000000,
        "year": 2014,
        "creators": [
          "Koehn, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NewsCommentary_2014",
        "entity_type": "Dataset",
        "name": "News Commentary",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Machine Translation",
        "size": 5500000,
        "year": 2014,
        "creators": [
          "Koehn, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "UN_2014",
        "entity_type": "Dataset",
        "name": "UN",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Machine Translation",
        "size": 421000000,
        "year": 2014,
        "creators": [
          "Koehn, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CrawledCorpora_2014",
        "entity_type": "Dataset",
        "name": "Crawled Corpora",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Machine Translation",
        "size": 870000000,
        "year": 2014,
        "creators": [
          "Koehn, P."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "BLEU_score_Translation",
        "entity_type": "Metric",
        "name": "BLEU Score",
        "description": "Bilingual Evaluation Understudy Score",
        "category": "Translation Performance",
        "formula": "Not explicitly defined in the paper"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Perplexity_LanguageModel",
        "entity_type": "Metric",
        "name": "Perplexity",
        "description": "Measure of how well a probability distribution or probability model predicts a sample",
        "category": "Language Model Performance",
        "formula": "Not explicitly defined in the paper"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kushman2014_EquationSolver",
        "entity_type": "Algorithm",
        "name": "EquationSolver",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": 2014,
        "authors": [
          "Nate Kushman",
          "Yoav Artzi",
          "Luke Zettlemoyer",
          "Regina Barzilay"
        ],
        "task": "自动求解代数文字题",
        "dataset": [
          "Algebra.com_2014"
        ],
        "metrics": [
          "Equation_Accuracy",
          "Answer_Accuracy"
        ],
        "architecture": {
          "components": [
            "模板选择",
            "槽位实例化",
            "对齐模型"
          ],
          "connections": [
            "模板与方程系统的映射",
            "槽位与文本的对齐"
          ],
          "mechanisms": [
            "联合对数线性分布",
            "模板归纳",
            "隐变量优化"
          ]
        },
        "methodology": {
          "training_strategy": [
            "弱监督学习",
            "完全监督学习"
          ],
          "parameter_tuning": [
            "L-BFGS优化",
            "L2正则化"
          ]
        },
        "feature_processing": [
          "词性标注",
          "词形还原",
          "依存句法分析",
          "词汇化特征"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Algebra.com_2014",
        "entity_type": "Dataset",
        "name": "Algebra.com",
        "description": "从Algebra.com收集的代数文字题数据集",
        "domain": "代数",
        "size": 514,
        "year": 2014,
        "creators": [
          "Nate Kushman",
          "Yoav Artzi",
          "Luke Zettlemoyer",
          "Regina Barzilay"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Equation_Accuracy",
        "entity_type": "Metric",
        "name": "Equation Accuracy",
        "description": "系统生成正确方程组的频率",
        "category": "方程评估",
        "formula": "正确方程组数量 / 总方程组数量"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Answer_Accuracy",
        "entity_type": "Metric",
        "name": "Answer Accuracy",
        "description": "生成数值答案正确的频率",
        "category": "答案评估",
        "formula": "正确答案数量 / 总答案数量"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hosseini2014_ARIS",
        "entity_type": "Algorithm",
        "name": "ARIS",
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "year": 2014,
        "authors": [
          "Mohammad Javad Hosseini",
          "Hannaneh Hajishirzi",
          "Oren Etzioni",
          "Nate Kushman"
        ],
        "task": "Solving arithmetic word problems",
        "dataset": [
          "MA1_2014",
          "MA2_2014",
          "IXL_2014"
        ],
        "metrics": [
          "Accuracy_Classification",
          "Verb_Categorization_Accuracy"
        ],
        "architecture": {
          "components": [
            "Entity recognizer",
            "Container recognizer",
            "Attribute recognizer",
            "Quantity recognizer",
            "Verb category classifier"
          ],
          "connections": [
            "Dependency parser",
            "Coreference resolution",
            "Named entity recognizer"
          ],
          "mechanisms": [
            "State transitions",
            "Equation formation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised learning",
            "Support Vector Machines"
          ],
          "parameter_tuning": [
            "Feature extraction",
            "Regularization"
          ]
        },
        "feature_processing": [
          "Similarity-based features",
          "WordNet-based features",
          "Structural features"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Verb_Categorization_Accuracy",
        "entity_type": "Metric",
        "name": "Verb Categorization Accuracy",
        "description": "Accuracy of verb categorization in sentences",
        "category": "Verb categorization evaluation",
        "formula": "Correctly categorized verbs / Total verbs"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Mitra2016_FormulaApplication",
        "entity_type": "Algorithm",
        "name": "Formula Application",
        "title": "Learning To Use Formulas To Solve Simple Arithmetic Problems",
        "year": 2016,
        "authors": [
          "Mitra, Arindam",
          "Baral, Chitta"
        ],
        "task": "Solving simple arithmetic word problems",
        "dataset": [
          "AddSub_2014"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Feature extraction",
            "Log-linear model",
            "Probabilistic model"
          ],
          "connections": [
            "Feature extraction -> Log-linear model",
            "Log-linear model -> Probabilistic model"
          ],
          "mechanisms": [
            "Feature function",
            "Parameter estimation",
            "Stochastic gradient descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised learning"
          ],
          "parameter_tuning": [
            "Parameter vector θ"
          ]
        },
        "feature_processing": [
          "WordNet",
          "ConceptNet",
          "Stanford CoreNLP"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AddSub_2014",
        "entity_type": "Dataset",
        "name": "AddSub",
        "description": "A dataset of simple addition-subtraction arithmetic problems",
        "domain": "Natural Language Processing",
        "size": 395,
        "year": 2014,
        "creators": [
          "Hosseini, Mohammad Javad",
          "Hajishirzi, Hannaneh",
          "Etzioni, Oren",
          "Kushman, Nate"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hochreiter1997_LongShortTermMemory",
        "entity_type": "Algorithm",
        "name": "Long Short-Term Memory (LSTM)",
        "title": "Long Short-Term Memory",
        "year": 1997,
        "authors": [
          "Hochreiter, S.",
          "Schmidhuber, J."
        ],
        "task": "Sequence Modeling, Long Time Lag Problems",
        "dataset": [],
        "metrics": [
          "Success Rate",
          "Training Time",
          "Error Rate"
        ],
        "architecture": {
          "components": [
            "Memory Cells",
            "Input Gates",
            "Output Gates",
            "Constant Error Carousels (CECs)"
          ],
          "connections": [
            "Fully Connected Hidden Layer",
            "Self-Connections within Memory Cells"
          ],
          "mechanisms": [
            "Constant Error Flow",
            "Multiplicative Gates"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Truncated Backpropagation Through Time (BPTT)",
            "Real-Time Recurrent Learning (RTRL)"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Initial Bias Values"
          ]
        },
        "feature_processing": [
          "Handling Noise",
          "Protecting Internal State from Drift"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Hochreiter1991_ConstantErrorCarrousel",
        "entity_type": "Algorithm",
        "name": "Constant Error Carrousel (CEC)",
        "title": "Long Short-Term Memory",
        "year": 1991,
        "authors": [
          "Hochreiter, S."
        ],
        "task": "Long Time Lag Problems",
        "dataset": [],
        "metrics": [
          "Error Rate"
        ],
        "architecture": {
          "components": [
            "Linear Unit with Fixed Self-Connection"
          ],
          "connections": [
            "Self-Connection"
          ],
          "mechanisms": [
            "Constant Error Flow"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Truncated Backpropagation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Elman1988_RecurrentCascadeCorrelation",
        "entity_type": "Algorithm",
        "name": "Recurrent Cascade-Correlation",
        "title": "Long Short-Term Memory",
        "year": 1988,
        "authors": [
          "Elman, J. L."
        ],
        "task": "Sequence Modeling",
        "dataset": [],
        "metrics": [
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Hidden Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Recurrent Cascade-Correlation Learning Algorithm"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Robinson1987_RealTimeRecurrentLearning",
        "entity_type": "Algorithm",
        "name": "Real-Time Recurrent Learning (RTRL)",
        "title": "Long Short-Term Memory",
        "year": 1987,
        "authors": [
          "Robinson, A. J.",
          "Fallside, F."
        ],
        "task": "Sequence Modeling",
        "dataset": [],
        "metrics": [
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Hidden Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Real-Time Recurrent Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Williams1990_BackPropagationThroughTime",
        "entity_type": "Algorithm",
        "name": "Back-Propagation Through Time (BPTT)",
        "title": "Long Short-Term Memory",
        "year": 1990,
        "authors": [
          "Williams, R. J.",
          "Peng, J."
        ],
        "task": "Sequence Modeling",
        "dataset": [],
        "metrics": [
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Hidden Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Back-Propagation Through Time"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Schmidhuber1992_NeuralSequenceChunker",
        "entity_type": "Algorithm",
        "name": "Neural Sequence Chunker",
        "title": "Long Short-Term Memory",
        "year": 1992,
        "authors": [
          "Schmidhuber, J."
        ],
        "task": "Sequence Modeling",
        "dataset": [],
        "metrics": [
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Two Networks"
          ],
          "connections": [
            "Inter-Network Connections"
          ],
          "mechanisms": [
            "Chunking"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Chunking"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "EmbeddedReberGrammar_1989",
        "entity_type": "Dataset",
        "name": "Embedded Reber Grammar",
        "description": "A synthetic dataset for evaluating sequence modeling algorithms",
        "domain": "Natural Language Processing",
        "size": "Not specified",
        "year": 1989,
        "creators": [
          "Smith, A. W.",
          "Zipser, D."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "SuccessRate_Classification",
        "entity_type": "Metric",
        "name": "Success Rate",
        "description": "Percentage of successful trials",
        "category": "Classification Evaluation",
        "formula": "Number of successful trials / Total number of trials"
      }
    },
    {
      "metric_entity": {
        "metric_id": "TrainingTime_Performance",
        "entity_type": "Metric",
        "name": "Training Time",
        "description": "Time required to train the model",
        "category": "Performance Evaluation",
        "formula": "Total training time"
      }
    },
    {
      "metric_entity": {
        "metric_id": "ErrorRate_Classification",
        "entity_type": "Metric",
        "name": "Error Rate",
        "description": "Average error rate over multiple trials",
        "category": "Classification Evaluation",
        "formula": "Sum of errors / Number of trials"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goyal2017_DeepLSTMNormImage",
        "entity_type": "Algorithm",
        "name": "Deeper LSTM Question + norm Image",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "year": 2017,
        "authors": [
          "Yash Goyal",
          "Tejas Khot",
          "Douglas Summers-Stay",
          "Dhruv Batra",
          "Devi Parikh"
        ],
        "task": "Visual Question Answering",
        "dataset": [
          "VQA_2015",
          "Balanced_VQA_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "CNN embedding",
            "LSTM embedding",
            "point-wise multiplication",
            "multi-layer perceptron classifier"
          ],
          "connections": [
            "image embedding -> point-wise multiplication",
            "question embedding -> point-wise multiplication",
            "point-wise multiplication -> MLP classifier"
          ],
          "mechanisms": [
            "embedding",
            "multiplication",
            "classification"
          ]
        },
        "methodology": {
          "training_strategy": [
            "supervised learning"
          ],
          "parameter_tuning": [
            "learning rate",
            "batch size"
          ]
        },
        "feature_processing": [
          "CNN feature extraction",
          "LSTM sequence modeling"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Lu2016_HierarchicalCoAttention",
        "entity_type": "Algorithm",
        "name": "Hierarchical Co-attention",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "year": 2017,
        "authors": [
          "Yash Goyal",
          "Tejas Khot",
          "Douglas Summers-Stay",
          "Dhruv Batra",
          "Devi Parikh"
        ],
        "task": "Visual Question Answering",
        "dataset": [
          "VQA_2015",
          "Balanced_VQA_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "word-level attention",
            "phrase-level attention",
            "question-level attention"
          ],
          "connections": [
            "image features -> co-attention",
            "question features -> co-attention",
            "co-attended features -> answer prediction"
          ],
          "mechanisms": [
            "attention",
            "co-attention",
            "recursive combination"
          ]
        },
        "methodology": {
          "training_strategy": [
            "supervised learning"
          ],
          "parameter_tuning": [
            "learning rate",
            "batch size"
          ]
        },
        "feature_processing": [
          "image feature extraction",
          "question feature extraction"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Fukui2016_MultimodalCompactBilinearPooling",
        "entity_type": "Algorithm",
        "name": "Multimodal Compact Bilinear Pooling",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "year": 2017,
        "authors": [
          "Yash Goyal",
          "Tejas Khot",
          "Douglas Summers-Stay",
          "Dhruv Batra",
          "Devi Parikh"
        ],
        "task": "Visual Question Answering",
        "dataset": [
          "VQA_2015",
          "Balanced_VQA_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "multimodal compact bilinear pooling",
            "fully-connected layer"
          ],
          "connections": [
            "image features -> bilinear pooling",
            "language features -> bilinear pooling",
            "pooled features -> fully-connected layer"
          ],
          "mechanisms": [
            "bilinear pooling",
            "attention"
          ]
        },
        "methodology": {
          "training_strategy": [
            "supervised learning"
          ],
          "parameter_tuning": [
            "learning rate",
            "batch size"
          ]
        },
        "feature_processing": [
          "image feature extraction",
          "language feature extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "VQA_2015",
        "entity_type": "Dataset",
        "name": "VQA",
        "description": "Visual Question Answering dataset with real images from COCO",
        "domain": "Computer Vision and Natural Language Processing",
        "size": 204000,
        "year": 2015,
        "creators": [
          "Stanislaw Antol",
          "Aishwarya Agrawal",
          "Jiasen Lu",
          "Margaret Mitchell",
          "Dhruv Batra",
          "C. Lawrence Zitnick",
          "Devi Parikh"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Balanced_VQA_2017",
        "entity_type": "Dataset",
        "name": "Balanced VQA",
        "description": "Balanced Visual Question Answering dataset with complementary images",
        "domain": "Computer Vision and Natural Language Processing",
        "size": 443000,
        "year": 2017,
        "creators": [
          "Yash Goyal",
          "Tejas Khot",
          "Douglas Summers-Stay",
          "Dhruv Batra",
          "Devi Parikh"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Entropy_AnswerDistribution",
        "entity_type": "Metric",
        "name": "Entropy",
        "description": "Measure of uncertainty in the answer distribution",
        "category": "Distribution Evaluation",
        "formula": "-Σ p(x) log p(x)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall_at_5",
        "entity_type": "Metric",
        "name": "Recall@5",
        "description": "Proportion of times the human-picked counter-example is among the top-5 in the sorted list",
        "category": "Explanation Evaluation",
        "formula": "Number of times human-picked image is in top-5 / Total number of queries"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2018_KNOWLEDGE",
        "entity_type": "Algorithm",
        "name": "KNOWLEDGE",
        "title": "Mapping to Declarative Knowledge for Word Problem Solving",
        "year": 2018,
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "Arithmetic Word Problem Solving",
        "dataset": [
          "AllArith",
          "Perturb",
          "Aggregate"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Transfer",
            "Dimensional Analysis",
            "Part-Whole Relation",
            "Explicit Math"
          ],
          "connections": [
            "Concept Selection",
            "Declarative Rule Selection"
          ],
          "mechanisms": [
            "Latent Variable Modeling",
            "Beam Search"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Two Stage Learning",
            "Latent Structured SVM"
          ],
          "parameter_tuning": [
            "Weight Vectors for Concepts and Rules"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Coreference Resolution",
          "Verb Classification",
          "Rate Component Detection"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArith_2017",
        "entity_type": "Dataset",
        "name": "AllArith",
        "description": "Arithmetic word problems dataset",
        "domain": "Mathematics",
        "size": 831,
        "year": 2017,
        "creators": [
          "Roy, S.",
          "Roth, D."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Perturb_2018",
        "entity_type": "Dataset",
        "name": "Perturb",
        "description": "New word problems created by perturbing original problems",
        "domain": "Mathematics",
        "size": 661,
        "year": 2018,
        "creators": [
          "Roy, S.",
          "Roth, D."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Aggregate_2018",
        "entity_type": "Dataset",
        "name": "Aggregate",
        "description": "Combined dataset of AllArith and Perturb",
        "domain": "Mathematics",
        "size": 1492,
        "year": 2018,
        "creators": [
          "Roy, S.",
          "Roth, D."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_MathDQN",
        "entity_type": "Algorithm",
        "name": "MathDQN",
        "title": "MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": "自动求解算术文字题",
        "dataset": [
          "AI2_2014",
          "IL_2015",
          "CC_2015",
          "ArithS",
          "ArithM"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "状态表示",
            "动作选择",
            "奖励函数",
            "两层前馈神经网络"
          ],
          "connections": [
            "状态到动作的选择",
            "奖励反馈到网络参数更新"
          ],
          "mechanisms": [
            "深度强化学习框架",
            "ε-greedy策略",
            "经验回放缓冲区"
          ]
        },
        "methodology": {
          "training_strategy": [
            "DQN框架下的训练",
            "ε-greedy策略",
            "经验回放缓冲区"
          ],
          "parameter_tuning": [
            "学习率设置为0.0001",
            "折扣因子γ=0.9",
            "重放缓冲区大小为15,000"
          ]
        },
        "feature_processing": [
          "数量模式提取",
          "重新排序机制",
          "特征向量表示"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AI2_2014",
        "entity_type": "Dataset",
        "name": "AI2",
        "description": "包含395个单步或多步算术文字题，仅涉及加法和减法",
        "domain": "算术文字题求解",
        "size": 395,
        "year": 2014,
        "creators": [
          "Hosseini et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IL_2015",
        "entity_type": "Dataset",
        "name": "IL",
        "description": "包含562个单步文字题，涉及加法、减法、乘法和除法",
        "domain": "算术文字题求解",
        "size": 562,
        "year": 2015,
        "creators": [
          "Roy, Vieira, and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CC_2015",
        "entity_type": "Dataset",
        "name": "CC",
        "description": "包含600个多步问题，不包含无关数量，涉及四种操作符的组合",
        "domain": "算术文字题求解",
        "size": 600,
        "year": 2015,
        "creators": [
          "Roy and Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ArithS",
        "entity_type": "Dataset",
        "name": "ArithS",
        "description": "包含890个单步算术问题，仅涉及一个操作符",
        "domain": "算术文字题求解",
        "size": 890,
        "year": 2018,
        "creators": [
          "Wang et al."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ArithM",
        "entity_type": "Dataset",
        "name": "ArithM",
        "description": "包含667个多步算术问题，至少涉及两个操作符",
        "domain": "算术文字题求解",
        "size": 667,
        "year": 2018,
        "creators": [
          "Wang et al."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_ExpTree",
        "entity_type": "Algorithm",
        "name": "ExpTree",
        "title": "Solving General Arithmetic Word Problems",
        "year": 2015,
        "authors": [
          "Roy, S.",
          "Roth, D."
        ],
        "task": "自动求解算术文字题",
        "dataset": [
          "AI2_2014",
          "IL_2015",
          "CC_2015"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "表达式树构建",
            "数量相关性预测分类器",
            "操作符分类器"
          ],
          "connections": [
            "数量到操作符的选择",
            "表达式树的构建"
          ],
          "mechanisms": [
            "表达式树评分函数"
          ]
        },
        "methodology": {
          "training_strategy": [
            "基于SVM的分类器训练"
          ],
          "parameter_tuning": [
            "无具体参数调整说明"
          ]
        },
        "feature_processing": [
          "数量模式提取"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Koncel-Kedziorski2015_ALGES",
        "entity_type": "Algorithm",
        "name": "ALGES",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": 2015,
        "authors": [
          "Koncel-Kedziorski, R.",
          "Hajishirzi, H.",
          "Sabharwal, A.",
          "Etzioni, O.",
          "Ang, S. D."
        ],
        "task": "自动求解算术文字题",
        "dataset": [
          "AI2_2014",
          "IL_2015",
          "CC_2015"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "整数线性规划枚举",
            "候选树评分"
          ],
          "connections": [
            "数量到表达式树的映射"
          ],
          "mechanisms": [
            "评分函数"
          ]
        },
        "methodology": {
          "training_strategy": [
            "基于整数线性规划的枚举"
          ],
          "parameter_tuning": [
            "无具体参数调整说明"
          ]
        },
        "feature_processing": [
          "数量模式提取"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2017_UnitDep",
        "entity_type": "Algorithm",
        "name": "UnitDep",
        "title": "Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving",
        "year": 2017,
        "authors": [
          "Roy, S.",
          "Roth, D."
        ],
        "task": "自动求解算术文字题",
        "dataset": [
          "AI2_2014",
          "IL_2015",
          "CC_2015"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "单位依赖图",
            "评分函数"
          ],
          "connections": [
            "数量到表达式树的映射"
          ],
          "mechanisms": [
            "评分函数"
          ]
        },
        "methodology": {
          "training_strategy": [
            "基于评分函数的优化"
          ],
          "parameter_tuning": [
            "无具体参数调整说明"
          ]
        },
        "feature_processing": [
          "数量模式提取"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sutskever2014_SequenceToSequenceLearning",
        "entity_type": "Algorithm",
        "name": "Sequence to Sequence Learning",
        "year": 2014,
        "authors": [
          "Sutskever, Ilya",
          "Vinyals, Oriol",
          "Le, Quoc V."
        ],
        "task": "Machine Translation, Image Caption Generation, Constituency Parsing",
        "dataset": [
          "WMT'15 English-German",
          "Penn Tree Bank",
          "High-Confidence Corpus",
          "Image Captioning Dataset"
        ],
        "metrics": [
          "BLEU",
          "Perplexity",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Recurrent Neural Networks (RNN)",
            "Long Short-Term Memory (LSTM)",
            "Gated Recurrent Unit (GRU)"
          ],
          "connections": [
            "Recurrent Connections",
            "Embeddings"
          ],
          "mechanisms": [
            "Attention Mechanism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Alternating Training",
            "Parameter Updates Allocation"
          ],
          "parameter_tuning": [
            "Mixing Ratios",
            "Learning Rate Halving"
          ]
        },
        "feature_processing": [
          "Input Sequence Reversal",
          "Dropout"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WMT15_English-German_2015",
        "entity_type": "Dataset",
        "name": "WMT'15 English-German",
        "description": "Parallel corpus for English-German translation",
        "domain": "Natural Language Processing",
        "size": 4500000,
        "year": 2015,
        "creators": [
          "Bojar, Ondřej",
          "Chatterjee, Rajen",
          "Federmann, Christian",
          "Haddow, Barry",
          "Huck, Matthias",
          "Hokamp, Chris",
          "Koehn, Philipp",
          "Logacheva, Varvara",
          "Monz, Christof",
          "Negri, Matteo",
          "Post, Matt",
          "Scarton, Carolina",
          "Specia, Lucia",
          "Turchi, Marco"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PennTreeBank_1993",
        "entity_type": "Dataset",
        "name": "Penn Tree Bank",
        "description": "Corpus for syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 40000,
        "year": 1993,
        "creators": [
          "Marcus, Mitchell P.",
          "Marcinkiewicz, Mary Ann",
          "Santorini, Beatrice"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "HighConfidenceCorpus_2015",
        "entity_type": "Dataset",
        "name": "High-Confidence Corpus",
        "description": "Large parsing resource for syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 11000000,
        "year": 2015,
        "creators": [
          "Vinyals, Oriol",
          "Kaiser, Lukasz",
          "Koo, Terry",
          "Petrov, Slav",
          "Sutskever, Ilya",
          "Hinton, Geoffrey"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ImageCaptioningDataset_2015",
        "entity_type": "Dataset",
        "name": "Image Captioning Dataset",
        "description": "Dataset of image and caption pairs",
        "domain": "Computer Vision",
        "size": 596000,
        "year": 2015,
        "creators": [
          "Vinyals, Oriol",
          "Toshev, Alexander",
          "Bengio, Samy",
          "Erhan, Dumitru"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "BLEU_Translation",
        "entity_type": "Metric",
        "name": "BLEU",
        "description": "Bilingual Evaluation Understudy",
        "category": "Translation Evaluation",
        "formula": "Exponential of the weighted harmonic mean of n-gram precision and brevity penalty"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Perplexity_LanguageModeling",
        "entity_type": "Metric",
        "name": "Perplexity",
        "description": "Measure of how well a probability distribution or probability model predicts a sample",
        "category": "Language Modeling Evaluation",
        "formula": "2^(-1/N * sum(log2(P(w_i))))"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1Score_Parsing",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Parsing Evaluation",
        "formula": "2 * (precision * recall) / (precision + recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bahdanau2015_AttentionMechanism",
        "entity_type": "Algorithm",
        "name": "Attention Mechanism",
        "year": 2015,
        "authors": [
          "Bahdanau, Dzmitry",
          "Cho, Kyunghyun",
          "Bengio, Yoshua"
        ],
        "task": "Machine Translation",
        "dataset": [
          "WMT'15 English-German"
        ],
        "metrics": [
          "BLEU",
          "Perplexity"
        ],
        "architecture": {
          "components": [
            "Encoder Hidden States",
            "Alignment Weights"
          ],
          "connections": [
            "Random Access Memory"
          ],
          "mechanisms": [
            "Attention"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Alternating Training"
          ],
          "parameter_tuning": [
            "Mixing Ratios"
          ]
        },
        "feature_processing": [
          "Input Sequence Reversal"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Dai2015_SemiSupervisedSequenceLearning",
        "entity_type": "Algorithm",
        "name": "Semi-Supervised Sequence Learning",
        "year": 2015,
        "authors": [
          "Dai, Andrew M.",
          "Le, Quoc V."
        ],
        "task": "Sequence to Sequence Learning",
        "dataset": [
          "Monolingual Corpora"
        ],
        "metrics": [
          "Perplexity"
        ],
        "architecture": {
          "components": [
            "Autoencoders",
            "Skip-Thought Vectors"
          ],
          "connections": [
            "Recurrent Connections"
          ],
          "mechanisms": [
            "Unsupervised Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Pretraining and Finetuning"
          ],
          "parameter_tuning": [
            "Mixing Ratios"
          ]
        },
        "feature_processing": [
          "Sentence Splitting"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Sundaram2015_WordProblemSolver",
        "entity_type": "Algorithm",
        "name": "Word Problem Solver",
        "title": "Natural Language Processing for Solving Simple Word Problems",
        "year": 2015,
        "authors": [
          "Sundaram, S. S.",
          "Khemani, D."
        ],
        "task": "Solving arithmetic word problems",
        "dataset": [
          "DS1_2014",
          "DS2_2014",
          "DS3_2014"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Knowledge Representation",
            "Temporal Schemas",
            "Common Sense Law of Inertia"
          ],
          "connections": [
            "Schemas",
            "Temporal Ordering"
          ],
          "mechanisms": [
            "Change In",
            "Change Out",
            "Combine",
            "Compare Plus",
            "Compare Minus",
            "Increase",
            "Reduction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Knowledge-based NLP"
          ],
          "parameter_tuning": [
            "Heuristics"
          ]
        },
        "feature_processing": [
          "Resolving Conjunctions",
          "Preprocessing Currency",
          "Resolving Co-references",
          "Preprocessing Sentences",
          "Resolving Entities"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DS1_2014",
        "entity_type": "Dataset",
        "name": "DS1",
        "description": "Dataset for simple arithmetic word problems",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DS2_2014",
        "entity_type": "Dataset",
        "name": "DS2",
        "description": "Dataset for arithmetic word problems with irrelevant information",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "DS3_2014",
        "entity_type": "Dataset",
        "name": "DS3",
        "description": "Dataset for complex arithmetic word problems",
        "domain": "Natural Language Processing",
        "year": 2014,
        "creators": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Huang2018_CASS",
        "entity_type": "Algorithm",
        "name": "CASS",
        "title": "Neural Math Word Problem Solver with Reinforcement Learning",
        "year": 2018,
        "authors": [
          "Danqing Huang",
          "Jing Liu",
          "Chin-Yew Lin",
          "Jian Yin"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Alg514",
          "NumWord",
          "Dolphin18K"
        ],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Sequence-to-sequence model",
            "Copy mechanism",
            "Alignment mechanism"
          ],
          "connections": [
            "Encoder-decoder architecture",
            "Attention mechanism"
          ],
          "mechanisms": [
            "Copy mechanism",
            "Alignment mechanism",
            "Reinforcement learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Reinforcement learning",
            "Policy gradient",
            "Pre-training with maximum likelihood"
          ],
          "parameter_tuning": [
            "Hyper-parameter λ for supervised attention",
            "Dropout rate",
            "Beam size"
          ]
        },
        "feature_processing": [
          "Number mapping",
          "Post-processing step to recover tokens"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NumWord_2015",
        "entity_type": "Dataset",
        "name": "NumWord",
        "description": "Contains 2,871 number word problems with 1,183 templates",
        "domain": "Mathematics",
        "size": 2871,
        "year": 2015,
        "creators": [
          "Shuming Shi",
          "Yuehui Wang",
          "Chin-Yew Lin",
          "Xiaojiang Liu",
          "Yong Rui"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Solution_Accuracy",
        "entity_type": "Metric",
        "name": "Solution Accuracy",
        "description": "The proportion of correctly solved math word problems",
        "category": "Math Word Problem Solving Evaluation",
        "formula": "Number of correctly solved problems / Total number of problems"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Seq2SeqAttn_2017",
        "entity_type": "Algorithm",
        "name": "Seq2SeqAttn",
        "title": "Deep Neural Model for Math Word Problem Problems",
        "year": 2017,
        "authors": [
          "Yan Wang",
          "Xiaojiang Liu",
          "Shuming Shi"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Alg514",
          "NumWord",
          "Dolphin18K"
        ],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Sequence-to-sequence model",
            "Attention mechanism"
          ],
          "connections": [
            "Encoder-decoder architecture"
          ],
          "mechanisms": [
            "Attention mechanism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum likelihood estimation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Number mapping"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "FeatureBasedModel_2017",
        "entity_type": "Algorithm",
        "name": "Feature-based Model",
        "title": "Learning Fine-grained Expressions to Solve Math Word Problems",
        "year": 2017,
        "authors": [
          "Danqing Huang",
          "Shuming Shi",
          "Chin-Yew Lin",
          "Jian Yin"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Alg514",
          "NumWord",
          "Dolphin18K"
        ],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Template retrieval",
            "Equation ranking"
          ],
          "connections": [],
          "mechanisms": [
            "Feature extraction",
            "Ranking"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Feature extraction"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SINGLEEQ_2015",
        "entity_type": "Dataset",
        "name": "SINGLEEQ",
        "description": "Grade-school algebra word problems that map to single equations",
        "domain": "Mathematics",
        "size": 508,
        "year": 2015,
        "creators": [
          "Rik Koncel-Kedziorski",
          "Hannaneh Hajishirzi",
          "Ashish Sabharwal",
          "Oren Etzioni",
          "Siena Dumas Ang"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ADDSUB_2014",
        "entity_type": "Dataset",
        "name": "ADDSUB",
        "description": "Addition and subtraction word problems with irrelevant distractor quantities",
        "domain": "Mathematics",
        "year": 2014,
        "creators": [
          "Mohammad Javad Hosseini",
          "Hannaneh Hajishirzi",
          "Oren Etzioni",
          "Nate Kushman"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Socher2013_CompositionalVectorGrammar",
        "entity_type": "Algorithm",
        "name": "Compositional Vector Grammar (CVG)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": 2013,
        "authors": [
          "Socher, R.",
          "Bauer, J.",
          "Manning, C. D.",
          "Ng, A. Y."
        ],
        "task": "自然语言处理中的句法分析",
        "dataset": [
          "Penn Treebank WSJ"
        ],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Probabilistic Context-Free Grammar (PCFG)",
            "Recursive Neural Network (RNN)",
            "Syntactically Untied Recursive Neural Network (SU-RNN)"
          ],
          "connections": [
            "PCFG与RNN结合",
            "SU-RNN权重依赖于子节点的语法类别"
          ],
          "mechanisms": [
            "组合向量表示",
            "连续语义表示"
          ]
        },
        "methodology": {
          "training_strategy": [
            "最大间隔训练目标",
            "AdaGrad优化方法"
          ],
          "parameter_tuning": [
            "正则化参数λ=10^-4",
            "词向量维度25"
          ]
        },
        "feature_processing": [
          "分布式的词向量表示",
          "基于上下文的词嵌入"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PennTreebank_WSJ_2013",
        "entity_type": "Dataset",
        "name": "Penn Treebank WSJ",
        "description": "华尔街日报部分的Penn Treebank语料库",
        "domain": "自然语言处理",
        "size": "未具体说明",
        "year": 2013,
        "creators": [
          "未具体说明"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Parsing",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "句法分析的F1得分",
        "category": "句法分析评估",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Socher2013_StandardRNN",
        "entity_type": "Algorithm",
        "name": "Standard Recursive Neural Network (RNN)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": 2013,
        "authors": [
          "Socher, R.",
          "Bauer, J.",
          "Manning, C. D.",
          "Ng, A. Y."
        ],
        "task": "自然语言处理中的句法分析",
        "dataset": [
          "Penn Treebank WSJ"
        ],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "递归神经网络"
          ],
          "connections": [
            "相同的权重矩阵应用于所有节点"
          ],
          "mechanisms": [
            "组合函数用于计算非终端节点的向量表示"
          ]
        },
        "methodology": {
          "training_strategy": [
            "标准RNN训练方法"
          ],
          "parameter_tuning": [
            "未具体说明"
          ]
        },
        "feature_processing": [
          "词向量表示"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Socher2013_SyntacticallyUntiedRNN",
        "entity_type": "Algorithm",
        "name": "Syntactically Untied Recursive Neural Network (SU-RNN)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": 2013,
        "authors": [
          "Socher, R.",
          "Bauer, J.",
          "Manning, C. D.",
          "Ng, A. Y."
        ],
        "task": "自然语言处理中的句法分析",
        "dataset": [
          "Penn Treebank WSJ"
        ],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "递归神经网络"
          ],
          "connections": [
            "权重依赖于子节点的语法类别"
          ],
          "mechanisms": [
            "组合函数根据子节点的语法类别变化"
          ]
        },
        "methodology": {
          "training_strategy": [
            "最大间隔训练目标",
            "AdaGrad优化方法"
          ],
          "parameter_tuning": [
            "正则化参数λ=10^-4",
            "词向量维度25"
          ]
        },
        "feature_processing": [
          "分布式的词向量表示",
          "基于上下文的词嵌入"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_QuantityExtraction",
        "entity_type": "Algorithm",
        "name": "Quantity Extraction",
        "year": 2015,
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": "Quantity Recognition and Normalization",
        "dataset": [
          "RTE Datasets",
          "Newswire Text"
        ],
        "metrics": [
          "Precision",
          "Recall",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Segmentation",
            "Standardization",
            "Unit Inference"
          ],
          "connections": [
            "Segmentation -> Standardization",
            "Standardization -> Unit Inference"
          ],
          "mechanisms": [
            "Sequence Segmentation",
            "Rule-Based Standardization",
            "Semantic Role Labeling",
            "Coreference Resolution"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Cross-Validation"
          ],
          "parameter_tuning": [
            "Feature Engineering",
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "Word Class Features",
          "Character-Based Features",
          "Part of Speech Tags",
          "Contextual Features"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_QuantityEntailment",
        "entity_type": "Algorithm",
        "name": "Quantity Entailment",
        "year": 2015,
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": "Textual Entailment",
        "dataset": [
          "RTE Datasets",
          "Newswire Text"
        ],
        "metrics": [
          "Precision",
          "Recall",
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Extraction Phase",
            "Reasoning Phase"
          ],
          "connections": [
            "Extraction Phase -> Reasoning Phase"
          ],
          "mechanisms": [
            "Implicit Quantity Productions",
            "Quantity Comparisons",
            "Monotonicity Verification"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Cross-Validation"
          ],
          "parameter_tuning": [
            "Feature Engineering",
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "WordNet Synsets",
          "Coreference Resolution",
          "Semantic Role Labeling"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_MathWordProblemSolver",
        "entity_type": "Algorithm",
        "name": "Math Word Problem Solver",
        "year": 2015,
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": "Elementary School Math Word Problems",
        "dataset": [
          "Math Word Problems"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Quantity Pair Classifier",
            "Operation Classifier",
            "Order Classifier"
          ],
          "connections": [
            "Quantity Pair Classifier -> Operation Classifier -> Order Classifier"
          ],
          "mechanisms": [
            "Cascade of Classifiers",
            "Feature Engineering",
            "Sparse Averaged Perceptron"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Cross-Validation"
          ],
          "parameter_tuning": [
            "Feature Engineering",
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "Unigrams and Bigrams",
          "POS Tags",
          "Relevant Pair of Quantities",
          "Relevant Operation",
          "Relevant Order"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "RTE_Datasets_2006",
        "entity_type": "Dataset",
        "name": "RTE Datasets",
        "description": "Textual Entailment datasets",
        "domain": "Natural Language Processing",
        "size": 384,
        "year": 2006,
        "creators": [
          "I. Dagan",
          "O. Glickman",
          "B. Magnini"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Newswire_Text_2015",
        "entity_type": "Dataset",
        "name": "Newswire Text",
        "description": "Sentences from news articles containing quantity mentions",
        "domain": "Natural Language Processing",
        "size": 600,
        "year": 2015,
        "creators": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Math_Word_Problems_2015",
        "entity_type": "Dataset",
        "name": "Math Word Problems",
        "description": "Elementary school math word problems",
        "domain": "Education",
        "size": 864,
        "year": 2015,
        "creators": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Classification",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "F1 Score is the harmonic mean of precision and recall",
        "category": "Classification Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Jung2004_WindowedHoughTransform",
        "entity_type": "Algorithm",
        "name": "Windowed Hough Transform",
        "title": "Rectangle detection based on a windowed Hough transform",
        "year": 2004,
        "authors": [
          "Claudio Rosito Jung",
          "Rodrigo Schramm"
        ],
        "task": "Rectangle detection",
        "dataset": [],
        "metrics": [
          "Detection accuracy"
        ],
        "architecture": {
          "components": [
            "Sliding window",
            "Hough Transform",
            "Peak extraction",
            "Geometric constraints"
          ],
          "connections": [
            "Sliding window -> Hough Transform -> Peak extraction -> Geometric constraints"
          ],
          "mechanisms": [
            "Sliding window over image",
            "Compute Hough Transform in small regions",
            "Extract peaks from Hough image",
            "Detect rectangles using geometric conditions"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": [
            "Thresholds Tθ, Tρ, TL, Tα"
          ]
        },
        "feature_processing": [
          "Edge detection",
          "Ring-like search region"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "SyntheticImage_2004",
        "entity_type": "Dataset",
        "name": "Synthetic image",
        "description": "Synthetic image containing several geometric objects",
        "domain": "Computer vision",
        "size": "256 × 256 pixels",
        "year": 2004,
        "creators": [
          "Claudio Rosito Jung",
          "Rodrigo Schramm"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "NaturalImage_2004",
        "entity_type": "Dataset",
        "name": "Natural image",
        "description": "Natural image used for testing rectangle detection",
        "domain": "Computer vision",
        "size": "Not specified",
        "year": 2004,
        "creators": [
          "Claudio Rosito Jung",
          "Rodrigo Schramm"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "DetectionAccuracy_RectangleDetection",
        "entity_type": "Metric",
        "name": "Detection accuracy",
        "description": "Accuracy of detecting rectangles in images",
        "category": "Object detection",
        "formula": "Number of correctly detected rectangles / Total number of rectangles"
      }
    },
    {
      "metric_entity": {
        "metric_id": "ErrorMeasure_RectangleDetection",
        "entity_type": "Metric",
        "name": "Error measure",
        "description": "Error measure for detected rectangles",
        "category": "Object detection",
        "formula": "E(Pk, Pl)= √a(∆θk^2+∆θl^2+∆α^2)+ b(∆ρk^2+∆ρl^2)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bakman2023_ROBUST",
        "entity_type": "Algorithm",
        "name": "ROBUST",
        "title": "ROBUST UNDERSTANDING OF WORD PROBLEMS WITH EXTRANEOUS INFORMATION",
        "year": 2023,
        "authors": [
          "Bakman, Y."
        ],
        "task": "Understanding and solving arithmetic word problems with extraneous information",
        "dataset": [],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Change formulas",
            "Schema instantiations"
          ],
          "connections": [
            "Correspondence between formula variables and problem propositions"
          ],
          "mechanisms": [
            "Parsing natural language",
            "Cautious strategy for schema instantiation creation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Not applicable"
          ],
          "parameter_tuning": [
            "Not applicable"
          ]
        },
        "feature_processing": [
          "Change verb categorization",
          "Elementary component splitting"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kwiatkowski2013_SemanticParser",
        "entity_type": "Algorithm",
        "name": "Semantic Parser with On-the-fly Ontology Matching",
        "year": 2013,
        "authors": [
          "Kwiatkowski, T.",
          "Choi, E.",
          "Artzi, Y.",
          "Zettlemoyer, L."
        ],
        "task": "Question Answering",
        "dataset": [
          "GeoQuery_1996",
          "FreebaseQA_2013"
        ],
        "metrics": [
          "Recall_QA",
          "Precision_QA",
          "F1_QA"
        ],
        "architecture": {
          "components": [
            "Probabilistic CCG",
            "Ontology Matching Model"
          ],
          "connections": [
            "CCG Parsing -> Ontology Matching"
          ],
          "mechanisms": [
            "Domain-independent Parsing",
            "Structure Matching",
            "Constant Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Online Learning",
            "Perceptron"
          ],
          "parameter_tuning": [
            "Feature Weights"
          ]
        },
        "feature_processing": [
          "CCG Parse Features",
          "Structural Features",
          "Lexical Features",
          "Knowledge Base Features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "GeoQuery_1996",
        "entity_type": "Dataset",
        "name": "GeoQuery",
        "year": 1996,
        "creators": [
          "Zelle, J.",
          "Mooney, R."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "FreebaseQA_2013",
        "entity_type": "Dataset",
        "name": "Freebase QA",
        "year": 2013,
        "creators": [
          "Cai, Q.",
          "Yates, A."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Recall_QA",
        "entity_type": "Metric",
        "name": "Recall",
        "category": "Question Answering Evaluation",
        "formula": "Number of Correct Answers / Total Number of Questions"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Precision_QA",
        "entity_type": "Metric",
        "name": "Precision",
        "category": "Question Answering Evaluation",
        "formula": "Number of Correct Answers / Number of Produced Queries"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_QA",
        "entity_type": "Metric",
        "name": "F1 Score",
        "category": "Question Answering Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Chiang2019_SemanticallyAlignedEquationGeneration",
        "entity_type": "Algorithm",
        "name": "Semantically-Aligned Equation Generation",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": 2019,
        "authors": [
          "Ting-Rui Chiang",
          "Yun-Nung Chen"
        ],
        "task": "Solving and Reasoning Math Word Problems",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Stack",
            "Semantic Transformer"
          ],
          "connections": [
            "Encoder-Decoder",
            "Decoder-Stack",
            "Stack-Semantic Transformer"
          ],
          "mechanisms": [
            "Attention Mechanism",
            "Gating Mechanism",
            "Semantic Representation Extraction",
            "Operand Selection",
            "Operator Application"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Fully Supervised Training",
            "Postfix Representation Transformation"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Dropout Rate"
          ]
        },
        "feature_processing": [
          "Bidirectional LSTM for Semantic Representation",
          "Attention Mechanism for Problem Information Incorporation"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Knuth1968_ContextFreeGrammarAlgorithm",
        "entity_type": "Algorithm",
        "name": "Context-Free Grammar Algorithm",
        "title": "Semantics of Context-Free Languages",
        "year": 1968,
        "authors": [
          "Knuth, D. E."
        ],
        "task": "Parsing and semantics analysis of context-free grammars",
        "architecture": {
          "components": [
            "Directed graphs",
            "Derivation trees"
          ],
          "connections": [
            "Paths between vertices",
            "Arcs in directed graphs"
          ],
          "mechanisms": [
            "Graph construction",
            "Cycle detection"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Graph-based representation of grammar structures"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "CycleDetection_Semantics",
        "entity_type": "Metric",
        "name": "Cycle Detection",
        "description": "Detection of oriented cycles in directed graphs",
        "category": "Semantic role validation",
        "formula": "Presence or absence of cycles in directed graphs"
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WMT14_EnglishToFrench",
        "entity_type": "Dataset",
        "name": "WMT'14 English to French",
        "description": "English to French translation dataset used in WMT'14",
        "domain": "Natural Language Processing",
        "size": 12000000,
        "year": 2014,
        "creators": [
          "WMT"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "BLEU_TranslationQuality",
        "entity_type": "Metric",
        "name": "BLEU score",
        "description": "A method for automatic evaluation of machine translation",
        "category": "Translation Quality",
        "formula": "Not explicitly defined in the paper"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kalchbrenner2013_RecurrentContinuousTranslationModels",
        "entity_type": "Algorithm",
        "name": "Recurrent Continuous Translation Models",
        "title": "Recurrent Continuous Translation Models",
        "year": 2013,
        "authors": [
          "Kalchbrenner, N.",
          "Blunsom, P."
        ],
        "task": "Machine Translation",
        "dataset": [
          "Not specified in this context"
        ],
        "metrics": [
          "Not specified in this context"
        ],
        "architecture": {
          "components": [
            "Convolutional Neural Networks (CNN)"
          ],
          "connections": [
            "Input sentence to vector",
            "Vector to output sentence"
          ],
          "mechanisms": [
            "Mapping sentences to vectors"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Not specified in this context"
          ],
          "parameter_tuning": [
            "Not specified in this context"
          ]
        },
        "feature_processing": [
          "Not specified in this context"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bahdanau2014_NeuralMachineTranslation",
        "entity_type": "Algorithm",
        "name": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
        "year": 2014,
        "authors": [
          "Bahdanau, D.",
          "Cho, K.",
          "Bengio, Y."
        ],
        "task": "Machine Translation",
        "dataset": [
          "Not specified in this context"
        ],
        "metrics": [
          "Not specified in this context"
        ],
        "architecture": {
          "components": [
            "Attention mechanism"
          ],
          "connections": [
            "Input sequence to aligned output sequence"
          ],
          "mechanisms": [
            "Jointly learning to align and translate"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Not specified in this context"
          ],
          "parameter_tuning": [
            "Not specified in this context"
          ]
        },
        "feature_processing": [
          "Not specified in this context"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Cho2014_LearningPhraseRepresentations",
        "entity_type": "Algorithm",
        "name": "Learning Phrase Representations using RNN Encoder-Decoder",
        "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
        "year": 2014,
        "authors": [
          "Cho, K.",
          "Merrienboer, B.",
          "Gulcehre, C.",
          "Bougares, F.",
          "Schwenk, H.",
          "Bengio, Y."
        ],
        "task": "Machine Translation",
        "dataset": [
          "Not specified in this context"
        ],
        "metrics": [
          "Not specified in this context"
        ],
        "architecture": {
          "components": [
            "RNN Encoder-Decoder"
          ],
          "connections": [
            "Input sequence to fixed-dimensional vector",
            "Fixed-dimensional vector to output sequence"
          ],
          "mechanisms": [
            "Phrase representations"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Not specified in this context"
          ],
          "parameter_tuning": [
            "Not specified in this context"
          ]
        },
        "feature_processing": [
          "Not specified in this context"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wiseman2016_BSO",
        "entity_type": "Algorithm",
        "name": "Beam Search Optimization (BSO)",
        "title": "Sequence-to-Sequence Learning as Beam-Search Optimization",
        "year": 2016,
        "authors": [
          "Sam Wiseman",
          "Alexander M. Rush"
        ],
        "task": "Sequence-to-Sequence Learning",
        "dataset": [
          "PTB_2015",
          "IWSLT_2014"
        ],
        "metrics": [
          "BLEU_SentenceLevel",
          "UAS_Parsing",
          "LAS_Parsing"
        ],
        "architecture": {
          "components": [
            "LSTM Encoder",
            "LSTM Decoder",
            "Global Attention Model"
          ],
          "connections": [
            "Input Feeding",
            "Beam Search"
          ],
          "mechanisms": [
            "LaSO Framework",
            "Non-probabilistic Scoring Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search Training",
            "Curriculum Beam Strategy",
            "Pre-training with Cross-Entropy Loss"
          ],
          "parameter_tuning": [
            "Mini-batch Adagrad",
            "Gradient Renormalization",
            "Learning Rates"
          ]
        },
        "feature_processing": [
          "Word Embeddings Initialization",
          "Dropout Regularization"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PTB_2015",
        "entity_type": "Dataset",
        "name": "Penn Treebank",
        "description": "Standard dataset for parsing and language modeling",
        "domain": "Natural Language Processing",
        "size": "Varies by split",
        "year": 2015,
        "creators": [
          "Various contributors"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IWSLT_2014",
        "entity_type": "Dataset",
        "name": "IWSLT 2014",
        "description": "German-to-English translation dataset from TED talks",
        "domain": "Machine Translation",
        "size": "153K training sentences, 7K development sentences, 7K test sentences",
        "year": 2014,
        "creators": [
          "Mauro Cettolo",
          "Jan Niehues",
          "Sebastian Stüker",
          "Luisa Bentivogli",
          "Marcello Federico"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "BLEU_SentenceLevel",
        "entity_type": "Metric",
        "name": "BLEU",
        "description": "Sentence-level BLEU score for evaluating machine translation",
        "category": "Machine Translation Evaluation",
        "formula": "Smoothed, sentence-level BLEU score"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Haghighi2009_SimpleCoreferenceResolution",
        "entity_type": "Algorithm",
        "name": "Simple Coreference Resolution",
        "title": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
        "year": 2009,
        "authors": [
          "Aria Haghighi",
          "Dan Klein"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE2004-ROTH-DEV",
          "ACE2004-CULOTTA-TEST",
          "ACE2004-NWIRE",
          "MUC-6-TEST",
          "BLIPP",
          "WIKI"
        ],
        "metrics": [
          "Pairwise F1",
          "b3",
          "MUC",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Syntactic Module",
            "Semantic Module",
            "Selection Module"
          ],
          "connections": [
            "Syntactic Paths Extraction",
            "Compatibility Lists",
            "Tree Distance Minimization"
          ],
          "mechanisms": [
            "Syntactic Constraints",
            "Semantic Compatibility Filtering",
            "Agreement Constraints",
            "Syntactic Configuration Constraints"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Unsupervised Learning",
            "Deterministic Function"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Syntactic Parsing",
          "Semantic Compatibility Mining",
          "Appositive Annotation",
          "Predicate Nominative Annotation"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-ROTH-DEV_2009",
        "entity_type": "Dataset",
        "name": "ACE2004-ROTH-DEV",
        "description": "Development set split of the ACE 2004 training set",
        "domain": "Natural Language Processing",
        "size": 68,
        "year": 2009,
        "creators": [
          "Bengston, E.",
          "Roth, D."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-CULOTTA-TEST_2009",
        "entity_type": "Dataset",
        "name": "ACE2004-CULOTTA-TEST",
        "description": "Test set split of the ACE 2004 training set",
        "domain": "Natural Language Processing",
        "size": 107,
        "year": 2009,
        "creators": [
          "Culotta, A.",
          "Bengston, E.",
          "Roth, D."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004-NWIRE_2009",
        "entity_type": "Dataset",
        "name": "ACE2004-NWIRE",
        "description": "ACE 2004 Newswire set",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": 2009,
        "creators": [
          "Poon, H.",
          "Domingos, P."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MUC-6-TEST_2009",
        "entity_type": "Dataset",
        "name": "MUC-6-TEST",
        "description": "MUC6 formal evaluation set",
        "domain": "Natural Language Processing",
        "size": 30,
        "year": 2009,
        "creators": [
          "Vilain, M.",
          "Burger, J.",
          "Aberdeen, J.",
          "Connolly, D.",
          "Hirschman, L."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "BLIPP_2009",
        "entity_type": "Dataset",
        "name": "BLIPP",
        "description": "1.8 million sentences of newswire parsed with the Charniak parser",
        "domain": "Natural Language Processing",
        "size": 1800000,
        "year": 2009,
        "creators": [
          "Charniak, E."
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WIKI_2009",
        "entity_type": "Dataset",
        "name": "WIKI",
        "description": "25k articles of English Wikipedia abstracts parsed by the Klein and Manning parser",
        "domain": "Natural Language Processing",
        "size": 25000,
        "year": 2009,
        "creators": [
          "Klein, D.",
          "Manning, C."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Pairwise_F1_Coreference",
        "entity_type": "Metric",
        "name": "Pairwise F1",
        "description": "Precision, recall, and F1 over all pairs of mentions in the same entity cluster",
        "category": "Coreference Resolution",
        "formula": "F1 = 2 * (precision * recall) / (precision + recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "b3_Coreference",
        "entity_type": "Metric",
        "name": "b3",
        "description": "For each mention, form the intersection between the predicted cluster and the true cluster for that mention",
        "category": "Coreference Resolution",
        "formula": "F1 = 2 * (precision * recall) / (precision + recall)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "CEAF_Coreference",
        "entity_type": "Metric",
        "name": "CEAF",
        "description": "Scores the best match between true and predicted clusters using a similarity function",
        "category": "Coreference Resolution",
        "formula": "Best match using φ3 similarity function"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2016_ExpressionTreeBasedSolver",
        "entity_type": "Algorithm",
        "name": "Expression Tree Based Solver",
        "title": "Solving General Arithmetic Word Problems",
        "year": 2016,
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "自动求解算术文字题",
        "dataset": [
          "AI2 Dataset",
          "IL Dataset",
          "Commoncore Dataset"
        ],
        "metrics": [
          "Accuracy",
          "Relax Accuracy",
          "Strict Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Tree",
            "Monotonic Expression Tree",
            "Quantity Schema"
          ],
          "connections": [
            "LCA Operation Prediction",
            "Relevance Classification"
          ],
          "mechanisms": [
            "Constrained Inference Framework",
            "Global Inference Module"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Multiclass SVM Classifier",
            "Binary SVM Classifier"
          ],
          "parameter_tuning": [
            "wIRR"
          ]
        },
        "feature_processing": [
          "Unit Features",
          "Related NP Features",
          "Miscellaneous Features",
          "Question Features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AI2_Dataset_2014",
        "entity_type": "Dataset",
        "name": "AI2 Dataset",
        "description": "包含395个加减法问题的数据集",
        "domain": "自然语言处理",
        "size": 395,
        "year": 2014,
        "creators": [
          "M. J. Hosseini",
          "H. Hajishirzi",
          "O. Etzioni",
          "N. Kushman"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "IL_Dataset_2015",
        "entity_type": "Dataset",
        "name": "IL Dataset",
        "description": "包含562个算术问题的数据集，每个问题可以通过一个操作解决",
        "domain": "自然语言处理",
        "size": 562,
        "year": 2015,
        "creators": [
          "S. Roy",
          "T. Vieira",
          "D. Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Commoncore_Dataset_2016",
        "entity_type": "Dataset",
        "name": "Commoncore Dataset",
        "description": "包含600个多步算术问题的新数据集",
        "domain": "自然语言处理",
        "size": 600,
        "year": 2016,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Relax_Accuracy_Classification",
        "entity_type": "Metric",
        "name": "Relax Accuracy",
        "description": "放松准确率（部分正确）",
        "category": "分类评估",
        "formula": "部分正确的样本数/总样本数"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Strict_Accuracy_Classification",
        "entity_type": "Metric",
        "name": "Strict Accuracy",
        "description": "严格准确率（完全正确）",
        "category": "分类评估",
        "formula": "完全正确的样本数/总样本数"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2016_RelationKnowledgePoweredModel",
        "entity_type": "Algorithm",
        "name": "Relation Knowledge Powered Model (RK)",
        "title": "Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding",
        "year": 2016,
        "authors": [
          "Huazheng Wang",
          "Fei Tian",
          "Bin Gao",
          "Chengjieren Zhu",
          "Jiang Bian",
          "Tie-Yan Liu"
        ],
        "task": "自动解答语言理解类IQ测试题",
        "dataset": [
          "wiki2014",
          "自定义IQ测试集"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "问题分类器",
            "词义和关系嵌入模型",
            "特定解题器"
          ],
          "connections": [
            "问题分类器 -> 词义和关系嵌入模型 -> 特定解题器"
          ],
          "mechanisms": [
            "多义词聚类",
            "关系知识嵌入",
            "翻译距离最小化"
          ]
        },
        "methodology": {
          "training_strategy": [
            "负采样",
            "多义词聚类",
            "关系知识嵌入"
          ],
          "parameter_tuning": [
            "窗口大小",
            "嵌入维度",
            "负采样次数",
            "训练轮次"
          ]
        },
        "feature_processing": [
          "上下文窗口表示",
          "TF-IDF加权",
          "球形k均值聚类"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "wiki2014_2014",
        "entity_type": "Dataset",
        "name": "wiki2014",
        "description": "来自维基百科的大规模文本快照",
        "domain": "自然语言处理",
        "size": 3400000000,
        "year": 2014,
        "creators": [
          "Wikipedia Contributors"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CustomIQTestSet_2016",
        "entity_type": "Dataset",
        "name": "自定义IQ测试集",
        "description": "从已出版的IQ测试书中收集的语言理解类问题及其答案",
        "domain": "心理学",
        "size": 232,
        "year": 2016,
        "creators": [
          "Huazheng Wang",
          "Fei Tian",
          "Bin Gao",
          "Chengjieren Zhu",
          "Jiang Bian",
          "Tie-Yan Liu"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bengio2003_NeuralProbabilisticLanguageModel",
        "entity_type": "Algorithm",
        "name": "Neural Probabilistic Language Model",
        "year": 2003,
        "authors": [
          "Yoshua Bengio",
          "Rejean Ducharme",
          "Pascal Vincent",
          "Christian Jauvin"
        ],
        "task": "语言建模",
        "dataset": [
          "未指定"
        ],
        "metrics": [
          "未指定"
        ],
        "architecture": {
          "components": [
            "神经网络"
          ],
          "connections": [
            "未指定"
          ],
          "mechanisms": [
            "概率语言模型"
          ]
        },
        "methodology": {
          "training_strategy": [
            "未指定"
          ],
          "parameter_tuning": [
            "未指定"
          ]
        },
        "feature_processing": [
          "未指定"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Blei2003_LatentDirichletAllocation",
        "entity_type": "Algorithm",
        "name": "Latent Dirichlet Allocation (LDA)",
        "year": 2003,
        "authors": [
          "David M Blei",
          "Andrew Y Ng",
          "Michael I Jordan"
        ],
        "task": "主题建模",
        "dataset": [
          "wiki2014"
        ],
        "metrics": [
          "未指定"
        ],
        "architecture": {
          "components": [
            "潜在狄利克雷分配"
          ],
          "connections": [
            "未指定"
          ],
          "mechanisms": [
            "主题分布"
          ]
        },
        "methodology": {
          "training_strategy": [
            "未指定"
          ],
          "parameter_tuning": [
            "主题数量"
          ]
        },
        "feature_processing": [
          "未指定"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Collobert2008_UnifiedArchitectureForNLP",
        "entity_type": "Algorithm",
        "name": "Unified Architecture for Natural Language Processing",
        "year": 2008,
        "authors": [
          "Ronan Collobert",
          "Jason Weston"
        ],
        "task": "自然语言处理",
        "dataset": [
          "未指定"
        ],
        "metrics": [
          "未指定"
        ],
        "architecture": {
          "components": [
            "深度神经网络"
          ],
          "connections": [
            "多任务学习"
          ],
          "mechanisms": [
            "未指定"
          ]
        },
        "methodology": {
          "training_strategy": [
            "未指定"
          ],
          "parameter_tuning": [
            "未指定"
          ]
        },
        "feature_processing": [
          "未指定"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Pennington2014_Glove",
        "entity_type": "Algorithm",
        "name": "Glove",
        "year": 2014,
        "authors": [
          "Jeffrey Pennington",
          "Richard Socher",
          "Christopher D Manning"
        ],
        "task": "词向量表示",
        "dataset": [
          "wiki2014"
        ],
        "metrics": [
          "未指定"
        ],
        "architecture": {
          "components": [
            "全局矩阵分解"
          ],
          "connections": [
            "共现矩阵"
          ],
          "mechanisms": [
            "全局统计信息"
          ]
        },
        "methodology": {
          "training_strategy": [
            "未指定"
          ],
          "parameter_tuning": [
            "未指定"
          ]
        },
        "feature_processing": [
          "未指定"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Huang2012_MultiSenseWordEmbedding",
        "entity_type": "Algorithm",
        "name": "Multi-Sense Word Embedding",
        "year": 2012,
        "authors": [
          "Eric H Huang",
          "Richard Socher",
          "Christopher D Manning",
          "Andrew Y Ng"
        ],
        "task": "多义词嵌入",
        "dataset": [
          "未指定"
        ],
        "metrics": [
          "未指定"
        ],
        "architecture": {
          "components": [
            "多义词嵌入"
          ],
          "connections": [
            "未指定"
          ],
          "mechanisms": [
            "全局上下文",
            "多个词原型"
          ]
        },
        "methodology": {
          "training_strategy": [
            "未指定"
          ],
          "parameter_tuning": [
            "未指定"
          ]
        },
        "feature_processing": [
          "未指定"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Alvin2014_ProblemGenerationAlgorithm",
        "entity_type": "Algorithm",
        "name": "Problem Generation Algorithm",
        "title": "Synthesis of Geometry Proof Problems",
        "year": 2014,
        "authors": [
          "Chris Alvin",
          "Sumit Gulwani",
          "Rupak Majumdar",
          "Supratik Mukhopadhyay"
        ],
        "task": "Geometry Proof Problem Generation",
        "dataset": [],
        "metrics": [
          "Number of Generated Problems",
          "Time Taken to Generate Problems"
        ],
        "architecture": {
          "components": [
            "Hypergraph Construction",
            "Minimal Assumption Generation",
            "Strictly Interesting Problem Synthesis"
          ],
          "connections": [
            "Hypergraph Construction -> Minimal Assumption Generation -> Strictly Interesting Problem Synthesis"
          ],
          "mechanisms": [
            "Hypergraph Reachability",
            "Fixed-Point Procedure",
            "Non-Deterministic Choices"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Implicit Facts",
          "Explicit Facts",
          "Axioms"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Number_of_Generated_Problems",
        "entity_type": "Metric",
        "name": "Number of Generated Problems",
        "description": "The number of geometry proof problems generated by the algorithm",
        "category": "Problem Generation Evaluation",
        "formula": "Total number of problems generated"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Time_Taken_to_Generate_Problems",
        "entity_type": "Metric",
        "name": "Time Taken to Generate Problems",
        "description": "The time taken by the algorithm to generate geometry proof problems",
        "category": "Efficiency Evaluation",
        "formula": "Average time per figure"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Completeness_of_Problems",
        "entity_type": "Metric",
        "name": "Completeness",
        "description": "Whether the problem is completely defined by the assumptions",
        "category": "Problem Quality Evaluation",
        "formula": "Boolean classification of completeness"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2019_TemplateBasedSolver",
        "entity_type": "Algorithm",
        "name": "Template-Based Math Word Problem Solver",
        "title": "Template-Based Math Word Problem Solvers with Recursive Neural Networks",
        "year": 2019,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Jipeng Zhang",
          "Xing Xu",
          "Lianli Gao",
          "Bing Tian Dai",
          "Heng Tao Shen"
        ],
        "task": "自动求解数学文字题",
        "dataset": [
          "Math23K_2017",
          "MAWPS_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Seq2Seq模型",
            "Bi-LSTM",
            "Self Attention",
            "Recursive Neural Network"
          ],
          "connections": [
            "Seq2Seq模型预测树结构模板",
            "Bi-LSTM编码数量",
            "Self Attention捕捉长距离依赖",
            "Recursive Neural Network推断未知操作符"
          ],
          "mechanisms": [
            "方程模板归一化",
            "操作符封装"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Adam优化器",
            "SGD优化器"
          ],
          "parameter_tuning": [
            "学习率调整",
            "动量因子设置"
          ]
        },
        "feature_processing": [
          "词嵌入",
          "自注意力机制"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_DNS",
        "entity_type": "Algorithm",
        "name": "DNS",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": 2017,
        "authors": [
          "Lei Wang",
          "Yongquan Liu",
          "Shuming Shi"
        ],
        "task": "自动求解数学文字题",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Seq2Seq模型",
            "LSTM",
            "GRU"
          ],
          "connections": [
            "Seq2Seq模型直接生成数学表达式"
          ],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Robaidek2018_GenerationModel",
        "entity_type": "Algorithm",
        "name": "Generation Model",
        "title": "Data-driven methods for solving algebra word problems",
        "year": 2018,
        "authors": [
          "Ben Robaidek",
          "Rik Koncel-Kedziorski",
          "Hannaneh Hajishirzi"
        ],
        "task": "自动求解代数文字题",
        "dataset": [
          "MAWPS_2016",
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "LSTM",
            "CNN"
          ],
          "connections": [
            "Seq2Seq模型直接生成数学表达式"
          ],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018b_MathDQN",
        "entity_type": "Algorithm",
        "name": "MathDQN",
        "title": "MathDQN: Solving arithmetic word problems via deep reinforcement learning",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingwen Song",
          "Lei Guo",
          "Heng Tao Shen"
        ],
        "task": "自动求解算术文字题",
        "dataset": [
          "MAWPS_2016"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Deep Reinforcement Learning",
            "Expression Tree"
          ],
          "connections": [
            "迭代构建表达式树"
          ],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_EquationNormalization",
        "entity_type": "Algorithm",
        "name": "Equation Normalization",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Expression Tree",
            "Normalization Rules"
          ],
          "connections": [
            "Rule 1",
            "Rule 2",
            "Bracket Elimination"
          ],
          "mechanisms": [
            "Order Duplication Handling",
            "Bracket Duplication Handling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SEQ2SEQ Framework"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2017_SEQ2SEQFramework",
        "entity_type": "Algorithm",
        "name": "SEQ2SEQ Framework",
        "year": 2017,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Attention Mechanism"
          ],
          "connections": [
            "Bidirectional LSTM",
            "Convolutional SEQ2SEQ",
            "Transformer"
          ],
          "mechanisms": [
            "Global Attention",
            "Multi-head Self-attention"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Adam Optimizer",
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_DeepNeuralSolver",
        "entity_type": "Algorithm",
        "name": "Deep Neural Solver",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "SEQ2SEQ Model",
            "GRU Encoder",
            "LSTM Decoder"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Adam Optimizer",
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_DNSHybrid",
        "entity_type": "Algorithm",
        "name": "DNS-Hybrid",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "SEQ2SEQ Model",
            "Retrieval-based Solver"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Adam Optimizer",
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_BiLSTM",
        "entity_type": "Algorithm",
        "name": "BiLSTM",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM"
          ],
          "connections": [
            "Two-layer Bi-LSTM Encoder",
            "Two-layer LSTM Decoder"
          ],
          "mechanisms": [
            "Global Attention"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Adam Optimizer",
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Gehring2017_ConvolutionalSEQ2SEQ",
        "entity_type": "Algorithm",
        "name": "Convolutional SEQ2SEQ",
        "year": 2017,
        "authors": [
          "Jonas Gehring",
          "Michael Auli",
          "David Grangier",
          "Denis Yarats",
          "Yann N Dauphin"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Convolutional Layers"
          ],
          "connections": [
            "Four-layer Encoder",
            "Three-layer Decoder"
          ],
          "mechanisms": [
            "Gate Linear Units"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Early Stopping",
            "Learning Rate Annealing"
          ],
          "parameter_tuning": [
            "Adam Optimizer",
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Vaswani2017_Transformer",
        "entity_type": "Algorithm",
        "name": "Transformer",
        "year": 2017,
        "authors": [
          "Ashish Vaswani",
          "Noam Shazeer",
          "Niki Parmar",
          "Jakob Uszkoreit",
          "Llion Jones",
          "Aidan N Gomez",
          "Łukasz Kaiser",
          "Illia Polosukhin"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Multi-head Self-attention",
            "Position-wise Fully-connected Feed-forward Network"
          ],
          "connections": [
            "Four-layer Stack"
          ],
          "mechanisms": [
            "Self-attention"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Adam Optimizer",
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Wang2018_EnsembleModel",
        "entity_type": "Algorithm",
        "name": "Ensemble Model",
        "year": 2018,
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": "Math Word Problem Solving",
        "dataset": [
          "Math23K_2017"
        ],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "BiLSTM",
            "Convolutional SEQ2SEQ",
            "Transformer"
          ],
          "connections": [
            "Generation Probability Selection"
          ],
          "mechanisms": [
            "Model Combination"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Adam Optimizer",
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Significant Number Identification"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Fletcher1985_WORDPRO",
        "entity_type": "Algorithm",
        "name": "WORDPRO",
        "title": "Understanding and solving arithmetic word problems: A computer simulation",
        "year": 1985,
        "authors": [
          "Fletcher, C. R."
        ],
        "task": "理解并解决算术文字问题",
        "dataset": [],
        "metrics": [
          "Run-Time Statistics"
        ],
        "architecture": {
          "components": [
            "Production Rules",
            "Set Schema",
            "Transfer Schema",
            "Superset Schema",
            "More-Than/Less-Than Schema"
          ],
          "connections": [
            "STM to LTM",
            "Text Base to Problem Model"
          ],
          "mechanisms": [
            "Meaning Postulates",
            "Arithmetic Strategies",
            "Problem-Solving Procedures"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Simulating third-grade children's problem-solving processes"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Propositional Representation",
          "Bilevel Representation"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "RunTimeStatistics_ProblemSolving",
        "entity_type": "Metric",
        "name": "Run-Time Statistics",
        "description": "程序运行时统计信息",
        "category": "程序性能评估",
        "formula": "包括触发的生产规则数量、转换次数、LTM搜索次数以及跨处理周期保留的最大块数"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Novak1990_BEATRIX",
        "entity_type": "Algorithm",
        "name": "BEATRIX",
        "title": "Understanding Natural Language with Diagrams",
        "year": 1990,
        "authors": [
          "Novak, G. S.",
          "Bulko, W."
        ],
        "task": "理解物理问题中的自然语言和图表",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "English-parser",
            "Diagram-parser",
            "Coreference-resolver"
          ],
          "connections": [
            "TEXT-MODEL",
            "PICTURE-MODEL",
            "PROBLEM-MODEL"
          ],
          "mechanisms": [
            "Blackboard architecture",
            "Opportunistic co-parsers"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "自然语言处理",
          "图表解析",
          "核心ference解析"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "PhysicsProblems_1990",
        "entity_type": "Dataset",
        "name": "Physics Problems",
        "description": "包含自然语言文本和图表的物理问题数据集",
        "domain": "物理问题求解",
        "size": null,
        "year": 1990,
        "creators": [
          "Novak, G. S.",
          "Bulko, W."
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Correctness_ProblemModel",
        "entity_type": "Metric",
        "name": "Correctness",
        "description": "问题模型的正确性",
        "category": "问题理解评估",
        "formula": "正确理解的问题数量 / 总问题数量"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bulko1988_BEATRIX",
        "entity_type": "Algorithm",
        "name": "BEATRIX",
        "title": "Understanding Text With an Accompanying Diagram",
        "year": 1988,
        "authors": [
          "William C. Bulko"
        ],
        "task": "Physics Problem Solving",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Graphic Interface",
            "Blackboard System",
            "Understanding Module",
            "Parsing Module"
          ],
          "connections": [
            "Text Parsing -> Blackboard",
            "Picture Parsing -> Blackboard",
            "Coreference Resolution -> Problem Model"
          ],
          "mechanisms": [
            "Coreference Resolution",
            "Object Identification",
            "Semantic Network Modeling"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Correction Facility",
          "Special Point Detection",
          "Object Hypothesis Generation"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "CollegeLevelPhysicsTextbooks_1988",
        "entity_type": "Dataset",
        "name": "College-Level Physics Textbooks",
        "description": "Collection of physics problems from college-level textbooks",
        "domain": "Physics",
        "size": "Not specified",
        "year": 1988,
        "creators": [
          "Various Authors"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Correctness_Completeness",
        "entity_type": "Metric",
        "name": "Correctness and Completeness",
        "description": "Validation of the correctness and completeness of the generated problem model",
        "category": "Model Validation",
        "formula": "Not specified"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Bengtson2008_PairwiseCoreferenceModel",
        "entity_type": "Algorithm",
        "name": "Pairwise Coreference Model",
        "title": "Understanding the Value of Features for Coreference Resolution",
        "year": 2008,
        "authors": [
          "Eric Bengtson",
          "Dan Roth"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE 2004 English Training Data"
        ],
        "metrics": [
          "B-Cubed F-Score",
          "MUC F-Score"
        ],
        "architecture": {
          "components": [
            "Pairwise Coreference Function",
            "Best-Link Decision Model",
            "Averaged Perceptron Learning Algorithm"
          ],
          "connections": [
            "Pairwise Coreference Function -> Best-Link Decision Model"
          ],
          "mechanisms": [
            "Pairwise Classification",
            "Graph-based Linking"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Averaged Perceptron"
          ],
          "parameter_tuning": [
            "Threshold Optimization",
            "Regularization Parameter Tuning"
          ]
        },
        "feature_processing": [
          "Mention Types",
          "String Relation Features",
          "Semantic Features",
          "Relative Location Features",
          "Learned Features",
          "Aligned Modifiers",
          "Memorization Features",
          "Predicted Entity Types"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "ACE2004_EnglishTrainingData",
        "entity_type": "Dataset",
        "name": "ACE 2004 English Training Data",
        "description": "A dataset for coreference resolution provided by NIST",
        "domain": "Natural Language Processing",
        "size": 336,
        "year": 2004,
        "creators": [
          "NIST"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "B-Cubed_F-Score",
        "entity_type": "Metric",
        "name": "B-Cubed F-Score",
        "description": "A measure of the overlap of predicted clusters and true clusters",
        "category": "Clustering Evaluation",
        "formula": "Harmonic Mean of Precision and Recall"
      }
    },
    {
      "metric_entity": {
        "metric_id": "MUC_F-Score",
        "entity_type": "Metric",
        "name": "MUC F-Score",
        "description": "Official MUC scoring algorithm",
        "category": "Clustering Evaluation",
        "formula": "Harmonic Mean of Precision and Recall"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Ng2002_BestLinkDecisionModel",
        "entity_type": "Algorithm",
        "name": "Best-Link Decision Model",
        "title": "Improving Machine Learning Approaches to Coreference Resolution",
        "year": 2002,
        "authors": [
          "Vincent Ng",
          "Claire Cardie"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE 2004 English Training Data"
        ],
        "metrics": [
          "B-Cubed F-Score",
          "MUC F-Score"
        ],
        "architecture": {
          "components": [
            "Best-Link Decision Model"
          ],
          "connections": [],
          "mechanisms": [
            "Pairwise Classification",
            "Graph-based Linking"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Culotta2007_ComplexCoreferenceModel",
        "entity_type": "Algorithm",
        "name": "Complex Coreference Model",
        "title": "First-Order Probabilistic Models for Coreference Resolution",
        "year": 2007,
        "authors": [
          "Arnie Culotta",
          "Michael Wick",
          "Robert Hall",
          "Andrew McCallum"
        ],
        "task": "Coreference Resolution",
        "dataset": [
          "ACE 2004 English Training Data"
        ],
        "metrics": [
          "B-Cubed F-Score",
          "MUC F-Score"
        ],
        "architecture": {
          "components": [
            "Complex Coreference Model"
          ],
          "connections": [],
          "mechanisms": [
            "Non-Pairwise Classification",
            "Partial Clusters"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2017_UNITDEP",
        "entity_type": "Algorithm",
        "name": "UNITDEP",
        "title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
        "year": 2017,
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "Arithmetic Word Problem Solving",
        "dataset": [
          "AllArith",
          "AllArithLex",
          "AllArithTmpl"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Vertex Classifier",
            "Edge Classifier",
            "Constrained Inference Module"
          ],
          "connections": [
            "Vertex Classifier -> Constrained Inference Module",
            "Edge Classifier -> Constrained Inference Module"
          ],
          "mechanisms": [
            "Decomposed Model",
            "Joint Inference Procedure"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Beam Search"
          ],
          "parameter_tuning": [
            "Scaling Parameters (λIRR, λVERTEX, λEDGE)"
          ]
        },
        "feature_processing": [
          "Context Features",
          "Rule-based Extraction Features"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArithLex_2017",
        "entity_type": "Dataset",
        "name": "AllArithLex",
        "description": "Subset of AllArith with low lexical overlap",
        "domain": "Natural Language Processing",
        "size": 415,
        "year": 2017,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "AllArithTmpl_2017",
        "entity_type": "Dataset",
        "name": "AllArithTmpl",
        "description": "Subset of AllArith with low template overlap",
        "domain": "Natural Language Processing",
        "size": 415,
        "year": 2017,
        "creators": [
          "Subhro Roy",
          "Dan Roth"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Roy2015_LCA++",
        "entity_type": "Algorithm",
        "name": "LCA++",
        "title": "Solving General Arithmetic Word Problems",
        "year": 2015,
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "Arithmetic Word Problem Solving",
        "dataset": [
          "AllArith",
          "AllArithLex",
          "AllArithTmpl"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Irrelevance Classifier",
            "LCA Operation Classifier"
          ],
          "connections": [
            "Irrelevance Classifier -> Constrained Inference Module",
            "LCA Operation Classifier -> Constrained Inference Module"
          ],
          "mechanisms": [
            "Monotonic Expression Tree"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Scaling Parameter (λIRR)"
          ]
        },
        "feature_processing": [
          "Context Features"
        ]
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kushman2014_TEMPLATE",
        "entity_type": "Algorithm",
        "name": "TEMPLATE",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": 2014,
        "authors": [
          "N. Kushman",
          "L. Zettlemoyer",
          "R. Barzilay",
          "Y. Artzi"
        ],
        "task": "Algebra Word Problem Solving",
        "dataset": [
          "Algebra Word Problems"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Template Matching"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Template-based Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Koncel-Kedziorski2015_SINGLEEQ",
        "entity_type": "Algorithm",
        "name": "SINGLEEQ",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": 2015,
        "authors": [
          "R. Koncel-Kedziorski",
          "H. Hajishirzi",
          "A. Sabharwal",
          "O. Etzioni",
          "S. Ang"
        ],
        "task": "Single Equation Word Problem Solving",
        "dataset": [
          "Single Equation Word Problems"
        ],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Parsing"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": []
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Antol2015_VQA",
        "entity_type": "Algorithm",
        "name": "VQA",
        "title": "VQA: Visual Question Answering",
        "year": 2015,
        "authors": [
          "Stanislaw Antol",
          "Aishwarya Agrawal",
          "Jiasen Lu",
          "Margaret Mitchell",
          "Dhruv Batra",
          "C. Lawrence Zitnick",
          "Devi Parikh"
        ],
        "task": "Visual Question Answering",
        "dataset": [
          "MS COCO_2014",
          "Abstract Scenes_2015"
        ],
        "metrics": [
          "Accuracy_OpenAnswer",
          "Accuracy_MultipleChoice"
        ],
        "architecture": {
          "components": [
            "Multi-Layer Perceptron (MLP)",
            "LSTM"
          ],
          "connections": [
            "Question to MLP",
            "Image to MLP",
            "Question to LSTM",
            "Image to LSTM"
          ],
          "mechanisms": [
            "Element-wise Multiplication",
            "Softmax Layer"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Concatenation of Features"
          ],
          "parameter_tuning": [
            "Dropout",
            "Hidden Units",
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Bag-of-Words",
          "VGGNet Features",
          "One-Hot Encoding"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "MS COCO_2014",
        "entity_type": "Dataset",
        "name": "MS COCO",
        "description": "Microsoft Common Objects in Context dataset",
        "domain": "Computer Vision",
        "size": 204721,
        "year": 2014,
        "creators": [
          "Tsung-Yi Lin",
          "Michael Maire",
          "Serge Belongie",
          "James Hays",
          "Peter Perona",
          "Devon Ramanan",
          "Piotr Dollar",
          "C. Lawrence Zitnick"
        ]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Abstract Scenes_2015",
        "entity_type": "Dataset",
        "name": "Abstract Scenes",
        "description": "Dataset of abstract scenes for VQA",
        "domain": "Computer Vision",
        "size": 50000,
        "year": 2015,
        "creators": [
          "Stanislaw Antol",
          "C. Lawrence Zitnick",
          "Devi Parikh"
        ]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_OpenAnswer",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Accuracy for open-answer task",
        "category": "Classification Evaluation",
        "formula": "min(# humans that provided that answer / 3, 1)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_MultipleChoice",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Accuracy for multiple-choice task",
        "category": "Classification Evaluation",
        "formula": "min(# humans that provided that answer / 3, 1)"
      }
    }
  ],
  "is_complete": true,
  "extraction_time": 1748341714.893279
}