{"text": "【文档名】\nThe Gap of Semantic Parsing A Survey on Automatic(2019).pdf\n【文档类型】\npdf\n【文档内容】\narX iv:1 808 .07 290 v2 [cs .CL ]  2 9 A pr 2 019\n1\nThe Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers\nDongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai and Heng Tao Shen\nAbstract—Solving mathematical word problems(MWPs) automatically is challenging, primarily due to the semantic gap between human-readable words and machine-understandable logics. Despite the long history dated back to the 1960s, MWPs have regained intensive attention in the past few years with the advancement of Artificial Intelligence(AI). Solving MWPs successfully is considered as a milestone towards general AI. Many systems have claimed promising results in self-crafted and small-scale datasets. However, when applied on large and diverse datasets, none of the proposed methods in the literature achieves high precision, revealing that current MWP solvers still have much room for improvement. This motivated us to present a comprehensive survey to deliver a clear and complete picture of automatic math problem solvers. In this survey, we emphasize on algebraic word problems, summarize their extracted features and proposed techniques to bridge the semantic gap, and compare their performance in the publicly accessible datasets. We also cover automatic solvers for other types of math problems such as geometric problems that require the understanding of diagrams. Finally, we identify several emerging research directions for the readers with interests in MWPs.\nIndex Terms—math word problem, semantic parser, reasoning, survey, natural language processing, machine learning\n✦\n1 INTRODUCTION\nDesigning an automatic solver for mathematical word problems (MWPs) has a long history dated back to the 1960s[1],[2], [3], and continues to attract intensive research attention. In the past three years, more than 40 publications on this topic have emerged in the premier venues of artificial intelligence. The problem is particularly challenging because there remains a wide semantic gap to parse the human-readable words into machine-understandable logics so as to facilitate quantitative reasoning.\nHence, MWPs solvers are broadly considered as good test beds to evaluate the intelligence level of agents in terms of natural language understanding[4],[5] and the successful solving of MWPs would constitute a milestone towards general AI.\nWe categorize the evolution of MWP solvers into three major stages according to the technologies behind them, as depicted in Figure 1. In the first pioneering stage, roughly from the year 1960 to 2010, systems such as STUDENT[1], DEDUCOM[6], WORDPRO[7] and ROBUST[8], manually craft rules and schemas for pattern matchings. Thereupon, these solvers heavily rely on human interventions and can only resolve a limited number of scenarios that are defined in advance. Those early efforts for automatic understanding of natural language mathematical problems have been thoroughly reviewed in[9]. We exclude them from the scope of this survey paper and focus on the recent technology developments that have not been covered in the previous survey[9].\nIn the second stage, MWP solvers made use of semantic parsing[10],[11], with the objective of mapping the sentences\n• D. Zhang and L. Zhang are with the College of Computer Science and Technology, Zhejiang University, China. Emails:{zhangdongxi-ang37,zglumg}@gmail.com;\n• L. Wang and H. T. Shen are with the Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China. Emails: demolei@outlook.com; shenhengtao@hotmail.com\n• B. T. Dai is with School of Information Systems, Singapore Management University. Email:btdai@smu.edu.sg\n• Corresponding Author: Heng Tao Shen.\n1960-2010 2017-2019 2011-2017\n| |Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| ---|---|---|\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing|Deep Learning\tReinforcement Learning|\nFig. 1. Technology evolving trend in solving MWPs.\nfrom problem statements into structured logic representations so as to facilitate quantitative reasoning. It has regained considerable interests from the academic community, and a booming number of methods have been proposed in the past years. These methods leveraged various strategies of feature engineering and statistical learning for performance boosting. The authors of these methods also claimed promising results in their public or manually harvested datasets. In this paper, one of our tasks is to present a comprehensive review on the proposed methods in this stage. The methods will be initially organized according to the sub-tasks of MWPs which they were designed to solve, such as arithmetic word problem(in Section 2), equation set word problem(in Section 3) and geometric word problem(in Section 5). We then examine the proposed techniques in each sub-task with a clear technical organization and accountable experimental evaluations.\nMWP solvers in the third stage were originated from an empirical work[12]. Its experimental results on a large-scale and diversified dataset showed that the status of MWP solvers was not as optimistic as they claimed to be. In fact, the accuracies of many approaches dropped sharply and there is a great room for improvement in this research area. To design more accurate and robust solutions, the subsequent publications are forked into two directions. One is to continue refining the technology of semantic parsing. For instance, Huang et al.\n2\nproposed a new type of semantic representation to conduct fine-grained inference[13]. The other direction attempts to exploit the advantages of deep learning models, with the availability of large-scale training datasets. This is an emerging research direction for MWP solvers and we observed multiple instances, including Deep Neural Solver[14], Seq2SeqET[15], StackDecoder[16], MathDQN[17], CASS[18], T-RNN[19]. These models represent a new technology trend in the research topic of MWP solvers and will be paid special attention in the survey.\nTo sum up, we present a comprehensive survey to review the MWP solvers proposed in recent years. Researchers in the community can benefit from this survey in the following ways:\n1) We provide a wide coverage on the math word problems, including arithmetic word problem, equation set problem, geometry word problem and miscellaneous sub-tasks related to automatic math solvers. The practitioners can easily identify all relevant approaches for performance evaluations.\nWe observed that the unawareness of relevant competitors occurs occasionally in the past literature. We are positive that the availability of our survey can help avoid such unawareness.\n2) The solvers designed for arithmetic word problems(AWP) with only one unknown variable and equation set problems (ESP) with multiple unknown variables are often not differ-entiated by previous works. In fact, the methods proposed for ESP are more general and can be used to solve AWP. In this survey, we clearly identify the difference and organize them in separate sections.\n3) Feature engineering plays a vital role to bridge the gap of semantic parsing. Almost all MWP solvers state their strategies of crafting effective features, resulting in a very diversified group of features, and there is a lack of clear organization among these features. In this survey, we will be the first to summarize all these proposed features in Table 5.\n4) As for the fairness on performance evaluations, ideally, there should be a benchmark dataset well accepted and widely adopted by the MWP research community, just like Ima-geNet[20] for visual object recognition and VQA[21],[22] for visual question answering. Unfortunately, we observed that many approaches tend to compile their own datasets to verify their superiorities, resulting in missing of relevant competitors as mentioned above. Tables 2 and 3 integrate the results of the existing methods on all public datasets.\nAfter collecting the accuracies that have been reported in the past literature, we observed many empty cells in the table. Each empty cell refers to a missing experiment on a particular algorithm and dataset. In this survey, we make our best efforts to fill the missing results by conducting a number of additional experiments, that allow us to provide a more comprehensive comparison and explicit analysis.\nThe remainder of the paper is organized as follows. We first review the arithmetic word problem solvers in Section 2, followed by equation set problem solvers in Section 3. Since feature engineering deserves special attention, we summarize the extracted features as well as the associated pre-processing techniques in Section 4. The geometric word problem solvers are reviewed in Section 5. We also cover miscellaneous automatic solvers related to math problems in Section 6. We conclude the paper and point out several future directions in MWPs that are worth examination in the final section.\n2 ARITHMETIC WORD PROBLEM SOLVER\nThe arithmetic word problems are targeted for elementary school students. The input is the text description for the math problem, represented in the form of a sequence of k\nwords 〈w0,w1,...,wk〉. There are n quantities q1,q2,...,qn\nmentioned in the text and an unknown variable x whose value is to be resolved. Our goal is to extract the relevant quantities and map this problem into an arithmetic expression E whose evaluation value provides the solution to the problem. There are only four types of fundamental operators O={+,−,×,÷} involved in the expression E.\nAn example of arithmetic word problem is illustrated in Figure 2. The relevant quantities to be extracted from the text include 17, 7 and 80. The number of hours spent on the bike is the unknown variable x. To solve the problem, we need to identify the correct operators between the quantities and their operation order such that we can obtain the final equation 17+ 7x= 80 or expression x=(80− 17)÷ 7 and return 9 as the solution to this problem.\nFig. 2. An example of arithmetic word problem.\nIn this section, we consider feature extraction as a black box and focus on the high-level algorithms and models. The details of feature extraction will be comprehensively present in Section 4. We classify existing algebra word problem solvers into three categories:\n| rule-based,|statistic-based,|tree-based and|\n| ---|---|---|\ndeep learning(DL)-based methods.\n2.1 Rule-based Methods\nThe early approaches to math word problems are rule-based systems based on hand engineering. Published in 1985, WORD-PRO[7] solves one-step arithmetic problems. It predefines four types of schemas, including change-in, change-out, combine and compare. The problem text is transformed into a set of propositions and the answer is derived with simple reasoning based on the propositions. Another system ROBUST, developed by Bakman[8], could understand free-format multi-step arith-metic word problems. It further expands the change schema of WORDPRO[7] into six distinct categories. The problem text is split into sentences and each sentence is mapped to a proposition.\nYun et al. also proposed to use schema for multi-step math problem solving[23]. However, the implementation details are not explicitly revealed. Since these systems have been out of date, we only provide such a brief overview to cover the representative ones. Readers can refer to[9] for a comprehensive survey of\n3\nearly rule-driven systems for automatic understanding of natural language math problems.\n2.2 Statistic-based Methods\nThe statistic-based methods leverage traditional machine learning models to identify the entities, quantities and operators from the problem text and yield the numeric answer with simple logic inference procedure. The scheme of quantity entailment proposed in[24] can be used to solve arithmetic problems with only one operator. It involves three types of classifiers to detect different properties of the word problem. The quantity pair classifier is trained to determine which pair of quantities would be used to derive the answer. The operator classifier picks the operator op ∈{+,−,×,÷} with the highest probability. The order classifier is relevant only for problems involving subtraction or division because the order of operands matters for these two types of operators. With the inferred expression, it is straightforward to calculate the numeric answer for the simple math problem.\nTo solve math problems with multi-step arithmetic expression, the statistic-based methods require more advanced logic templates.\nThis usually incurs additional overhead to annotate the text problems and associate them with the introduced template. As an early attempt, ARIS[25] defines a logic template named state that consists of a set of entities, their containers, attributes, quantities and relations. For example, “Liz has 9 black kittens” initializes the number of kitten(referring to an entity) with black color(referring to an attribute) and belonging to Liz (referring to a container). The solution splits the problem text into fragments and tracks the update of the states by verb categorization. More specifically, the verbs are classified into seven categories: observation, positive, negative, positive transfer, negative transfer, construct and destroy. To train such a classifier, we need to annotate each split fragment in the training dataset with the associated verb category. Another drawback of ARIS is that it only supports addition and subtraction.[26] follows a similar processing logic to ARIS. It predefines a corpus of logic representation named schema, inspired by[8]. The sentences in the text problem are examined sequentially until the sentence matches a schema, triggering an update operation to modify the number associated with the entities.\nMitra et al. proposed a new logic template named formula[27].\nThree types of formulas are defined, including part whole, change and comparison, to solve problems with addition and subtraction operators. For example, the text problem “Dan grew 42 turnips and 38 cantelopes. Jessica grew 47 turnips. How many turnips did they grow in total?” is annotated with the part-whole template: 〈whole: x,parts:{42,47}〉. To solve a math problem, the first step connects the assertions to the formulas. In the second step, the most probable formula is identified using the log-linear model with learned parameters and converted into an algebraic equation.\nAnother type of annotation is introduced in[28],[29] to facilitate solving a math problem. A group of logic forms are predefined and the problem text is converted into the logic form representation by certain mapping rules. For instance, the sentence “Fred picks 36 limes” will be transformed into verb(v1,pick) & nsubj(v1,Fred)& dobj(v1,n1)& head(n1,lime)& nummod(n1,36). Finally, logic inference is performed on the derived logic statements to obtain the answer.\nTo sum up, these statistical-based methods have two draw-backs that limit their usability. First, it requires additional an-notation overhead that prevents them from handling large-scale\ndatasets. Second, these methods are essentially based on a set of pre-defined templates, which are brittle and rigid. It will take great efforts to extend the templates to support other operators like multiplication and division. It is also not robust to diversified datasets. In the following, we will introduce the tree-based solutions, which are widely adopted and become the main-streaming solutions to arithmetic word problems.\n2.3 Tree-Based Methods\nThe arithmetic expression can be naturally represented as a binary tree structure such that the operators with higher priority are placed in the lower level and the root of the tree contains the operator with the lowest priority. The idea of tree-based approaches[30],[31],[32],[17] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner. One of the advantages is that there is no need for additional annotations such as equation template, tags or logic forms. Figure 3 shows two tree examples derived from the math word problem in Figure 2. One is called expression tree that is used in[30],[32],[17] and the other is called equation tree[31]. These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x.\n÷\n-7\n80 17\n=\n+ 80\n17\n7\n×\nx\nExpression Tree Equation Tree\nFig. 3. Examples of expression tree and equation tree for Figure 2.\nThe overall algorithmic framework among the tree-based approaches consists of two processing stages. In the first stage, the quantities are extracted from the text and form the bottom level of the tree. The candidate trees that are syntactically valid, but with different structures and internal nodes, are enumerated.\nIn the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution. A common strategy among these algorithms is to build a local classifier to determine the likelihood of an operator being selected as the internal node. Such local likelihood is taken into account in the global scoring function to determine the likelihood of the entire tree.\nRoy et al.[30] proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems. Its first strategy to reduce the search space is training a binary classifier to determine whether an extracted quantity is relevant or not. Only the relevant ones are used for tree construction and placed in the bottom level. The irrelevant quantities are discarded. The tree construction procedure is mapped to a collection of simple prediction problems, each determining the lowest common ancestor operation between a pair of quantities mentioned in the problem. The global scoring function for an enumerated tree takes into account two terms. The\n4\nfirst one, denoted by φ(q), is the likelihood of quantity q being irrelevant, i.e., q is not used in creating the expression tree. In the ideal case, all the irrelevant quantities are correctly predicted with high confidence, resulting in a large value for the sum of φ(q).\nThe other term, denoted by φ(op), is the likelihood of selecting op as the operator for an internal tree node. With these two factors, Score(E) is formally defined as\nScore(E)= w1\n∑\nq∈I(E)\nφ(q)+ ∑\nop∈N\nφ(op) (1)\nwhere I(E) is the group of irrelevant quantities that are not included in expression E, and N refers to the set of internal tree nodes.\nTo further reduce the tree enumeration space, beam search is applied in[30]. To generate the next state T ′ from the current partial tree, the algorithm avoids choosing all the possible pairs of terms and determining their operator. Instead, only top-k candidates with the highest partial scores are retained.\nExperimental results with k= 200 show that the strategy achieves a good balance between accuracy and running time. The service is also published as a web tool[33] and it can respond promptly to a math word problem.\nALGES[31] differs from[30] in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees. More specifically, ALGES does not discard irrevalent quantities, but enumerates all the syntactically valid trees.\nInteger Linear Programming(ILP) is applied as it can help enforce the constraints such as syntactic validity, type consistence and domain specific simplicity considerations. Consequently, its computation cost is dozens of times higher than that in[30], according to an efficiency evaluation in[17]. Second, its scoring function is different from Equation 1. There is no need for the term φ(q) because ALGES does not build a classifier to check the quantity relevance. Besides the monotonic aggregation of the likelihood from local operator classifiers, the scoring function incorporates a new termφ(P)= θT fp to assign a coherence score for the tree instance. Here, fP is the global feature extracted from a problem text P, and θ refers to the parameter vector.\nThe goal of[34] is also to build an equation tree by parsing the problem text. It makes two assumptions that can simplify the tree construction, but also limit its applicability. First, the final output equation form is restricted to have at most two variables. Second, each quantity mentioned in the sentence can be used at most once in the final equation. The tree construction procedure consists of a pipeline of predictors that identify irrelevant quantities, recognize grounded variables, and generate the final equation tree.\nWith customized feature selection and SVM based classifier, the relevant quantities and variables are extracted and used as the leaf nodes of the equation tree. The tree is built in a bottom-up manner.\nIt is worth noting that to reduce the search space and simplify the tree construction, only adjacent nodes are combined to generate their parent node.\nUnitDep[32] can be viewed as an extension work of[30] by the same authors. An important concept, named Unit Dependency Graph(UDG), is proposed to enhance the scoring function. The vertices in UDG consist of the extracted quantities. If the quantity correspond to a rate(e.g., 8 dollars per hour), the vertex is marked as RATE. There are six types of edge relations to be considered, such as whether two quantities are associated with the same unit.\nBuilding the UDG requires additional annotation overhead as we need to train two classifiers for the nodes and edges. The node classifier determines whether a node is associated with a rate. The\nedge classifier predicts the type of relationship between any pair of quantity nodes. Given a valid unit dependency graph G generated by the classifiers, its likelihood is defined as\nφ(G)= ∑\nv∈G∧LABEL(v)=rate\nP(v)+ λ∑\ne∈G\nP(e) (2)\nIn other words, we sum up the prediction probability for the RATE nodes and all the edges. The new scoring function for an expression tree extends Equation 1 by incorporatingφ(G). Rules are defined to enforce the rate consistence between an expression tree T and a candidate graph G. For example, if vi is the only node in the tree that is labeled RATE and it appears in the question, there should not exist a path from the leaf node to the root which only contains operators of addition and subtraction. Finally, the candidate graph G with the highest likelihood and rate-consistent with T is used to calculate the total score of T.\n2.4 DL-based Methods\nIn recent years, deep learning(DL) has witnessed great success in a wide spectrum of “smart” applications, such as visual question answering[35],[36], video captioning[37],[38],[39], [40], personal interest inference[41],[42],[43] and smart transportation[44]. The main advantage is that with sufficient amount of training data, DL is able to learn an effective feature representation in a data-driven manner without human intervention. It is not surprising to notice that several efforts have been attempted to apply DL for math word problem solving.\nDeep Neural Solver(DNS)[14] is a pioneering work designed for equation set problems, which will be introduced in more details in the next section. Following DNS, there have emerged multiple DL-based solvers for arithmetic word problems.\nSeq2SeqET[15] extended the idea of DNS by using expression tree as the output sequence. In other words, it applied seq2seq model to convert the problem text into an expression tree, which can be viewed as a template. Given the output of an expression tree or template, we can easily infer the numeric answer. To reduce the template space, equation normalization was proposed in Seq2SeqET so that duplicate representation of expression trees can be unified.\nStackDecoder[16] is also based on seq2seq model. Its encoder extracts semantic meanings of quantities in the question text and the decoder is equipped with a stack to facilitate tracking the semantic meanings of operands.\nT-RNN[19] can be viewed as an improvement of Seq2SeqET, in terms of quantity encoding, template representation and tree construction. First, an effective embedding network with Bi-LSTM and self attention is used to vectorize the quantities. Second, the detailed operators in the templates are encapsulated to further reduce the number of template space. For example, n1+n2, n1−n2, n1×n2, and n1÷n2 are mapped to the same template n1〈op〉n2. Third, a recursive neural network is applied to infer the unknown variables in the expression tree in a recursive manner.\nWang et al. made the first attempt of applying deep re-inforcement learning to solve arithmetic word problems[17].\nThe motivation is that deep Q-network has witnessed success in solving various problems with big search space such as playing text-based games[45], information extraction[46], text generation[47] and object detection in images[48]. To fit the math problem scenario, they formulate the expression tree construction as a Markov Decision Process and propose the MathDQN that is customized from the general deep reinforcement\n5\nlearning framework. Technically, they tailor the definitions of states, actions, and reward functions which are key components in the reinforcement learning framework. By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Compared to the aforementioned approaches, MathDQN iteratively picks the best operator for two selected quantities. This procedure can be viewed as beam search with k= 1 when exploiting candidate expression trees. Its deep Q-network acts as the operator classifier and guides the model to select the most promising operator for tree construction.\n2.5 Dataset Repository and Performance Analysis\nThe accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites. These datasets are small-scale and contain hundreds of math problems. In this subsection, we make a summary on the datasets that have been used in the aforementioned papers.\nMoreover, we organize the performance results on these datasets into one unified table. We also make our best efforts to conduct additional experiments. The new results are highlighted in blue color. In this way, readers can easily identify the best performers in each dataset.\n2.5.1 Datasets\nThere have been a number of datasets collected for the arithmetic word problems. We present their descriptions in the following and summarize the statistics of the datasets in Table 1.\n1) AI2[25]. There are 395 single-step or multi-step arithmetic word problems for the third, fourth, and fifth graders. It involves problems that can be solved with only addition and subtraction. The dataset is harvested from two websites: math-aids.com and ixl.com and comprises three subsets: MA1(from math-aids.com), IXL(from ixl.com) and MA2(from math-aids.com). Among them, IXL and MA2 are more challenging than MA1 because IXL contains more information gaps and MA2 includes more irrelevant information in its math problems.\n2) IL[30]. The problems are collected from websites k5learning.com and dadsworksheets.com. The problems that require background knowledge(e.g., “apple is fruit” and “a week comprises 7 days”) are pruned. To improve the diversity, the problems are clustered by textual similarity.\nFor each cluster, at most 5 problems are retained. Finally, the dataset contains 562 single-step word problems with only one operator, including addition, subtraction, multiplication, and division.\n3) CC[30]. The dataset is designed for mult-step math prob-lems. It contains 600 multi-step problems without irrelevant quantities, harvested from commoncoresheets.com. The dataset involves various combinations of four basic operators, including(a) addition followed by subtraction;(b) subtraction followed by addition;(c) addition and multiplication;(d) addition and division;(e) subtraction and multiplication; and (f) subtraction and division. It is worth noting that this dataset does not incorporate irrelevant quantities in the problem text.\nHence, there is no need to apply the quantity relevance classifier for the algorithms containing this component.\nTABLE 1 Statistics of arithmetic word problem datasets.\nDataset # problems # single-op # multi-op operators O MA1 134 112 22 {+,−} IXL 140 119 21 {+,−} MA2 121 96 25 {+,−} AI2 395 327 68 {+,−} IL 562 562 0 {+,−,×,÷} CC 600 0 600 {+,−,×,÷} SingleEQ 508 390 118 {+,−,×,÷} AllArith 831 634 197 {+,−,×,÷} MAWPS-S 2,373 1,311 1,062 {+,−,×,÷} Dolphin-S 7,070 115 6,955 {+,−,×,÷} Math23K 23,162 3,131 20,031 {+,−,×,÷}\n4) SingleEQ[31]. The dataset contains both single-step and multi-step arithmetic problems and is a mixture of prob-lems from a number of sources, including math-aids.com, k5learning.com, ixl.com and a subset of the data from AI2. Each problem involves operators of multiplication, division, subtraction, and addition over non-negative rational numbers.\n5) AllArith[32]. The dataset is a mixture of the data from AI2, IL, CC and SingleEQ. All mentions of quantities are normalized into digit representation. To capture how well the automatic solvers can distinguish between different problem types, near-duplicate problems(with over 80% match of unigrams and bigrams) are removed. Finally, there remain 831 math problems.\n6) MAWPS[49] is another testbed for arithmetic word prob-lems with one unknown variable in the question. Its objective is to compile a dataset of varying complexity from different websites. Operationally, it combines the published word problem datasets used in[25],[50],[31],[30]. There are 2,373 questions in the harvested dataset.\n7) Dolphin-S. This is a subset of Dolphin18K[12] which originally contains 18,460 problems and 5,871 templates with one or multiple equations. The problems whose template is associated with only one problem are extracted as the dataset of Dolphin-S. It contains 115 problems with single operator and 6,955 problems with multiple operators.\n8) Math23K[14]. The dataset contains Chinese math word problems for elementary school students and is crawled from multiple online education websites. Initially, 60,000 problems with only one unknown variable are collected. The equation templates are extracted in a rule-based manner. To ensure high precision, a large number of problems that do not fit the rules are discarded. Finally, 23,161 math problems with 2,187 templates remain.\n2.5.2 Performance Analysis\nGiven the aforementioned datasets, we merge the experimental results reported from previous works into one table. Such a unified organization can facilitate readers in identifying the methods with superior performance in each dataset. As shown in Table 2, the rows refer to the corpus of datasets and the columns are the statistic-based and tree-based methods. The cells are filled with the accuracies of these algorithms when solving math word problems in different datasets. We conduct additional experiments to cover all the cells by the tree-based solutions. These new experiment results are highlighted in blue color. Those with missing value\n6\nTABLE 2 Accuracy of statistic-based and tree-based methods in solving arithmetic problems.\nAI2 IL CC SingleEQ AllArith Dolphin-S MAWPS-S Math23K # problems 395 562 600 508 831 7,070 2,373 23,162\noperators O {+,−} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷}\nStatistic-based\nARIS[25] 2014 77.7 --48 ----Schema[26] 2015 88.64 -------Formula[27] 2016 86.07 -------LogicForm[28],[29] 2016 84.8 80.1 53.5 -----\nTree-based ALGES[31] 2015 52.4 72.9 65 72 60.4 ---ExpressionTree[30] 2015 72 73.9 45.2 66.38 79.4 26.11 --UNITDEP[32] 2017 56.2 71.0 53.5 72.25 81.7 28.78 --\nDL-based\nMathDQN[17] 2018 78.5 73.3 75.5 52.96 72.68 30.06 60.25 -Seq2SeqET[15] 2018 -------66.7 T-RNN[19] 2019 -----39.1 66.8 66.9 StackDecoder[16] 2019 -------65.8\nare indicated by “-” and it means that there was no experiment conducted for the algorithm in the particular dataset. The main reason is that they require particular efforts on logic templates and annotations, which are very non-trivial and cumbersome for experiment reproduction. There is no algorithm comparison for the dataset Math23K because the problem text is in Chinese and the feature extraction technologies proposed in the statistic-based and tree-based approaches are not applicable. From the results in Table 2, we derive the following observations worth noting and provide reasonings to explain the results.\nFirst, the statistic-based methods with advanced logic rep-resentation, such as Schema[26], Formula[27] and Logic-Form[28],[29], achieve dominating performance in the AI2 dataset. Their superiority is primarily owned to the additional efforts on annotating the text problem with more advanced logic representation. These annotations allow them to conduct fine-grained reasoning. In contrast, ARIS[25] does not work as good because it focuses on “change” schema of quantities and does not fully exploit other schemas like “compare”[26]. Since there are only hundreds of math problems in the datasets, it is feasible to make an exhaustive scan on the math problems and manually define the templates to fit these datasets. For instance, all quantities and the main-goal are first identified by rules in LogicForm[28],[29] and explicitly associated with their role-tags. Thus, with sufficient human intervention, the accuracy of statistic-baesd methods in AI2 can boost to 88.64%, much higher than that of tree-based methods. Nevertheless, these statistic-based methods are considered as brittle and rigid[12] and not scalable to handle large and diversified datasets, primarily due to the heavy annotation cost to train an accurate mapping between the text and the logic representation.\nSecond, the results of tree-based methods in AI2, IL and CC are collected from[17] where the same experimental setting of 3-fold cross validation is applied. It is interesting to observe that ALGES[31], ExpressionTree[30] and UNITDEP[32] cannot perform equally well on the three datasets.\nALGES works poorly in AI2 because irrelevant quantities exist in its math problems and ALGES is not trained with a classifier to get rid of them.\nHowever, it outperforms ExpressionTree and UNITDEP by a wide margin in the CC dataset because CC does not involve irrelevant quantities. In addition, this dataset only contains multi-step math problems. ALGES exploits the whole search space to enumerate all the possible trees, whereas ExpressionTree and ALGES use beam search for efficiency concern. UNITDEP does not work\nwell in AI2 because this dataset only involves operators+ and − and the unit dependency graph does not take effect. Moreover, its proposed context feature poses a negative impact on the AI2 dataset. After removing this feature from the input vector fed to the classifier, the accuracy in AI2 increases from 56.2% to 74.7%, but the result in CC drops from the current accuracy of 53.5% to 47.3%. Such an observation implies the limitation of hand-crafted features in UNITDEP. Among the three datasets AI2, IL and CC, MathDQN[17] achieves leading or comparable performance. In the CC dataset, which contains only multi-step problems and is considered as the most challenging one, MathDQN yields remarkable improvement and boosts the accuracy from 65% (derived by ALGES) to 75.5%. This is because MathDQN models the tree construction as Markov Decision Process and leverage the strengths of deep Q-network (DQN). By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Consequently, the RL framework demonstrates higher generality and robustness than the other tree-based methods when handling complicated scenarios. In the IL dataset, its performance is not superior to ExpressionTree as IL only contains one-step math problems. There is no need for hierarchical tree construction and cannot expose the strength of Markov Decision Process in MathDQN or the exhaustive enumeration strategy in ALGES.\nAs to the datasets of SingleEQ and AllArith, UNITDEP is a winner in both datasets, owning to the effectiveness of the proposed unit dependency graph(UDG). In the math problems with operators{×,÷}, the unit and rate are important clues to determine the correct quantities and operators in the math expression. The UDG poses constraints on unit compatibility to filter the false candidates in the expression tree construction. It can alleviate the brittleness of the unit extraction system, even though it requires additional annotation overhead in order to induce UDGs.\nLast but not the least, these experiments were conducted on small-scale datasets and their performances on larger and more diversified datasets remain unclear. Recently, Huang et al. have noticed the gap and released Dolphin18K\nThe Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers\nDongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai and Heng Tao Shen\nAbstract—Solving mathematical word problems(MWPs) automatically is challenging, primarily due to the semantic gap between human-readable words and machine-understandable logics. Despite the long history dated back to the 1960s, MWPs have regained intensive attention in the past few years with the advancement of Artificial Intelligence(AI). Solving MWPs successfully is considered as a milestone towards general AI. Many systems have claimed promising results in self-crafted and small-scale datasets. However, when applied on large and diverse datasets, none of the proposed methods in the literature achieves high precision, revealing that current MWP solvers still have much room for improvement. This motivated us to present a comprehensive survey to deliver a clear and complete picture of automatic math problem solvers. In this survey, we emphasize on algebraic word problems, summarize their extracted features and proposed techniques to bridge the semantic gap, and compare their performance in the publicly accessible datasets. We also cover automatic solvers for other types of math problems such as geometric problems that require the understanding of diagrams. Finally, we identify several emerging research directions for the readers with interests in MWPs.\nIndex Terms—math word problem, semantic parser, reasoning, survey, natural language processing, machine learning\n✦\n1 INTRODUCTION\nDesigning an automatic solver for mathematical word problems (MWPs) has a long history dated back to the 1960s[1],[2], [3], and continues to attract intensive research attention. In the past three years, more than 40 publications on this topic have emerged in the premier venues of artificial intelligence. The problem is particularly challenging because there remains a wide semantic gap to parse the human-readable words into machine-understandable logics so as to facilitate quantitative reasoning.\nHence, MWPs solvers are broadly considered as good test beds to evaluate the intelligence level of agents in terms of natural language understanding[4],[5] and the successful solving of MWPs would constitute a milestone towards general AI.\nWe categorize the evolution of MWP solvers into three major stages according to the technologies behind them, as depicted in Figure 1. In the first pioneering stage, roughly from the year 1960 to 2010, systems such as STUDENT[1], DEDUCOM[6], WORDPRO[7] and ROBUST[8], manually craft rules and schemas for pattern matchings. Thereupon, these solvers heavily rely on human interventions and can only resolve a limited number of scenarios that are defined in advance. Those early efforts for automatic understanding of natural language mathematical problems have been thoroughly reviewed in[9]. We exclude them from the scope of this survey paper and focus on the recent technology developments that have not been covered in the previous survey[9].\nIn the second stage, MWP solvers made use of semantic parsing[10],[11], with the objective of mapping the sentences\n• D. Zhang and L. Zhang are with the College of Computer Science and Technology, Zhejiang University, China. Emails:{zhangdongxi-ang37,zglumg}@gmail.com;\n• L. Wang and H. T. Shen are with the Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China. Emails: demolei@outlook.com; shenhengtao@hotmail.com\n• B. T. Dai is with School of Information Systems, Singapore Management University. Email:btdai@smu.edu.sg\n• Corresponding Author: Heng Tao Shen.\n1960-2010 2017-2019 2011-2017\n| |Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| ---|---|---|\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing|Deep Learning\tReinforcement Learning|\nFig. 1. Technology evolving trend in solving MWPs.\nfrom problem statements into structured logic representations so as to facilitate quantitative reasoning. It has regained considerable interests from the academic community, and a booming number of methods have been proposed in the past years. These methods leveraged various strategies of feature engineering and statistical learning for performance boosting. The authors of these methods also claimed promising results in their public or manually harvested datasets. In this paper, one of our tasks is to present a comprehensive review on the proposed methods in this stage. The methods will be initially organized according to the sub-tasks of MWPs which they were designed to solve, such as arithmetic word problem(in Section 2), equation set word problem(in Section 3) and geometric word problem(in Section 5). We then examine the proposed techniques in each sub-task with a clear technical organization and accountable experimental evaluations.\nMWP solvers in the third stage were originated from an empirical work[12]. Its experimental results on a large-scale and diversified dataset showed that the status of MWP solvers was not as optimistic as they claimed to be. In fact, the accuracies of many approaches dropped sharply and there is a great room for improvement in this research area. To design more accurate and robust solutions, the subsequent publications are forked into two directions. One is to continue refining the technology of semantic parsing. For instance, Huang et al.\n2\nproposed a new type of semantic representation to conduct fine-grained inference[13]. The other direction attempts to exploit the advantages of deep learning models, with the availability of large-scale training datasets. This is an emerging research direction for MWP solvers and we observed multiple instances, including Deep Neural Solver[14], Seq2SeqET[15], StackDecoder[16], MathDQN[17], CASS[18], T-RNN[19]. These models represent a new technology trend in the research topic of MWP solvers and will be paid special attention in the survey.\nTo sum up, we present a comprehensive survey to review the MWP solvers proposed in recent years. Researchers in the community can benefit from this survey in the following ways:\n1) We provide a wide coverage on the math word problems, including arithmetic word problem, equation set problem, geometry word problem and miscellaneous sub-tasks related to automatic math solvers. The practitioners can easily identify all relevant approaches for performance evaluations.\nWe observed that the unawareness of relevant competitors occurs occasionally in the past literature. We are positive that the availability of our survey can help avoid such unawareness.\n2) The solvers designed for arithmetic word problems(AWP) with only one unknown variable and equation set problems (ESP) with multiple unknown variables are often not differ-entiated by previous works. In fact, the methods proposed for ESP are more general and can be used to solve AWP. In this survey, we clearly identify the difference and organize them in separate sections.\n3) Feature engineering plays a vital role to bridge the gap of semantic parsing. Almost all MWP solvers state their strategies of crafting effective features, resulting in a very diversified group of features, and there is a lack of clear organization among these features. In this survey, we will be the first to summarize all these proposed features in Table 5.\n4) As for the fairness on performance evaluations, ideally, there should be a benchmark dataset well accepted and widely adopted by the MWP research community, just like Ima-geNet[20] for visual object recognition and VQA[21],[22] for visual question answering. Unfortunately, we observed that many approaches tend to compile their own datasets to verify their superiorities, resulting in missing of relevant competitors as mentioned above. Tables 2 and 3 integrate the results of the existing methods on all public datasets.\nAfter collecting the accuracies that have been reported in the past literature, we observed many empty cells in the table. Each empty cell refers to a missing experiment on a particular algorithm and dataset. In this survey, we make our best efforts to fill the missing results by conducting a number of additional experiments, that allow us to provide a more comprehensive comparison and explicit analysis.\nThe remainder of the paper is organized as follows. We first review the arithmetic word problem solvers in Section 2, followed by equation set problem solvers in Section 3. Since feature engineering deserves special attention, we summarize the extracted features as well as the associated pre-processing techniques in Section 4. The geometric word problem solvers are reviewed in Section 5. We also cover miscellaneous automatic solvers related to math problems in Section 6. We conclude the paper and point out several future directions in MWPs that are worth examination in the final section.\n2 ARITHMETIC WORD PROBLEM SOLVER\nThe arithmetic word problems are targeted for elementary school students. The input is the text description for the math problem, represented in the form of a sequence of k\nwords 〈w0,w1,...,wk〉. There are n quantities q1,q2,...,qn\nmentioned in the text and an unknown variable x whose value is to be resolved. Our goal is to extract the relevant quantities and map this problem into an arithmetic expression E whose evaluation value provides the solution to the problem. There are only four types of fundamental operators O={+,−,×,÷} involved in the expression E.\nAn example of arithmetic word problem is illustrated in Figure 2. The relevant quantities to be extracted from the text include 17, 7 and 80. The number of hours spent on the bike is the unknown variable x. To solve the problem, we need to identify the correct operators between the quantities and their operation order such that we can obtain the final equation 17+ 7x= 80 or expression x=(80− 17)÷ 7 and return 9 as the solution to this problem.\nFig. 2. An example of arithmetic word problem.\nIn this section, we consider feature extraction as a black box and focus on the high-level algorithms and models. The details of feature extraction will be comprehensively present in Section 4. We classify existing algebra word problem solvers into three categories:\n| rule-based,|statistic-based,|tree-based and|\n| ---|---|---|\ndeep learning(DL)-based methods.\n2.1 Rule-based Methods\nThe early approaches to math word problems are rule-based systems based on hand engineering. Published in 1985, WORD-PRO[7] solves one-step arithmetic problems. It predefines four types of schemas, including change-in, change-out, combine and compare. The problem text is transformed into a set of propositions and the answer is derived with simple reasoning based on the propositions. Another system ROBUST, developed by Bakman[8], could understand free-format multi-step arith-metic word problems. It further expands the change schema of WORDPRO[7] into six distinct categories. The problem text is split into sentences and each sentence is mapped to a proposition.\nYun et al. also proposed to use schema for multi-step math problem solving[23]. However, the implementation details are not explicitly revealed. Since these systems have been out of date, we only provide such a brief overview to cover the representative ones. Readers can refer to[9] for a comprehensive survey of\n3\nearly rule-driven systems for automatic understanding of natural language math problems.\n2.2 Statistic-based Methods\nThe statistic-based methods leverage traditional machine learning models to identify the entities, quantities and operators from the problem text and yield the numeric answer with simple logic inference procedure. The scheme of quantity entailment proposed in[24] can be used to solve arithmetic problems with only one operator. It involves three types of classifiers to detect different properties of the word problem. The quantity pair classifier is trained to determine which pair of quantities would be used to derive the answer. The operator classifier picks the operator op ∈{+,−,×,÷} with the highest probability. The order classifier is relevant only for problems involving subtraction or division because the order of operands matters for these two types of operators. With the inferred expression, it is straightforward to calculate the numeric answer for the simple math problem.\nTo solve math problems with multi-step arithmetic expression, the statistic-based methods require more advanced logic templates.\nThis usually incurs additional overhead to annotate the text problems and associate them with the introduced template. As an early attempt, ARIS[25] defines a logic template named state that consists of a set of entities, their containers, attributes, quantities and relations. For example, “Liz has 9 black kittens” initializes the number of kitten(referring to an entity) with black color(referring to an attribute) and belonging to Liz (referring to a container). The solution splits the problem text into fragments and tracks the update of the states by verb categorization. More specifically, the verbs are classified into seven categories: observation, positive, negative, positive transfer, negative transfer, construct and destroy. To train such a classifier, we need to annotate each split fragment in the training dataset with the associated verb category. Another drawback of ARIS is that it only supports addition and subtraction.[26] follows a similar processing logic to ARIS. It predefines a corpus of logic representation named schema, inspired by[8]. The sentences in the text problem are examined sequentially until the sentence matches a schema, triggering an update operation to modify the number associated with the entities.\nMitra et al. proposed a new logic template named formula[27].\nThree types of formulas are defined, including part whole, change and comparison, to solve problems with addition and subtraction operators. For example, the text problem “Dan grew 42 turnips and 38 cantelopes. Jessica grew 47 turnips. How many turnips did they grow in total?” is annotated with the part-whole template: 〈whole: x,parts:{42,47}〉. To solve a math problem, the first step connects the assertions to the formulas. In the second step, the most probable formula is identified using the log-linear model with learned parameters and converted into an algebraic equation.\nAnother type of annotation is introduced in[28],[29] to facilitate solving a math problem. A group of logic forms are predefined and the problem text is converted into the logic form representation by certain mapping rules. For instance, the sentence “Fred picks 36 limes” will be transformed into verb(v1,pick) & nsubj(v1,Fred)& dobj(v1,n1)& head(n1,lime)& nummod(n1,36). Finally, logic inference is performed on the derived logic statements to obtain the answer.\nTo sum up, these statistical-based methods have two draw-backs that limit their usability. First, it requires additional an-notation overhead that prevents them from handling large-scale\ndatasets. Second, these methods are essentially based on a set of pre-defined templates, which are brittle and rigid. It will take great efforts to extend the templates to support other operators like multiplication and division. It is also not robust to diversified datasets. In the following, we will introduce the tree-based solutions, which are widely adopted and become the main-streaming solutions to arithmetic word problems.\n2.3 Tree-Based Methods\nThe arithmetic expression can be naturally represented as a binary tree structure such that the operators with higher priority are placed in the lower level and the root of the tree contains the operator with the lowest priority. The idea of tree-based approaches[30],[31],[32],[17] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner. One of the advantages is that there is no need for additional annotations such as equation template, tags or logic forms. Figure 3 shows two tree examples derived from the math word problem in Figure 2. One is called expression tree that is used in[30],[32],[17] and the other is called equation tree[31]. These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x.\n÷\n-7\n80 17\n=\n+ 80\n17\n7\n×\nx\nExpression Tree Equation Tree\nFig. 3. Examples of expression tree and equation tree for Figure 2.\nThe overall algorithmic framework among the tree-based approaches consists of two processing stages. In the first stage, the quantities are extracted from the text and form the bottom level of the tree. The candidate trees that are syntactically valid, but with different structures and internal nodes, are enumerated.\nIn the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution. A common strategy among these algorithms is to build a local classifier to determine the likelihood of an operator being selected as the internal node. Such local likelihood is taken into account in the global scoring function to determine the likelihood of the entire tree.\nRoy et al.[30] proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems. Its first strategy to reduce the search space is training a binary classifier to determine whether an extracted quantity is relevant or not. Only the relevant ones are used for tree construction and placed in the bottom level. The irrelevant quantities are discarded. The tree construction procedure is mapped to a collection of simple prediction problems, each determining the lowest common ancestor operation between a pair of quantities mentioned in the problem. The global scoring function for an enumerated tree takes into account two terms. The\n4\nfirst one, denoted by φ(q), is the likelihood of quantity q being irrelevant, i.e., q is not used in creating the expression tree. In the ideal case, all the irrelevant quantities are correctly predicted with high confidence, resulting in a large value for the sum of φ(q).\nThe other term, denoted by φ(op), is the likelihood of selecting op as the operator for an internal tree node. With these two factors, Score(E) is formally defined as\nScore(E)= w1\n∑\nq∈I(E)\nφ(q)+ ∑\nop∈N\nφ(op) (1)\nwhere I(E) is the group of irrelevant quantities that are not included in expression E, and N refers to the set of internal tree nodes.\nTo further reduce the tree enumeration space, beam search is applied in[30]. To generate the next state T ′ from the current partial tree, the algorithm avoids choosing all the possible pairs of terms and determining their operator. Instead, only top-k candidates with the highest partial scores are retained.\nExperimental results with k= 200 show that the strategy achieves a good balance between accuracy and running time. The service is also published as a web tool[33] and it can respond promptly to a math word problem.\nALGES[31] differs from[30] in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees. More specifically, ALGES does not discard irrevalent quantities, but enumerates all the syntactically valid trees.\nInteger Linear Programming(ILP) is applied as it can help enforce the constraints such as syntactic validity, type consistence and domain specific simplicity considerations. Consequently, its computation cost is dozens of times higher than that in[30], according to an efficiency evaluation in[17]. Second, its scoring function is different from Equation 1. There is no need for the term φ(q) because ALGES does not build a classifier to check the quantity relevance. Besides the monotonic aggregation of the likelihood from local operator classifiers, the scoring function incorporates a new termφ(P)= θT fp to assign a coherence score for the tree instance. Here, fP is the global feature extracted from a problem text P, and θ refers to the parameter vector.\nThe goal of[34] is also to build an equation tree by parsing the problem text. It makes two assumptions that can simplify the tree construction, but also limit its applicability. First, the final output equation form is restricted to have at most two variables. Second, each quantity mentioned in the sentence can be used at most once in the final equation. The tree construction procedure consists of a pipeline of predictors that identify irrelevant quantities, recognize grounded variables, and generate the final equation tree.\nWith customized feature selection and SVM based classifier, the relevant quantities and variables are extracted and used as the leaf nodes of the equation tree. The tree is built in a bottom-up manner.\nIt is worth noting that to reduce the search space and simplify the tree construction, only adjacent nodes are combined to generate their parent node.\nUnitDep[32] can be viewed as an extension work of[30] by the same authors. An important concept, named Unit Dependency Graph(UDG), is proposed to enhance the scoring function. The vertices in UDG consist of the extracted quantities. If the quantity correspond to a rate(e.g., 8 dollars per hour), the vertex is marked as RATE. There are six types of edge relations to be considered, such as whether two quantities are associated with the same unit.\nBuilding the UDG requires additional annotation overhead as we need to train two classifiers for the nodes and edges. The node classifier determines whether a node is associated with a rate. The\nedge classifier predicts the type of relationship between any pair of quantity nodes. Given a valid unit dependency graph G generated by the classifiers, its likelihood is defined as\nφ(G)= ∑\nv∈G∧LABEL(v)=rate\nP(v)+ λ∑\ne∈G\nP(e) (2)\nIn other words, we sum up the prediction probability for the RATE nodes and all the edges. The new scoring function for an expression tree extends Equation 1 by incorporatingφ(G). Rules are defined to enforce the rate consistence between an expression tree T and a candidate graph G. For example, if vi is the only node in the tree that is labeled RATE and it appears in the question, there should not exist a path from the leaf node to the root which only contains operators of addition and subtraction. Finally, the candidate graph G with the highest likelihood and rate-consistent with T is used to calculate the total score of T.\n2.4 DL-based Methods\nIn recent years, deep learning(DL) has witnessed great success in a wide spectrum of “smart” applications, such as visual question answering[35],[36], video captioning[37],[38],[39], [40], personal interest inference[41],[42],[43] and smart transportation[44]. The main advantage is that with sufficient amount of training data, DL is able to learn an effective feature representation in a data-driven manner without human intervention. It is not surprising to notice that several efforts have been attempted to apply DL for math word problem solving.\nDeep Neural Solver(DNS)[14] is a pioneering work designed for equation set problems, which will be introduced in more details in the next section. Following DNS, there have emerged multiple DL-based solvers for arithmetic word problems.\nSeq2SeqET[15] extended the idea of DNS by using expression tree as the output sequence. In other words, it applied seq2seq model to convert the problem text into an expression tree, which can be viewed as a template. Given the output of an expression tree or template, we can easily infer the numeric answer. To reduce the template space, equation normalization was proposed in Seq2SeqET so that duplicate representation of expression trees can be unified.\nStackDecoder[16] is also based on seq2seq model. Its encoder extracts semantic meanings of quantities in the question text and the decoder is equipped with a stack to facilitate tracking the semantic meanings of operands.\nT-RNN[19] can be viewed as an improvement of Seq2SeqET, in terms of quantity encoding, template representation and tree construction. First, an effective embedding network with Bi-LSTM and self attention is used to vectorize the quantities. Second, the detailed operators in the templates are encapsulated to further reduce the number of template space. For example, n1+n2, n1−n2, n1×n2, and n1÷n2 are mapped to the same template n1〈op〉n2. Third, a recursive neural network is applied to infer the unknown variables in the expression tree in a recursive manner.\nWang et al. made the first attempt of applying deep re-inforcement learning to solve arithmetic word problems[17].\nThe motivation is that deep Q-network has witnessed success in solving various problems with big search space such as playing text-based games[45], information extraction[46], text generation[47] and object detection in images[48]. To fit the math problem scenario, they formulate the expression tree construction as a Markov Decision Process and propose the MathDQN that is customized from the general deep reinforcement\n5\nlearning framework. Technically, they tailor the definitions of states, actions, and reward functions which are key components in the reinforcement learning framework. By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Compared to the aforementioned approaches, MathDQN iteratively picks the best operator for two selected quantities. This procedure can be viewed as beam search with k= 1 when exploiting candidate expression trees. Its deep Q-network acts as the operator classifier and guides the model to select the most promising operator for tree construction.\n2.5 Dataset Repository and Performance Analysis\nThe accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites. These datasets are small-scale and contain hundreds of math problems. In this subsection, we make a summary on the datasets that have been used in the aforementioned papers.\nMoreover, we organize the performance results on these datasets into one unified table. We also make our best efforts to conduct additional experiments. The new results are highlighted in blue color. In this way, readers can easily identify the best performers in each dataset.\n2.5.1 Datasets\nThere have been a number of datasets collected for the arithmetic word problems. We present their descriptions in the following and summarize the statistics of the datasets in Table 1.\n1) AI2[25]. There are 395 single-step or multi-step arithmetic word problems for the third, fourth, and fifth graders. It involves problems that can be solved with only addition and subtraction. The dataset is harvested from two websites: math-aids.com and ixl.com and comprises three subsets: MA1(from math-aids.com), IXL(from ixl.com) and MA2(from math-aids.com). Among them, IXL and MA2 are more challenging than MA1 because IXL contains more information gaps and MA2 includes more irrelevant information in its math problems.\n2) IL[30]. The problems are collected from websites k5learning.com and dadsworksheets.com. The problems that require background knowledge(e.g., “apple is fruit” and “a week comprises 7 days”) are pruned. To improve the diversity, the problems are clustered by textual similarity.\nFor each cluster, at most 5 problems are retained. Finally, the dataset contains 562 single-step word problems with only one operator, including addition, subtraction, multiplication, and division.\n3) CC[30]. The dataset is designed for mult-step math prob-lems. It contains 600 multi-step problems without irrelevant quantities, harvested from commoncoresheets.com. The dataset involves various combinations of four basic operators, including(a) addition followed by subtraction;(b) subtraction followed by addition;(c) addition and multiplication;(d) addition and division;(e) subtraction and multiplication; and (f) subtraction and division. It is worth noting that this dataset does not incorporate irrelevant quantities in the problem text.\nHence, there is no need to apply the quantity relevance classifier for the algorithms containing this component.\nTABLE 1 Statistics of arithmetic word problem datasets.\nDataset # problems # single-op # multi-op operators O MA1 134 112 22 {+,−} IXL 140 119 21 {+,−} MA2 121 96 25 {+,−} AI2 395 327 68 {+,−} IL 562 562 0 {+,−,×,÷} CC 600 0 600 {+,−,×,÷} SingleEQ 508 390 118 {+,−,×,÷} AllArith 831 634 197 {+,−,×,÷} MAWPS-S 2,373 1,311 1,062 {+,−,×,÷} Dolphin-S 7,070 115 6,955 {+,−,×,÷} Math23K 23,162 3,131 20,031 {+,−,×,÷}\n4) SingleEQ[31]. The dataset contains both single-step and multi-step arithmetic problems and is a mixture of prob-lems from a number of sources, including math-aids.com, k5learning.com, ixl.com and a subset of the data from AI2. Each problem involves operators of multiplication, division, subtraction, and addition over non-negative rational numbers.\n5) AllArith[32]. The dataset is a mixture of the data from AI2, IL, CC and SingleEQ. All mentions of quantities are normalized into digit representation. To capture how well the automatic solvers can distinguish between different problem types, near-duplicate problems(with over 80% match of unigrams and bigrams) are removed. Finally, there remain 831 math problems.\n6) MAWPS[49] is another testbed for arithmetic word prob-lems with one unknown variable in the question. Its objective is to compile a dataset of varying complexity from different websites. Operationally, it combines the published word problem datasets used in[25],[50],[31],[30]. There are 2,373 questions in the harvested dataset.\n7) Dolphin-S. This is a subset of Dolphin18K[12] which originally contains 18,460 problems and 5,871 templates with one or multiple equations. The problems whose template is associated with only one problem are extracted as the dataset of Dolphin-S. It contains 115 problems with single operator and 6,955 problems with multiple operators.\n8) Math23K[14]. The dataset contains Chinese math word problems for elementary school students and is crawled from multiple online education websites. Initially, 60,000 problems with only one unknown variable are collected. The equation templates are extracted in a rule-based manner. To ensure high precision, a large number of problems that do not fit the rules are discarded. Finally, 23,161 math problems with 2,187 templates remain.\n2.5.2 Performance Analysis\nGiven the aforementioned datasets, we merge the experimental results reported from previous works into one table. Such a unified organization can facilitate readers in identifying the methods with superior performance in each dataset. As shown in Table 2, the rows refer to the corpus of datasets and the columns are the statistic-based and tree-based methods. The cells are filled with the accuracies of these algorithms when solving math word problems in different datasets. We conduct additional experiments to cover all the cells by the tree-based solutions. These new experiment results are highlighted in blue color. Those with missing value\n6\nTABLE 2 Accuracy of statistic-based and tree-based methods in solving arithmetic problems.\nAI2 IL CC SingleEQ AllArith Dolphin-S MAWPS-S Math23K # problems 395 562 600 508 831 7,070 2,373 23,162\noperators O {+,−} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷}\nStatistic-based\nARIS[25] 2014 77.7 --48 ----Schema[26] 2015 88.64 -------Formula[27] 2016 86.07 -------LogicForm[28],[29] 2016 84.8 80.1 53.5 -----\nTree-based ALGES[31] 2015 52.4 72.9 65 72 60.4 ---ExpressionTree[30] 2015 72 73.9 45.2 66.38 79.4 26.11 --UNITDEP[32] 2017 56.2 71.0 53.5 72.25 81.7 28.78 --\nDL-based\nMathDQN[17] 2018 78.5 73.3 75.5 52.96 72.68 30.06 60.25 -Seq2SeqET[15] 2018 -------66.7 T-RNN[19] 2019 -----39.1 66.8 66.9 StackDecoder[16] 2019 -------65.8\nare indicated by “-” and it means that there was no experiment conducted for the algorithm in the particular dataset. The main reason is that they require particular efforts on logic templates and annotations, which are very non-trivial and cumbersome for experiment reproduction. There is no algorithm comparison for the dataset Math23K because the problem text is in Chinese and the feature extraction technologies proposed in the statistic-based and tree-based approaches are not applicable. From the results in Table 2, we derive the following observations worth noting and provide reasonings to explain the results.\nFirst, the statistic-based methods with advanced logic rep-resentation, such as Schema[26], Formula[27] and Logic-Form[28],[29], achieve dominating performance in the AI2 dataset. Their superiority is primarily owned to the additional efforts on annotating the text problem with more advanced logic representation. These annotations allow them to conduct fine-grained reasoning. In contrast, ARIS[25] does not work as good because it focuses on “change” schema of quantities and does not fully exploit other schemas like “compare”[26]. Since there are only hundreds of math problems in the datasets, it is feasible to make an exhaustive scan on the math problems and manually define the templates to fit these datasets. For instance, all quantities and the main-goal are first identified by rules in LogicForm[28],[29] and explicitly associated with their role-tags. Thus, with sufficient human intervention, the accuracy of statistic-baesd methods in AI2 can boost to 88.64%, much higher than that of tree-based methods. Nevertheless, these statistic-based methods are considered as brittle and rigid[12] and not scalable to handle large and diversified datasets, primarily due to the heavy annotation cost to train an accurate mapping between the text and the logic representation.\nSecond, the results of tree-based methods in AI2, IL and CC are collected from[17] where the same experimental setting of 3-fold cross validation is applied. It is interesting to observe that ALGES[31], ExpressionTree[30] and UNITDEP[32] cannot perform equally well on the three datasets.\nALGES works poorly in AI2 because irrelevant quantities exist in its math problems and ALGES is not trained with a classifier to get rid of them.\nHowever, it outperforms ExpressionTree and UNITDEP by a wide margin in the CC dataset because CC does not involve irrelevant quantities. In addition, this dataset only contains multi-step math problems. ALGES exploits the whole search space to enumerate all the possible trees, whereas ExpressionTree and ALGES use beam search for efficiency concern. UNITDEP does not work\nwell in AI2 because this dataset only involves operators+ and − and the unit dependency graph does not take effect. Moreover, its proposed context feature poses a negative impact on the AI2 dataset. After removing this feature from the input vector fed to the classifier, the accuracy in AI2 increases from 56.2% to 74.7%, but the result in CC drops from the current accuracy of 53.5% to 47.3%. Such an observation implies the limitation of hand-crafted features in UNITDEP. Among the three datasets AI2, IL and CC, MathDQN[17] achieves leading or comparable performance. In the CC dataset, which contains only multi-step problems and is considered as the most challenging one, MathDQN yields remarkable improvement and boosts the accuracy from 65% (derived by ALGES) to 75.5%. This is because MathDQN models the tree construction as Markov Decision Process and leverage the strengths of deep Q-network (DQN). By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Consequently, the RL framework demonstrates higher generality and robustness than the other tree-based methods when handling complicated scenarios. In the IL dataset, its performance is not superior to ExpressionTree as IL only contains one-step math problems. There is no need for hierarchical tree construction and cannot expose the strength of Markov Decision Process in MathDQN or the exhaustive enumeration strategy in ALGES.\nAs to the datasets of SingleEQ and AllArith, UNITDEP is a winner in both datasets, owning to the effectiveness of the proposed unit dependency graph(UDG). In the math problems with operators{×,÷}, the unit and rate are important clues to determine the correct quantities and operators in the math expression. The UDG poses constraints on unit compatibility to filter the false candidates in the expression tree construction. It can alleviate the brittleness of the unit extraction system, even though it requires additional annotation overhead in order to induce UDGs.\nLast but not the least, these experiments were conducted on small-scale datasets and their performances on larger and more diversified datasets remain unclear. Recently, Huang et al. have noticed the gap and released Dolphin18K[12] which contains 18,460 problems and 5,871 templates with one or multiple equations. The findings in[12] are astonishing. The accuracies of existing approaches for equation set problems, which will be introduced in the next section, degrade sharply to less than 25%.\nThese methods cannot even perform\n【文档内容】\nThis motivated us to present a comprehensive survey to deliver a clear and complete picture of automatic math problem solvers. In this survey, we emphasize on algebraic word problems, summarize their extracted features and proposed techniques to bridge the semantic gap, and compare their performance in the publicly accessible datasets. We also cover automatic solvers for other types of math problems such as geometric problems that require the understanding of diagrams. Finally, we identify several emerging research directions for the readers with interests in MWPs.\nIndex Terms—math word problem, semantic parser, reasoning, survey, natural language processing, machine learning\n✦\n1 INTRODUCTION\nDesigning an automatic solver for mathematical word problems (MWPs) has a long history dated back to the 1960s[1],[2], [3], and continues to attract intensive research attention. In the past three years, more than 40 publications on this topic have emerged in the premier venues of artificial intelligence. The problem is particularly challenging because there remains a wide semantic gap to parse the human-readable words into machine-understandable logics so as to facilitate quantitative reasoning.\nHence, MWPs solvers are broadly considered as good test beds to evaluate the intelligence level of agents in terms of natural language understanding[4],[5] and the successful solving of MWPs would constitute a milestone towards general AI.\nWe categorize the evolution of MWP solvers into three major stages according to the technologies behind them, as depicted in Figure 1. In the first pioneering stage, roughly from the year 1960 to 2010, systems such as STUDENT[1], DEDUCOM[6], WORDPRO[7] and ROBUST[8], manually craft rules and schemas for pattern matchings. Thereupon, these solvers heavily rely on human interventions and can only resolve a limited number of scenarios that are defined in advance. Those early efforts for automatic understanding of natural language mathematical problems have been thoroughly reviewed in[9]. We exclude them from the scope of this survey paper and focus on the recent technology developments that have not been covered in the previous survey[9].\nIn the second stage, MWP solvers made use of semantic parsing[10],[11], with the objective of mapping the sentences\n• D. Zhang and L. Zhang are with the College of Computer Science and Technology, Zhejiang University, China. Emails:{zhangdongxi-ang37,zglumg}@gmail.com;\n• L. Wang and H. T. Shen are with the Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China. Emails: demolei@outlook.com; shenhengtao@hotmail.com\n• B. T. Dai is with School of Information Systems, Singapore Management University. Email:btdai@smu.edu.sg\n• Corresponding Author: Heng Tao Shen.\n1960-2010 2017-2019 2011-2017\n| |Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| ---|---|---|\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing|Deep Learning\tReinforcement Learning|\nFig. 1. Technology evolving trend in solving MWPs.\nfrom problem statements into structured logic representations so as to facilitate quantitative reasoning. It has regained considerable interests from the academic community, and a booming number of methods have been proposed in the past years. These methods leveraged various strategies of feature engineering and statistical learning for performance boosting. The authors of these methods also claimed promising results in their public or manually harvested datasets. In this paper, one of our tasks is to present a comprehensive review on the proposed methods in this stage. The methods will be initially organized according to the sub-tasks of MWPs which they were designed to solve, such as arithmetic word problem(in Section 2), equation set word problem(in Section 3) and geometric word problem(in Section 5). We then examine the proposed techniques in each sub-task with a clear technical organization and accountable experimental evaluations.\nMWP solvers in the third stage were originated from an empirical work[12]. Its experimental results on a large-scale and diversified dataset showed that the status of MWP solvers was not as optimistic as they claimed to be. In fact, the accuracies of many approaches dropped sharply and there is a great room for improvement in this research area. To design more accurate and robust solutions, the subsequent publications are forked into two directions. One is to continue refining the technology of semantic parsing. For instance, Huang et al.\n2\nproposed a new type of semantic representation to conduct fine-grained inference[13]. The other direction attempts to exploit the advantages of deep learning models, with the availability of large-scale training datasets. This is an emerging research direction for MWP solvers and we observed multiple instances, including Deep Neural Solver[14], Seq2SeqET[15], StackDecoder[16], MathDQN[17], CASS[18], T-RNN[19]. These models represent a new technology trend in the research topic of MWP solvers and will be paid special attention in the survey.\nTo sum up, we present a comprehensive survey to review the MWP solvers proposed in recent years. Researchers in the community can benefit from this survey in the following ways:\n1) We provide a wide coverage on the math word problems, including arithmetic word problem, equation set problem, geometry word problem and miscellaneous sub-tasks related to automatic math solvers. The practitioners can easily identify all relevant approaches for performance evaluations.\nWe observed that the unawareness of relevant competitors occurs occasionally in the past literature. We are positive that the availability of our survey can help avoid such unawareness.\n2) The solvers designed for arithmetic word problems(AWP) with only one unknown variable and equation set problems (ESP) with multiple unknown variables are often not differ-entiated by previous works. In fact, the methods proposed for ESP are more general and can be used to solve AWP. In this survey, we clearly identify the difference and organize them in separate sections.\n3) Feature engineering plays a vital role to bridge the gap of semantic parsing. Almost all MWP solvers state their strategies of crafting effective features, resulting in a very diversified group of features, and there is a lack of clear organization among these features. In this survey, we will be the first to summarize all these proposed features in Table 5.\n4) As for the fairness on performance evaluations, ideally, there should be a benchmark dataset well accepted and widely adopted by the MWP research community, just like Ima-geNet[20] for visual object recognition and VQA[21],[22] for visual question answering. Unfortunately, we observed that many approaches tend to compile their own datasets to verify their superiorities, resulting in missing of relevant competitors as mentioned above. Tables 2 and 3 integrate the results of the existing methods on all public datasets.\nAfter collecting the accuracies that have been reported in the past literature, we observed many empty cells in the table. Each empty cell refers to a missing experiment on a particular algorithm and dataset. In this survey, we make our best efforts to fill the missing results by conducting a number of additional experiments, that allow us to provide a more comprehensive comparison and explicit analysis.\nThe remainder of the paper is organized as follows. We first review the arithmetic word problem solvers in Section 2, followed by equation set problem solvers in Section 3. Since feature engineering deserves special attention, we summarize the extracted features as well as the associated pre-processing techniques in Section 4. The geometric word problem solvers are reviewed in Section 5. We also cover miscellaneous automatic solvers related to math problems in Section 6. We conclude the paper and point out several future directions in MWPs that are worth examination in the final section.\n2 ARITHMETIC WORD PROBLEM SOLVER\nThe arithmetic word problems are targeted for elementary school students. The input is the text description for the math problem, represented in the form of a sequence of k\nwords 〈w0,w1,...,wk〉. There are n quantities q1,q2,...,qn\nmentioned in the text and an unknown variable x whose value is to be resolved. Our goal is to extract the relevant quantities and map this problem into an arithmetic expression E whose evaluation value provides the solution to the problem. There are only four types of fundamental operators O={+,−,×,÷} involved in the expression E.\nAn example of arithmetic word problem is illustrated in Figure 2. The relevant quantities to be extracted from the text include 17, 7 and 80. The number of hours spent on the bike is the unknown variable x. To solve the problem, we need to identify the correct operators between the quantities and their operation order such that we can obtain the final equation 17+ 7x= 80 or expression x=(80− 17)÷ 7 and return 9 as the solution to this problem.\nFig. 2. An example of arithmetic word problem.\nIn this section, we consider feature extraction as a black box and focus on the high-level algorithms and models. The details of feature extraction will be comprehensively present in Section 4. We classify existing algebra word problem solvers into three categories:\n| rule-based,|statistic-based,|tree-based and|\n| ---|---|---|\ndeep learning(DL)-based methods.\n2.1 Rule-based Methods\nThe early approaches to math word problems are rule-based systems based on hand engineering. Published in 1985, WORD-PRO[7] solves one-step arithmetic problems. It predefines four types of schemas, including change-in, change-out, combine and compare. The problem text is transformed into a set of propositions and the answer is derived with simple reasoning based on the propositions. Another system ROBUST, developed by Bakman[8], could understand free-format multi-step arith-metic word problems. It further expands the change schema of WORDPRO[7] into six distinct categories. The problem text is split into sentences and each sentence is mapped to a proposition.\nYun et al. also proposed to use schema for multi-step math problem solving[23]. However, the implementation details are not explicitly revealed. Since these systems have been out of date, we only provide such a brief overview to cover the representative ones. Readers can refer to[9] for a comprehensive survey of\n3\nearly rule-driven systems for automatic understanding of natural language math problems.\n2.2 Statistic-based Methods\nThe statistic-based methods leverage traditional machine learning models to identify the entities, quantities and operators from the problem text and yield the numeric answer with simple logic inference procedure. The scheme of quantity entailment proposed in[24] can be used to solve arithmetic problems with only one operator. It involves three types of classifiers to detect different properties of the word problem. The quantity pair classifier is trained to determine which pair of quantities would be used to derive the answer. The operator classifier picks the operator op ∈{+,−,×,÷} with the highest probability. The order classifier is relevant only for problems involving subtraction or division because the order of operands matters for these two types of operators. With the inferred expression, it is straightforward to calculate the numeric answer for the simple math problem.\nTo solve math problems with multi-step arithmetic expression, the statistic-based methods require more advanced logic templates.\nThis usually incurs additional overhead to annotate the text problems and associate them with the introduced template. As an early attempt, ARIS[25] defines a logic template named state that consists of a set of entities, their containers, attributes, quantities and relations. For example, “Liz has 9 black kittens” initializes the number of kitten(referring to an entity) with black color(referring to an attribute) and belonging to Liz (referring to a container). The solution splits the problem text into fragments and tracks the update of the states by verb categorization. More specifically, the verbs are classified into seven categories: observation, positive, negative, positive transfer, negative transfer, construct and destroy. To train such a classifier, we need to annotate each split fragment in the training dataset with the associated verb category. Another drawback of ARIS is that it only supports addition and subtraction.[26] follows a similar processing logic to ARIS. It predefines a corpus of logic representation named schema, inspired by[8]. The sentences in the text problem are examined sequentially until the sentence matches a schema, triggering an update operation to modify the number associated with the entities.\nMitra et al. proposed a new logic template named formula[27].\nThree types of formulas are defined, including part whole, change and comparison, to solve problems with addition and subtraction operators. For example, the text problem “Dan grew 42 turnips and 38 cantelopes. Jessica grew 47 turnips. How many turnips did they grow in total?” is annotated with the part-whole template: 〈whole: x,parts:{42,47}〉. To solve a math problem, the first step connects the assertions to the formulas. In the second step, the most probable formula is identified using the log-linear model with learned parameters and converted into an algebraic equation.\nAnother type of annotation is introduced in[28],[29] to facilitate solving a math problem. A group of logic forms are predefined and the problem text is converted into the logic form representation by certain mapping rules. For instance, the sentence “Fred picks 36 limes” will be transformed into verb(v1,pick) & nsubj(v1,Fred)& dobj(v1,n1)& head(n1,lime)& nummod(n1,36). Finally, logic inference is performed on the derived logic statements to obtain the answer.\nTo sum up, these statistical-based methods have two draw-backs that limit their usability. First, it requires additional an-notation overhead that prevents them from handling large-scale\ndatasets. Second, these methods are essentially based on a set of pre-defined templates, which are brittle and rigid. It will take great efforts to extend the templates to support other operators like multiplication and division. It is also not robust to diversified datasets. In the following, we will introduce the tree-based solutions, which are widely adopted and become the main-streaming solutions to arithmetic word problems.\n2.3 Tree-Based Methods\nThe arithmetic expression can be naturally represented as a binary tree structure such that the operators with higher priority are placed in the lower level and the root of the tree contains the operator with the lowest priority. The idea of tree-based approaches[30],[31],[32],[17] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner. One of the advantages is that there is no need for additional annotations such as equation template, tags or logic forms. Figure 3 shows two tree examples derived from the math word problem in Figure 2. One is called expression tree that is used in[30],[32],[17] and the other is called equation tree[31]. These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x.\n÷\n-7\n80 17\n=\n+ 80\n17\n7\n×\nx\nExpression Tree Equation Tree\nFig. 3. Examples of expression tree and equation tree for Figure 2.\nThe overall algorithmic framework among the tree-based approaches consists of two processing stages. In the first stage, the quantities are extracted from the text and form the bottom level of the tree. The candidate trees that are syntactically valid, but with different structures and internal nodes, are enumerated.\nIn the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution. A common strategy among these algorithms is to build a local classifier to determine the likelihood of an operator being selected as the internal node. Such local likelihood is taken into account in the global scoring function to determine the likelihood of the entire tree.\nRoy et al.[30] proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems. Its first strategy to reduce the search space is training a binary classifier to determine whether an extracted quantity is relevant or not. Only the relevant ones are used for tree construction and placed in the bottom level. The irrelevant quantities are discarded. The tree construction procedure is mapped to a collection of simple prediction problems, each determining the lowest common ancestor operation between a pair of quantities mentioned in the problem. The global scoring function for an enumerated tree takes into account two terms. The\n4\nfirst one, denoted by φ(q), is the likelihood of quantity q being irrelevant, i.e., q is not used in creating the expression tree. In the ideal case, all the irrelevant quantities are correctly predicted with high confidence, resulting in a large value for the sum of φ(q).\nThe other term, denoted by φ(op), is the likelihood of selecting op as the operator for an internal tree node. With these two factors, Score(E) is formally defined as\nScore(E)= w1\n∑\nq∈I(E)\nφ(q)+ ∑\nop∈N\nφ(op) (1)\nwhere I(E) is the group of irrelevant quantities that are not included in expression E, and N refers to the set of internal tree nodes.\nTo further reduce the tree enumeration space, beam search is applied in[30]. To generate the next state T ′ from the current partial tree, the algorithm avoids choosing all the possible pairs of terms and determining their operator. Instead, only top-k candidates with the highest partial scores are retained.\nExperimental results with k= 200 show that the strategy achieves a good balance between accuracy and running time. The service is also published as a web tool[33] and it can respond promptly to a math word problem.\nALGES[31] differs from[30] in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees. More specifically, ALGES does not discard irrevalent quantities, but enumerates all the syntactically valid trees.\nInteger Linear Programming(ILP) is applied as it can help enforce the constraints such as syntactic validity, type consistence and domain specific simplicity considerations. Consequently, its computation cost is dozens of times higher than that in[30], according to an efficiency evaluation in[17]. Second, its scoring function is different from Equation 1. There is no need for the term φ(q) because ALGES does not build a classifier to check the quantity relevance. Besides the monotonic aggregation of the likelihood from local operator classifiers, the scoring function incorporates a new termφ(P)= θT fp to assign a coherence score for the tree instance. Here, fP is the global feature extracted from a problem text P, and θ refers to the parameter vector.\nThe goal of[34] is also to build an equation tree by parsing the problem text. It makes two assumptions that can simplify the tree construction, but also limit its applicability. First, the final output equation form is restricted to have at most two variables. Second, each quantity mentioned in the sentence can be used at most once in the final equation. The tree construction procedure consists of a pipeline of predictors that identify irrelevant quantities, recognize grounded variables, and generate the final equation tree.\nWith customized feature selection and SVM based classifier, the relevant quantities and variables are extracted and used as the leaf nodes of the equation tree. The tree is built in a bottom-up manner.\nIt is worth noting that to reduce the search space and simplify the tree construction, only adjacent nodes are combined to generate their parent node.\nUnitDep[32] can be viewed as an extension work of[30] by the same authors. An important concept, named Unit Dependency Graph(UDG), is proposed to enhance the scoring function. The vertices in UDG consist of the extracted quantities. If the quantity correspond to a rate(e.g., 8 dollars per hour), the vertex is marked as RATE. There are six types of edge relations to be considered, such as whether two quantities are associated with the same unit.\nBuilding the UDG requires additional annotation overhead as we need to train two classifiers for the nodes and edges. The node classifier determines whether a node is associated with a rate. The\nedge classifier predicts the type of relationship between any pair of quantity nodes. Given a valid unit dependency graph G generated by the classifiers, its likelihood is defined as\nφ(G)= ∑\nv∈G∧LABEL(v)=rate\nP(v)+ λ∑\ne∈G\nP(e) (2)\nIn other words, we sum up the prediction probability for the RATE nodes and all the edges. The new scoring function for an expression tree extends Equation 1 by incorporatingφ(G). Rules are defined to enforce the rate consistence between an expression tree T and a candidate graph G. For example, if vi is the only node in the tree that is labeled RATE and it appears in the question, there should not exist a path from the leaf node to the root which only contains operators of addition and subtraction. Finally, the candidate graph G with the highest likelihood and rate-consistent with T is used to calculate the total score of T.\n2.4 DL-based Methods\nIn recent years, deep learning(DL) has witnessed great success in a wide spectrum of “smart” applications, such as visual question answering[35],[36], video captioning[37],[38],[39], [40], personal interest inference[41],[42],[43] and smart transportation[44]. The main advantage is that with sufficient amount of training data, DL is able to learn an effective feature representation in a data-driven manner without human intervention. It is not surprising to notice that several efforts have been attempted to apply DL for math word problem solving.\nDeep Neural Solver(DNS)[14] is a pioneering work designed for equation set problems, which will be introduced in more details in the next section. Following DNS, there have emerged multiple DL-based solvers for arithmetic word problems.\nSeq2SeqET[15] extended the idea of DNS by using expression tree as the output sequence. In other words, it applied seq2seq model to convert the problem text into an expression tree, which can be viewed as a template. Given the output of an expression tree or template, we can easily infer the numeric answer. To reduce the template space, equation normalization was proposed in Seq2SeqET so that duplicate representation of expression trees can be unified.\nStackDecoder[16] is also based on seq2seq model. Its encoder extracts semantic meanings of quantities in the question text and the decoder is equipped with a stack to facilitate tracking the semantic meanings of operands.\nT-RNN[19] can be viewed as an improvement of Seq2SeqET, in terms of quantity encoding, template representation and tree construction. First, an effective embedding network with Bi-LSTM and self attention is used to vectorize the quantities. Second, the detailed operators in the templates are encapsulated to further reduce the number of template space. For example, n1+n2, n1−n2, n1×n2, and n1÷n2 are mapped to the same template n1〈op〉n2. Third, a recursive neural network is applied to infer the unknown variables in the expression tree in a recursive manner.\nWang et al. made the first attempt of applying deep re-inforcement learning to solve arithmetic word problems[17].\nThe motivation is that deep Q-network has witnessed success in solving various problems with big search space such as playing text-based games[45], information extraction[46], text generation[47] and object detection in images[48]. To fit the math problem scenario, they formulate the expression tree construction as a Markov Decision Process and propose the MathDQN that is customized from the general deep reinforcement\n5\nlearning framework. Technically, they tailor the definitions of states, actions, and reward functions which are key components in the reinforcement learning framework. By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Compared to the aforementioned approaches, MathDQN iteratively picks the best operator for two selected quantities. This procedure can be viewed as beam search with k= 1 when exploiting candidate expression trees. Its deep Q-network acts as the operator classifier and guides the model to select the most promising operator for tree construction.\n2.5 Dataset Repository and Performance Analysis\nThe accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites. These datasets are small-scale and contain hundreds of math problems. In this subsection, we make a summary on the datasets that have been used in the aforementioned papers.\nMoreover, we organize the performance results on these datasets into one unified table. We also make our best efforts to conduct additional experiments. The new results are highlighted in blue color. In this way, readers can easily identify the best performers in each dataset.\n2.5.1 Datasets\nThere have been a number of datasets collected for the arithmetic word problems. We present their descriptions in the following and summarize the statistics of the datasets in Table 1.\n1) AI2[25]. There are 395 single-step or multi-step arithmetic word problems for the third, fourth, and fifth graders. It involves problems that can be solved with only addition and subtraction. The dataset is harvested from two websites: math-aids.com and ixl.com and comprises three subsets: MA1(from math-aids.com), IXL(from ixl.com) and MA2(from math-aids.com). Among them, IXL and MA2 are more challenging than MA1 because IXL contains more information gaps and MA2 includes more irrelevant information in its math problems.\n2) IL[30]. The problems are collected from websites k5learning.com and dadsworksheets.com. The problems that require background knowledge(e.g., “apple is fruit” and “a week comprises 7 days”) are pruned. To improve the diversity, the problems are clustered by textual similarity.\nFor each cluster, at most 5 problems are retained. Finally, the dataset contains 562 single-step word problems with only one operator, including addition, subtraction, multiplication, and division.\n3) CC[30]. The dataset is designed for mult-step math prob-lems. It contains 600 multi-step problems without irrelevant quantities, harvested from commoncoresheets.com. The dataset involves various combinations of four basic operators, including(a) addition followed by subtraction;(b) subtraction followed by addition;(c) addition and multiplication;(d) addition and division;(e) subtraction and multiplication; and (f) subtraction and division. It is worth noting that this dataset does not incorporate irrelevant quantities in the problem text.\nHence, there is no need to apply the quantity relevance classifier for the algorithms containing this component.\nTABLE 1 Statistics of arithmetic word problem datasets.\nDataset # problems # single-op # multi-op operators O MA1 134 112 22 {+,−} IXL 140 119 21 {+,−} MA2 121 96 25 {+,−} AI2 395 327 68 {+,−} IL 562 562 0 {+,−,×,÷} CC 600 0 600 {+,−,×,÷} SingleEQ 508 390 118 {+,−,×,÷} AllArith 831 634 197 {+,−,×,÷} MAWPS-S 2,373 1,311 1,062 {+,−,×,÷} Dolphin-S 7,070 115 6,955 {+,−,×,÷} Math23K 23,162 3,131 20,031 {+,−,×,÷}\n4) SingleEQ[31]. The dataset contains both single-step and multi-step arithmetic problems and is a mixture of prob-lems from a number of sources, including math-aids.com, k5learning.com, ixl.com and a subset of the data from AI2. Each problem involves operators of multiplication, division, subtraction, and addition over non-negative rational numbers.\n5) AllArith[32]. The dataset is a mixture of the data from AI2, IL, CC and SingleEQ. All mentions of quantities are normalized into digit representation. To capture how well the automatic solvers can distinguish between different problem types, near-duplicate problems(with over 80% match of unigrams and bigrams) are removed. Finally, there remain 831 math problems.\n6) MAWPS[49] is another testbed for arithmetic word prob-lems with one unknown variable in the question. Its objective is to compile a dataset of varying complexity from different websites. Operationally, it combines the published word problem datasets used in[25],[50],[31],[30]. There are 2,373 questions in the harvested dataset.\n7) Dolphin-S. This is a subset of Dolphin18K[12] which originally contains 18,460 problems and 5,871 templates with one or multiple equations. The problems whose template is associated with only one problem are extracted as the dataset of Dolphin-S. It contains 115 problems with single operator and 6,955 problems with multiple operators.\n8) Math23K[14]. The dataset contains Chinese math word problems for elementary school students and is crawled from multiple online education websites. Initially, 60,000 problems with only one unknown variable are collected. The equation templates are extracted in a rule-based manner. To ensure high precision, a large number of problems that do not fit the rules are discarded. Finally, 23,161 math problems with 2,187 templates remain.\n2.5.2 Performance Analysis\nGiven the aforementioned datasets, we merge the experimental results reported from previous works into one table. Such a unified organization can facilitate readers in identifying the methods with superior performance in each dataset. As shown in Table 2, the rows refer to the corpus of datasets and the columns are the statistic-based and tree-based methods. The cells are filled with the accuracies of these algorithms when solving math word problems in different datasets. We conduct additional experiments to cover all the cells by the tree-based solutions. These new experiment results are highlighted in blue color. Those with missing value\n6\nTABLE 2 Accuracy of statistic-based and tree-based methods in solving arithmetic problems.\nAI2 IL CC SingleEQ AllArith Dolphin-S MAWPS-S Math23K # problems 395 562 600 508 831 7,070 2,373 23,162\noperators O {+,−} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷}\nStatistic-based\nARIS[25] 2014 77.7 --48 ----Schema[26] 2015 88.64 -------Formula[27] 2016 86.07 -------LogicForm[28],[29] 2016 84.8 80.1 53.5 -----\nTree-based ALGES[31] 2015 52.4 72.9 65 72 60.4 ---ExpressionTree[30] 2015 72 73.9 45.2 66.38 79.4 26.11 --UNITDEP[32] 2017 56.2 71.0 53.5 72.25 81.7 28.78 --\nDL-based\nMathDQN[17] 2018 78.5 73.3 75.5 52.96 72.68 30.06 60.25 -Seq2SeqET[15] 2018 -------66.7 T-RNN[19] 2019 -----39.1 66.8 66.9 StackDecoder[16] 2019 -------65.8\nare indicated by “-” and it means that there was no experiment conducted for the algorithm in the particular dataset. The main reason is that they require particular efforts on logic templates and annotations, which are very non-trivial and cumbersome for experiment reproduction. There is no algorithm comparison for the dataset Math23K because the problem text is in Chinese and the feature extraction technologies proposed in the statistic-based and tree-based approaches are not applicable. From the results in Table 2, we derive the following observations worth noting and provide reasonings to explain the results.\nFirst, the statistic-based methods with advanced logic rep-resentation, such as Schema[26], Formula[27] and Logic-Form[28],[29], achieve dominating performance in the AI2 dataset. Their superiority is primarily owned to the additional efforts on annotating the text problem with more advanced logic representation. These annotations allow them to conduct fine-grained reasoning. In contrast, ARIS[25] does not work as good because it focuses on “change” schema of quantities and does not fully exploit other schemas like “compare”[26]. Since there are only hundreds of math problems in the datasets, it is feasible to make an exhaustive scan on the math problems and manually define the templates to fit these datasets. For instance, all quantities and the main-goal are first identified by rules in LogicForm[28],[29] and explicitly associated with their role-tags. Thus, with sufficient human intervention, the accuracy of statistic-baesd methods in AI2 can boost to 88.64%, much higher than that of tree-based methods. Nevertheless, these statistic-based methods are considered as brittle and rigid[12] and not scalable to handle large and diversified datasets, primarily due to the heavy annotation cost to train an accurate mapping between the text and the logic representation.\nSecond, the results of tree-based methods in AI2, IL and CC are collected from[17] where the same experimental setting of 3-fold cross validation is applied. It is interesting to observe that ALGES[31], ExpressionTree[30] and UNITDEP[32] cannot perform equally well on the three datasets.\nALGES works poorly in AI2 because irrelevant quantities exist in its math problems and ALGES is not trained with a classifier to get rid of them.\nHowever, it outperforms ExpressionTree and UNITDEP by a wide margin in the CC dataset because CC does not involve irrelevant quantities. In addition, this dataset only contains multi-step math problems. ALGES exploits the whole search space to enumerate all the possible trees, whereas ExpressionTree and ALGES use beam search for efficiency concern. UNITDEP does not work\nwell in AI2 because this dataset only involves operators+ and − and the unit dependency graph does not take effect. Moreover, its proposed context feature poses a negative impact on the AI2 dataset. After removing this feature from the input vector fed to the classifier, the accuracy in AI2 increases from 56.2% to 74.7%, but the result in CC drops from the current accuracy of 53.5% to 47.3%. Such an observation implies the limitation of hand-crafted features in UNITDEP. Among the three datasets AI2, IL and CC, MathDQN[17] achieves leading or comparable performance. In the CC dataset, which contains only multi-step problems and is considered as the most challenging one, MathDQN yields remarkable improvement and boosts the accuracy from 65% (derived by ALGES) to 75.5%. This is because MathDQN models the tree construction as Markov Decision Process and leverage the strengths of deep Q-network (DQN). By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Consequently, the RL framework demonstrates higher generality and robustness than the other tree-based methods when handling complicated scenarios. In the IL dataset, its performance is not superior to ExpressionTree as IL only contains one-step math problems. There is no need for hierarchical tree construction and cannot expose the strength of Markov Decision Process in MathDQN or the exhaustive enumeration strategy in ALGES.\nAs to the datasets of SingleEQ and AllArith, UNITDEP is a winner in both datasets, owning to the effectiveness of the proposed unit dependency graph(UDG). In the math problems with operators{×,÷}, the unit and rate are important clues to determine the correct quantities and operators in the math expression. The UDG poses constraints on unit compatibility to filter the false candidates in the expression tree construction. It can alleviate the brittleness of the unit extraction system, even though it requires additional annotation overhead in order to induce UDGs.\nLast but not the least, these experiments were conducted on small-scale datasets and their performances on larger and more diversified datasets remain unclear. Recently, Huang et al. have noticed the gap and released Dolphin18K[12] which contains 18,460 problems and 5,871 templates with one or multiple equations. The findings in[12] are astonishing. The accuracies of existing approaches for equation set problems, which will be introduced in the next section, degrade sharply to less than 25%.\nThese methods cannot even perform better than a simple baseline\n7\nthat first uses text similarity to find the most similar text problem in the training dataset and then fills the number slots in its associated equation template. Math23K is another large-scale dataset which contains Chinese math word problems. The results are shown in the last three columns of Table 2. We can see that T-RNN is state-of-the-art method in the two largest datasets Dolphin-S and Math23K.\n3 EQUATION SET SOLVER\nThe equation set problems are much more challenging because they involve multiple unknown variables to resolve and require to formulate a set of equations to obtain the final solution. The aforementioned arithmetic math problem can be viewed as a simplified variant of equation set problem with only one unknown variable. Hence, the methods introduced in this section can also be applied to solve\n\n【文档内容】\nThis motivated us to present a comprehensive survey to deliver a clear and complete picture of automatic math problem solvers. In this survey, we emphasize on algebraic word problems, summarize their extracted features and proposed techniques to bridge the semantic gap, and compare their performance in the publicly accessible datasets. We also cover automatic solvers for other types of math problems such as geometric problems that require the understanding of diagrams. Finally, we identify several emerging research directions for the readers with interests in MWPs.\nIndex Terms—math word problem, semantic parser, reasoning, survey, natural language processing, machine learning\n✦\n1 INTRODUCTION\nDesigning an automatic solver for mathematical word problems (MWPs) has a long history dated back to the 1960s[1],[2], [3], and continues to attract intensive research attention. In the past three years, more than 40 publications on this topic have emerged in the premier venues of artificial intelligence. The problem is particularly challenging because there remains a wide semantic gap to parse the human-readable words into machine-understandable logics so as to facilitate quantitative reasoning.\nHence, MWPs solvers are broadly considered as good test beds to evaluate the intelligence level of agents in terms of natural language understanding[4],[5] and the successful solving of MWPs would constitute a milestone towards general AI.\nWe categorize the evolution of MWP solvers into three major stages according to the technologies behind them, as depicted in Figure 1. In the first pioneering stage, roughly from the year 1960 to 2010, systems such as STUDENT[1], DEDUCOM[6], WORDPRO[7] and ROBUST[8], manually craft rules and schemas for pattern matchings. Thereupon, these solvers heavily rely on human interventions and can only resolve a limited number of scenarios that are defined in advance. Those early efforts for automatic understanding of natural language mathematical problems have been thoroughly reviewed in[9]. We exclude them from the scope of this survey paper and focus on the recent technology developments that have not been covered in the previous survey[9].\nIn the second stage, MWP solvers made use of semantic parsing[10],[11], with the objective of mapping the sentences\n• D. Zhang and L. Zhang are with the College of Computer Science and Technology, Zhejiang University, China. Emails:{zhangdongxi-ang37,zglumg}@gmail.com;\n• L. Wang and H. T. Shen are with the Center for Future Media and School of Computer Science and Engineering, University of Electronic Science and Technology of China. Emails: demolei@outlook.com; shenhengtao@hotmail.com\n• B. T. Dai is with School of Information Systems, Singapore Management University. Email:btdai@smu.edu.sg\n• Corresponding Author: Heng Tao Shen.\n1960-2010 2017-2019 2011-2017\n| |Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| ---|---|---|\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing||\n| Rule-based Matching|Feature Engineering<br>Statistical Learning<br>Semantic Parsing|Deep Learning\tReinforcement Learning|\nFig. 1. Technology evolving trend in solving MWPs.\nfrom problem statements into structured logic representations so as to facilitate quantitative reasoning. It has regained considerable interests from the academic community, and a booming number of methods have been proposed in the past years. These methods leveraged various strategies of feature engineering and statistical learning for performance boosting. The authors of these methods also claimed promising results in their public or manually harvested datasets. In this paper, one of our tasks is to present a comprehensive review on the proposed methods in this stage. The methods will be initially organized according to the sub-tasks of MWPs which they were designed to solve, such as arithmetic word problem(in Section 2), equation set word problem(in Section 3) and geometric word problem(in Section 5). We then examine the proposed techniques in each sub-task with a clear technical organization and accountable experimental evaluations.\nMWP solvers in the third stage were originated from an empirical work[12]. Its experimental results on a large-scale and diversified dataset showed that the status of MWP solvers was not as optimistic as they claimed to be. In fact, the accuracies of many approaches dropped sharply and there is a great room for improvement in this research area. To design more accurate and robust solutions, the subsequent publications are forked into two directions. One is to continue refining the technology of semantic parsing. For instance, Huang et al.\n2\nproposed a new type of semantic representation to conduct fine-grained inference[13]. The other direction attempts to exploit the advantages of deep learning models, with the availability of large-scale training datasets. This is an emerging research direction for MWP solvers and we observed multiple instances, including Deep Neural Solver[14], Seq2SeqET[15], StackDecoder[16], MathDQN[17], CASS[18], T-RNN[19]. These models represent a new technology trend in the research topic of MWP solvers and will be paid special attention in the survey.\nTo sum up, we present a comprehensive survey to review the MWP solvers proposed in recent years. Researchers in the community can benefit from this survey in the following ways:\n1) We provide a wide coverage on the math word problems, including arithmetic word problem, equation set problem, geometry word problem and miscellaneous sub-tasks related to automatic math solvers. The practitioners can easily identify all relevant approaches for performance evaluations.\nWe observed that the unawareness of relevant competitors occurs occasionally in the past literature. We are positive that the availability of our survey can help avoid such unawareness.\n2) The solvers designed for arithmetic word problems(AWP) with only one unknown variable and equation set problems (ESP) with multiple unknown variables are often not differ-entiated by previous works. In fact, the methods proposed for ESP are more general and can be used to solve AWP. In this survey, we clearly identify the difference and organize them in separate sections.\n3) Feature engineering plays a vital role to bridge the gap of semantic parsing. Almost all MWP solvers state their strategies of crafting effective features, resulting in a very diversified group of features, and there is a lack of clear organization among these features. In this survey, we will be the first to summarize all these proposed features in Table 5.\n4) As for the fairness on performance evaluations, ideally, there should be a benchmark dataset well accepted and widely adopted by the MWP research community, just like Ima-geNet[20] for visual object recognition and VQA[21],[22] for visual question answering. Unfortunately, we observed that many approaches tend to compile their own datasets to verify their superiorities, resulting in missing of relevant competitors as mentioned above. Tables 2 and 3 integrate the results of the existing methods on all public datasets.\nAfter collecting the accuracies that have been reported in the past literature, we observed many empty cells in the table. Each empty cell refers to a missing experiment on a particular algorithm and dataset. In this survey, we make our best efforts to fill the missing results by conducting a number of additional experiments, that allow us to provide a more comprehensive comparison and explicit analysis.\nThe remainder of the paper is organized as follows. We first review the arithmetic word problem solvers in Section 2, followed by equation set problem solvers in Section 3. Since feature engineering deserves special attention, we summarize the extracted features as well as the associated pre-processing techniques in Section 4. The geometric word problem solvers are reviewed in Section 5. We also cover miscellaneous automatic solvers related to math problems in Section 6. We conclude the paper and point out several future directions in MWPs that are worth examination in the final section.\n2 ARITHMETIC WORD PROBLEM SOLVER\nThe arithmetic word problems are targeted for elementary school students. The input is the text description for the math problem, represented in the form of a sequence of k\nwords 〈w0,w1,...,wk〉. There are n quantities q1,q2,...,qn\nmentioned in the text and an unknown variable x whose value is to be resolved. Our goal is to extract the relevant quantities and map this problem into an arithmetic expression E whose evaluation value provides the solution to the problem. There are only four types of fundamental operators O={+,−,×,÷} involved in the expression E.\nAn example of arithmetic word problem is illustrated in Figure 2. The relevant quantities to be extracted from the text include 17, 7 and 80. The number of hours spent on the bike is the unknown variable x. To solve the problem, we need to identify the correct operators between the quantities and their operation order such that we can obtain the final equation 17+ 7x= 80 or expression x=(80− 17)÷ 7 and return 9 as the solution to this problem.\nFig. 2. An example of arithmetic word problem.\nIn this section, we consider feature extraction as a black box and focus on the high-level algorithms and models. The details of feature extraction will be comprehensively present in Section 4. We classify existing algebra word problem solvers into three categories:\n| rule-based,|statistic-based,|tree-based and|\n| ---|---|---|\ndeep learning(DL)-based methods.\n2.1 Rule-based Methods\nThe early approaches to math word problems are rule-based systems based on hand engineering. Published in 1985, WORD-PRO[7] solves one-step arithmetic problems. It predefines four types of schemas, including change-in, change-out, combine and compare. The problem text is transformed into a set of propositions and the answer is derived with simple reasoning based on the propositions. Another system ROBUST, developed by Bakman[8], could understand free-format multi-step arith-metic word problems. It further expands the change schema of WORDPRO[7] into six distinct categories. The problem text is split into sentences and each sentence is mapped to a proposition.\nYun et al. also proposed to use schema for multi-step math problem solving[23]. However, the implementation details are not explicitly revealed. Since these systems have been out of date, we only provide such a brief overview to cover the representative ones. Readers can refer to[9] for a comprehensive survey of\n3\nearly rule-driven systems for automatic understanding of natural language math problems.\n2.2 Statistic-based Methods\nThe statistic-based methods leverage traditional machine learning models to identify the entities, quantities and operators from the problem text and yield the numeric answer with simple logic inference procedure. The scheme of quantity entailment proposed in[24] can be used to solve arithmetic problems with only one operator. It involves three types of classifiers to detect different properties of the word problem. The quantity pair classifier is trained to determine which pair of quantities would be used to derive the answer. The operator classifier picks the operator op ∈{+,−,×,÷} with the highest probability. The order classifier is relevant only for problems involving subtraction or division because the order of operands matters for these two types of operators. With the inferred expression, it is straightforward to calculate the numeric answer for the simple math problem.\nTo solve math problems with multi-step arithmetic expression, the statistic-based methods require more advanced logic templates.\nThis usually incurs additional overhead to annotate the text problems and associate them with the introduced template. As an early attempt, ARIS[25] defines a logic template named state that consists of a set of entities, their containers, attributes, quantities and relations. For example, “Liz has 9 black kittens” initializes the number of kitten(referring to an entity) with black color(referring to an attribute) and belonging to Liz (referring to a container). The solution splits the problem text into fragments and tracks the update of the states by verb categorization. More specifically, the verbs are classified into seven categories: observation, positive, negative, positive transfer, negative transfer, construct and destroy. To train such a classifier, we need to annotate each split fragment in the training dataset with the associated verb category. Another drawback of ARIS is that it only supports addition and subtraction.[26] follows a similar processing logic to ARIS. It predefines a corpus of logic representation named schema, inspired by[8]. The sentences in the text problem are examined sequentially until the sentence matches a schema, triggering an update operation to modify the number associated with the entities.\nMitra et al. proposed a new logic template named formula[27].\nThree types of formulas are defined, including part whole, change and comparison, to solve problems with addition and subtraction operators. For example, the text problem “Dan grew 42 turnips and 38 cantelopes. Jessica grew 47 turnips. How many turnips did they grow in total?” is annotated with the part-whole template: 〈whole: x,parts:{42,47}〉. To solve a math problem, the first step connects the assertions to the formulas. In the second step, the most probable formula is identified using the log-linear model with learned parameters and converted into an algebraic equation.\nAnother type of annotation is introduced in[28],[29] to facilitate solving a math problem. A group of logic forms are predefined and the problem text is converted into the logic form representation by certain mapping rules. For instance, the sentence “Fred picks 36 limes” will be transformed into verb(v1,pick) & nsubj(v1,Fred)& dobj(v1,n1)& head(n1,lime)& nummod(n1,36). Finally, logic inference is performed on the derived logic statements to obtain the answer.\nTo sum up, these statistical-based methods have two draw-backs that limit their usability. First, it requires additional an-notation overhead that prevents them from handling large-scale\ndatasets. Second, these methods are essentially based on a set of pre-defined templates, which are brittle and rigid. It will take great efforts to extend the templates to support other operators like multiplication and division. It is also not robust to diversified datasets. In the following, we will introduce the tree-based solutions, which are widely adopted and become the main-streaming solutions to arithmetic word problems.\n2.3 Tree-Based Methods\nThe arithmetic expression can be naturally represented as a binary tree structure such that the operators with higher priority are placed in the lower level and the root of the tree contains the operator with the lowest priority. The idea of tree-based approaches[30],[31],[32],[17] is to transform the derivation of the arithmetic expression to constructing an equivalent tree structure step by step in a bottom-up manner. One of the advantages is that there is no need for additional annotations such as equation template, tags or logic forms. Figure 3 shows two tree examples derived from the math word problem in Figure 2. One is called expression tree that is used in[30],[32],[17] and the other is called equation tree[31]. These two types of trees are essentially equivalent and result in the same solution, except that equation tree contains a node for the unknown variable x.\n÷\n-7\n80 17\n=\n+ 80\n17\n7\n×\nx\nExpression Tree Equation Tree\nFig. 3. Examples of expression tree and equation tree for Figure 2.\nThe overall algorithmic framework among the tree-based approaches consists of two processing stages. In the first stage, the quantities are extracted from the text and form the bottom level of the tree. The candidate trees that are syntactically valid, but with different structures and internal nodes, are enumerated.\nIn the second stage, a scoring function is defined to pick the best matching candidate tree, which will be used to derive the final solution. A common strategy among these algorithms is to build a local classifier to determine the likelihood of an operator being selected as the internal node. Such local likelihood is taken into account in the global scoring function to determine the likelihood of the entire tree.\nRoy et al.[30] proposed the first algorithmic approach that leverages the concept of expression tree to solve arithmetic word problems. Its first strategy to reduce the search space is training a binary classifier to determine whether an extracted quantity is relevant or not. Only the relevant ones are used for tree construction and placed in the bottom level. The irrelevant quantities are discarded. The tree construction procedure is mapped to a collection of simple prediction problems, each determining the lowest common ancestor operation between a pair of quantities mentioned in the problem. The global scoring function for an enumerated tree takes into account two terms. The\n4\nfirst one, denoted by φ(q), is the likelihood of quantity q being irrelevant, i.e., q is not used in creating the expression tree. In the ideal case, all the irrelevant quantities are correctly predicted with high confidence, resulting in a large value for the sum of φ(q).\nThe other term, denoted by φ(op), is the likelihood of selecting op as the operator for an internal tree node. With these two factors, Score(E) is formally defined as\nScore(E)= w1\n∑\nq∈I(E)\nφ(q)+ ∑\nop∈N\nφ(op) (1)\nwhere I(E) is the group of irrelevant quantities that are not included in expression E, and N refers to the set of internal tree nodes.\nTo further reduce the tree enumeration space, beam search is applied in[30]. To generate the next state T ′ from the current partial tree, the algorithm avoids choosing all the possible pairs of terms and determining their operator. Instead, only top-k candidates with the highest partial scores are retained.\nExperimental results with k= 200 show that the strategy achieves a good balance between accuracy and running time. The service is also published as a web tool[33] and it can respond promptly to a math word problem.\nALGES[31] differs from[30] in two major ways. First, it adopts a more brutal-force manner to exploit all the possible equation trees. More specifically, ALGES does not discard irrevalent quantities, but enumerates all the syntactically valid trees.\nInteger Linear Programming(ILP) is applied as it can help enforce the constraints such as syntactic validity, type consistence and domain specific simplicity considerations. Consequently, its computation cost is dozens of times higher than that in[30], according to an efficiency evaluation in[17]. Second, its scoring function is different from Equation 1. There is no need for the term φ(q) because ALGES does not build a classifier to check the quantity relevance. Besides the monotonic aggregation of the likelihood from local operator classifiers, the scoring function incorporates a new termφ(P)= θT fp to assign a coherence score for the tree instance. Here, fP is the global feature extracted from a problem text P, and θ refers to the parameter vector.\nThe goal of[34] is also to build an equation tree by parsing the problem text. It makes two assumptions that can simplify the tree construction, but also limit its applicability. First, the final output equation form is restricted to have at most two variables. Second, each quantity mentioned in the sentence can be used at most once in the final equation. The tree construction procedure consists of a pipeline of predictors that identify irrelevant quantities, recognize grounded variables, and generate the final equation tree.\nWith customized feature selection and SVM based classifier, the relevant quantities and variables are extracted and used as the leaf nodes of the equation tree. The tree is built in a bottom-up manner.\nIt is worth noting that to reduce the search space and simplify the tree construction, only adjacent nodes are combined to generate their parent node.\nUnitDep[32] can be viewed as an extension work of[30] by the same authors. An important concept, named Unit Dependency Graph(UDG), is proposed to enhance the scoring function. The vertices in UDG consist of the extracted quantities. If the quantity correspond to a rate(e.g., 8 dollars per hour), the vertex is marked as RATE. There are six types of edge relations to be considered, such as whether two quantities are associated with the same unit.\nBuilding the UDG requires additional annotation overhead as we need to train two classifiers for the nodes and edges. The node classifier determines whether a node is associated with a rate. The\nedge classifier predicts the type of relationship between any pair of quantity nodes. Given a valid unit dependency graph G generated by the classifiers, its likelihood is defined as\nφ(G)= ∑\nv∈G∧LABEL(v)=rate\nP(v)+ λ∑\ne∈G\nP(e) (2)\nIn other words, we sum up the prediction probability for the RATE nodes and all the edges. The new scoring function for an expression tree extends Equation 1 by incorporatingφ(G). Rules are defined to enforce the rate consistence between an expression tree T and a candidate graph G. For example, if vi is the only node in the tree that is labeled RATE and it appears in the question, there should not exist a path from the leaf node to the root which only contains operators of addition and subtraction. Finally, the candidate graph G with the highest likelihood and rate-consistent with T is used to calculate the total score of T.\n2.4 DL-based Methods\nIn recent years, deep learning(DL) has witnessed great success in a wide spectrum of “smart” applications, such as visual question answering[35],[36], video captioning[37],[38],[39], [40], personal interest inference[41],[42],[43] and smart transportation[44]. The main advantage is that with sufficient amount of training data, DL is able to learn an effective feature representation in a data-driven manner without human intervention. It is not surprising to notice that several efforts have been attempted to apply DL for math word problem solving.\nDeep Neural Solver(DNS)[14] is a pioneering work designed for equation set problems, which will be introduced in more details in the next section. Following DNS, there have emerged multiple DL-based solvers for arithmetic word problems.\nSeq2SeqET[15] extended the idea of DNS by using expression tree as the output sequence. In other words, it applied seq2seq model to convert the problem text into an expression tree, which can be viewed as a template. Given the output of an expression tree or template, we can easily infer the numeric answer. To reduce the template space, equation normalization was proposed in Seq2SeqET so that duplicate representation of expression trees can be unified.\nStackDecoder[16] is also based on seq2seq model. Its encoder extracts semantic meanings of quantities in the question text and the decoder is equipped with a stack to facilitate tracking the semantic meanings of operands.\nT-RNN[19] can be viewed as an improvement of Seq2SeqET, in terms of quantity encoding, template representation and tree construction. First, an effective embedding network with Bi-LSTM and self attention is used to vectorize the quantities. Second, the detailed operators in the templates are encapsulated to further reduce the number of template space. For example, n1+n2, n1−n2, n1×n2, and n1÷n2 are mapped to the same template n1〈op〉n2. Third, a recursive neural network is applied to infer the unknown variables in the expression tree in a recursive manner.\nWang et al. made the first attempt of applying deep re-inforcement learning to solve arithmetic word problems[17].\nThe motivation is that deep Q-network has witnessed success in solving various problems with big search space such as playing text-based games[45], information extraction[46], text generation[47] and object detection in images[48]. To fit the math problem scenario, they formulate the expression tree construction as a Markov Decision Process and propose the MathDQN that is customized from the general deep reinforcement\n5\nlearning framework. Technically, they tailor the definitions of states, actions, and reward functions which are key components in the reinforcement learning framework. By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Compared to the aforementioned approaches, MathDQN iteratively picks the best operator for two selected quantities. This procedure can be viewed as beam search with k= 1 when exploiting candidate expression trees. Its deep Q-network acts as the operator classifier and guides the model to select the most promising operator for tree construction.\n2.5 Dataset Repository and Performance Analysis\nThe accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites. These datasets are small-scale and contain hundreds of math problems. In this subsection, we make a summary on the datasets that have been used in the aforementioned papers.\nMoreover, we organize the performance results on these datasets into one unified table. We also make our best efforts to conduct additional experiments. The new results are highlighted in blue color. In this way, readers can easily identify the best performers in each dataset.\n2.5.1 Datasets\nThere have been a number of datasets collected for the arithmetic word problems. We present their descriptions in the following and summarize the statistics of the datasets in Table 1.\n1) AI2[25]. There are 395 single-step or multi-step arithmetic word problems for the third, fourth, and fifth graders. It involves problems that can be solved with only addition and subtraction. The dataset is harvested from two websites: math-aids.com and ixl.com and comprises three subsets: MA1(from math-aids.com), IXL(from ixl.com) and MA2(from math-aids.com). Among them, IXL and MA2 are more challenging than MA1 because IXL contains more information gaps and MA2 includes more irrelevant information in its math problems.\n2) IL[30]. The problems are collected from websites k5learning.com and dadsworksheets.com. The problems that require background knowledge(e.g., “apple is fruit” and “a week comprises 7 days”) are pruned. To improve the diversity, the problems are clustered by textual similarity.\nFor each cluster, at most 5 problems are retained. Finally, the dataset contains 562 single-step word problems with only one operator, including addition, subtraction, multiplication, and division.\n3) CC[30]. The dataset is designed for mult-step math prob-lems. It contains 600 multi-step problems without irrelevant quantities, harvested from commoncoresheets.com. The dataset involves various combinations of four basic operators, including(a) addition followed by subtraction;(b) subtraction followed by addition;(c) addition and multiplication;(d) addition and division;(e) subtraction and multiplication; and (f) subtraction and division. It is worth noting that this dataset does not incorporate irrelevant quantities in the problem text.\nHence, there is no need to apply the quantity relevance classifier for the algorithms containing this component.\nTABLE 1 Statistics of arithmetic word problem datasets.\nDataset # problems # single-op # multi-op operators O MA1 134 112 22 {+,−} IXL 140 119 21 {+,−} MA2 121 96 25 {+,−} AI2 395 327 68 {+,−} IL 562 562 0 {+,−,×,÷} CC 600 0 600 {+,−,×,÷} SingleEQ 508 390 118 {+,−,×,÷} AllArith 831 634 197 {+,−,×,÷} MAWPS-S 2,373 1,311 1,062 {+,−,×,÷} Dolphin-S 7,070 115 6,955 {+,−,×,÷} Math23K 23,162 3,131 20,031 {+,−,×,÷}\n4) SingleEQ[31]. The dataset contains both single-step and multi-step arithmetic problems and is a mixture of prob-lems from a number of sources, including math-aids.com, k5learning.com, ixl.com and a subset of the data from AI2. Each problem involves operators of multiplication, division, subtraction, and addition over non-negative rational numbers.\n5) AllArith[32]. The dataset is a mixture of the data from AI2, IL, CC and SingleEQ. All mentions of quantities are normalized into digit representation. To capture how well the automatic solvers can distinguish between different problem types, near-duplicate problems(with over 80% match of unigrams and bigrams) are removed. Finally, there remain 831 math problems.\n6) MAWPS[49] is another testbed for arithmetic word prob-lems with one unknown variable in the question. Its objective is to compile a dataset of varying complexity from different websites. Operationally, it combines the published word problem datasets used in[25],[50],[31],[30]. There are 2,373 questions in the harvested dataset.\n7) Dolphin-S. This is a subset of Dolphin18K[12] which originally contains 18,460 problems and 5,871 templates with one or multiple equations. The problems whose template is associated with only one problem are extracted as the dataset of Dolphin-S. It contains 115 problems with single operator and 6,955 problems with multiple operators.\n8) Math23K[14]. The dataset contains Chinese math word problems for elementary school students and is crawled from multiple online education websites. Initially, 60,000 problems with only one unknown variable are collected. The equation templates are extracted in a rule-based manner. To ensure high precision, a large number of problems that do not fit the rules are discarded. Finally, 23,161 math problems with 2,187 templates remain.\n2.5.2 Performance Analysis\nThe performances of the arithmetic word problems are evaluated on the datasets that are manually harvested and annotated from online websites. These datasets are small-scale and contain hundreds of math problems. In this subsection, we make a summary on the datasets that have been used in the aforementioned papers.\nMoreover, we organize the performance results on these datasets into one unified table. We also make our best efforts to conduct additional experiments. The new results are highlighted in blue color. In this way, readers can easily identify the best performers in each dataset.\nTABLE 2 Accuracy of statistic-based and tree-based methods in solving arithmetic problems.\nAI2 IL CC SingleEQ AllArith Dolphin-S MAWPS-S Math23K # problems 395 562 600 508 831 7,070 2,373 23,162\noperators O {+,−} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷} {+,−,×,÷}\nStatistic-based\nARIS[25] 2014 77.7 --48 ----Schema[26] 2015 88.64 -------Formula[27] 2016 86.07 -------LogicForm[28],[29] 2016 84.8 80.1 53.5 -----\nTree-based ALGES[31] 2015 52.4 72.9 65 72 60.4 ---ExpressionTree[30] 2015 72 73.9 45.2 66.38 79.4 26.11 --UNITDEP[32] 2017 56.2 71.0 53.5 72.25 81.7 28.78 --\nDL-based\nMathDQN[17] 2018 78.5 73.3 75.5 52.96 72.68 30.06 60.25 -Seq2SeqET[15] 2018 -------66.7 T-RNN[19] 2019 -----39.1 66.8 66.9 StackDecoder[16] 2019 -------65.8\nare indicated by “-” and it means that there was no experiment conducted for the algorithm in the particular dataset. The main reason is that they require particular efforts on logic templates and annotations, which are very non-trivial and cumbersome for experiment reproduction. There is no algorithm comparison for the dataset Math23K because the problem text is in Chinese and the feature extraction technologies proposed in the statistic-based and tree-based approaches are not applicable. From the results in Table 2, we derive the following observations worth noting and provide reasonings to explain the results.\nFirst, the statistic-based methods with advanced logic rep-resentation, such as Schema[26], Formula[27] and Logic-Form[28],[29], achieve dominating performance in the AI2 dataset. Their superiority is primarily owned to the additional efforts on annotating the text problem with more advanced logic representation. These annotations allow them to conduct fine-grained reasoning. In contrast, ARIS[25] does not work as good because it focuses on “change” schema of quantities and does not fully exploit other schemas like “compare”[26]. Since there are only hundreds of math problems in the datasets, it is feasible to make an exhaustive scan on the math problems and manually define the templates to fit these datasets. For instance, all quantities and the main-goal are first identified by rules in LogicForm[28],[29] and explicitly associated with their role-tags. Thus, with sufficient human intervention, the accuracy of statistic-baesd methods in AI2 can boost to 88.64%, much higher than that of tree-based methods. Nevertheless, these statistic-based methods are considered as brittle and rigid[12] and not scalable to handle large and diversified datasets, primarily due to the heavy annotation cost to train an accurate mapping between the text and the logic representation.\nSecond, the results of tree-based methods in AI2, IL and CC are collected from[17] where the same experimental setting of 3-fold cross validation is applied. It is interesting to observe that ALGES[31], ExpressionTree[30] and UNITDEP[32] cannot perform equally well on the three datasets.\nALGES works poorly in AI2 because irrelevant quantities exist in its math problems and ALGES is not trained with a classifier to get rid of them.\nHowever, it outperforms ExpressionTree and UNITDEP by a wide margin in the CC dataset because CC does not involve irrelevant quantities. In addition, this dataset only contains multi-step math problems. ALGES exploits the whole search space to enumerate all the possible trees, whereas ExpressionTree and ALGES use beam search for efficiency concern. UNITDEP does not work\nwell in AI2 because this dataset only involves operators+ and − and the unit dependency graph does not take effect. Moreover, its proposed context feature poses a negative impact on the AI2 dataset. After removing this feature from the input vector fed to the classifier, the accuracy in AI2 increases from 56.2% to 74.7%, but the result in CC drops from the current accuracy of 53.5% to 47.3%. Such an observation implies the limitation of hand-crafted features in UNITDEP. Among the three datasets AI2, IL and CC, MathDQN[17] achieves leading or comparable performance. In the CC dataset, which contains only multi-step problems and is considered as the most challenging one, MathDQN yields remarkable improvement and boosts the accuracy from 65% (derived by ALGES) to 75.5%. This is because MathDQN models the tree construction as Markov Decision Process and leverage the strengths of deep Q-network (DQN). By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment. Consequently, the RL framework demonstrates higher generality and robustness than the other tree-based methods when handling complicated scenarios. In the IL dataset, its performance is not superior to ExpressionTree as IL only contains one-step math problems. There is no need for hierarchical tree construction and cannot expose the strength of Markov Decision Process in MathDQN or the exhaustive enumeration strategy in ALGES.\nAs to the datasets of SingleEQ and AllArith, UNITDEP is a winner in both datasets, owning to the effectiveness of the proposed unit dependency graph(UDG). In the math problems with operators{×,÷}, the unit and rate are important clues to determine the correct quantities and operators in the math expression. The UDG poses constraints on unit compatibility to filter the false candidates in the expression tree construction. It can alleviate the brittleness of the unit extraction system, even though it requires additional annotation overhead in order to induce UDGs.\nLast but not the least, these experiments were conducted on small-scale datasets and their performances on larger and more diversified datasets remain unclear. Recently, Huang et al. have noticed the gap and released Dolphin18K[12] which contains 18,460 problems and 5,871 templates with one or multiple equations. The findings in[12] are astonishing. The accuracies of existing approaches for equation set problems, which will be introduced in the next section, degrade sharply to less than 25%.\nThese methods cannot even perform better than a simple baseline\n7\nthat first uses text similarity to find the most similar text problem in the training dataset and then fills the number slots in its associated equation template. Math23K is another large-scale dataset which contains Chinese math word problems. The results are shown in the last three columns of Table 2. We can see that T-RNN is state-of-the-art method in the two largest datasets Dolphin-S and Math23K.\n3 EQUATION SET SOLVER\nThe equation set problems are much more challenging because they involve multiple unknown variables to resolve and require to formulate a set of equations to obtain the final solution. The aforementioned arithmetic math problem can be viewed as a simplified variant of equation set problem with only one unknown variable. Hence, the methods introduced in this section can also be applied to solve the problems in Section 2.\nFigure 4 shows", "complete": true, "attempt": 0, "timestamp": "2025-05-22T21:05:50.753661"}