[
    {
        "from_entity": "Wang2019_TemplateBasedMathWordProblemSolver",
        "to_entity": "Hosseini2014_StateTransitionModel",
        "relation_type": "Improve",
        "structure": "Architecture.Mechanism",
        "detail": "Template-based method with recursive neural networks improves upon state transition model",
        "problem_addressed": "Performance degradation in large space of target expressions",
        "evidence": "The best performer in Math23K is a seq2seq model based on LSTM to generate the math expression. However, the model suffers from performance degradation in large space of target expressions. In this paper, we propose a template-based solution based on recursive neural network for math expression construction.",
        "confidence": 0.95
    },
    {
        "from_entity": "Wang2019_TemplateBasedMathWordProblemSolver",
        "to_entity": "Math23K",
        "relation_type": "Use",
        "structure": "Dataset.Evaluation",
        "detail": "Evaluates the proposed method on Math23K dataset",
        "problem_addressed": "Evaluation of generalization and robustness",
        "evidence": "The experimental results clearly establish the superiority of our new framework as we improve the accuracy by a wide margin in two of the largest datasets, i.e., from 58.1% to 66.9% in Math23K",
        "confidence": 0.95
    },
    {
        "from_entity": "Wang2019_TemplateBasedMathWordProblemSolver",
        "to_entity": "MAWPS",
        "relation_type": "Use",
        "structure": "Dataset.Evaluation",
        "detail": "Evaluates the proposed method on MAWPS dataset",
        "problem_addressed": "Evaluation of generalization and robustness",
        "evidence": "The experimental results clearly establish the superiority of our new framework as we improve the accuracy by a wide margin in two of the largest datasets, i.e., from 62.8% to 66.8% in MAWPS.",
        "confidence": 0.95
    },
    {
        "from_entity": "Wang2019_TemplateBasedMathWordProblemSolver",
        "to_entity": "LSTM",
        "relation_type": "Optimize",
        "structure": "Architecture.Component",
        "detail": "Uses Bi-LSTM for encoding quantities",
        "problem_addressed": "Enhancing the encoding of numerical information",
        "evidence": "Then, we design a recursive neural network to encode the quantity with Bi-LSTM and self attention",
        "confidence": 0.9
    },
    {
        "from_entity": "Wang2019_TemplateBasedMathWordProblemSolver",
        "to_entity": "SelfAttention",
        "relation_type": "Use",
        "structure": "Architecture.Component",
        "detail": "Incorporates self-attention mechanism",
        "problem_addressed": "Improving the representation of numerical information",
        "evidence": "Then, we design a recursive neural network to encode the quantity with Bi-LSTM and self attention",
        "confidence": 0.9
    }
]