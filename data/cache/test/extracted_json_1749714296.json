{"entities": [{"algorithm_entity": {"algorithm_id": "Mikolov2013_SkipGram", "entity_type": "Algorithm", "name": "Skip-gram model", "year": 2013, "authors": ["Mikolov, T.", "Chen, K.", "Corrado, G.", "Dean, J."], "task": "Word and phrase representation learning", "dataset": ["News dataset_2013"], "metrics": ["Accuracy_Classification", "Syntactic_Analogy_Task", "Semantic_Analogy_Task"], "architecture": {"components": ["Input layer", "Hidden layer", "Output layer"], "connections": ["Word embeddings", "Hierarchical softmax", "Negative sampling"], "mechanisms": ["Continuous Bag-of-Words", "Subsampling of frequent words"]}, "methodology": {"training_strategy": ["Hierarchical softmax", "Negative sampling", "Subsampling of frequent words"], "parameter_tuning": ["Vector dimensionality", "Context size", "Training window size"]}, "feature_processing": ["Subsampling of frequent words", "Phrase identification"]}}, {"dataset_entity": {"dataset_id": "News_dataset_2013", "entity_type": "Dataset", "name": "News dataset", "description": "A large dataset consisting of various news articles", "domain": "Natural Language Processing", "size": 1000000000, "year": 2013, "creators": ["Google Inc."]}}, {"metric_entity": {"metric_id": "Accuracy_Classification", "entity_type": "Metric", "name": "Accuracy", "description": "Overall accuracy of the model", "category": "Classification", "formula": "Correct predictions / Total predictions"}}, {"metric_entity": {"metric_id": "Syntactic_Analogy_Task", "entity_type": "Metric", "name": "Syntactic analogy task accuracy", "description": "Accuracy on syntactic analogy tasks", "category": "Analogy task", "formula": "Correct answers / Total questions"}}, {"metric_entity": {"metric_id": "Semantic_Analogy_Task", "entity_type": "Metric", "name": "Semantic analogy task accuracy", "description": "Accuracy on semantic analogy tasks", "category": "Analogy task", "formula": "Correct answers / Total questions"}}, {"algorithm_entity": {"algorithm_id": "Morin2005_HierarchicalProbabilisticNNLM", "entity_type": "Algorithm", "name": "Hierarchical Probabilistic Neural Network Language Model", "year": 2005, "authors": ["Morin, F.", "Bengio, Y."], "task": "Language modeling", "dataset": ["News dataset_2013"], "metrics": ["Accuracy_Classification"], "architecture": {"components": ["Input layer", "Hidden layer", "Binary tree output layer"], "connections": ["Word embeddings", "Hierarchical softmax"], "mechanisms": ["Random walk for probability assignment"]}, "methodology": {"training_strategy": ["Hierarchical softmax"], "parameter_tuning": ["Vector dimensionality", "Context size"]}, "feature_processing": ["Subsampling of frequent words"]}}, {"algorithm_entity": {"algorithm_id": "Gutmann2012_NoiseContrastiveEstimation", "entity_type": "Algorithm", "name": "Noise Contrastive Estimation", "year": 2012, "authors": ["Gutmann, M.U.", "Hyv\u00e4rinen, A."], "task": "Language modeling", "dataset": ["News dataset_2013"], "metrics": ["Accuracy_Classification"], "architecture": {"components": ["Input layer", "Hidden layer", "Output layer"], "connections": ["Logistic regression", "Noise distribution"], "mechanisms": ["Log probability maximization"]}, "methodology": {"training_strategy": ["Noise distribution sampling"], "parameter_tuning": ["Number of negative samples"]}, "feature_processing": ["Subsampling of frequent words"]}}], "extraction_info": {"is_complete": false, "current_section": "INTRODUCTION", "next_section": "METHOD"}}