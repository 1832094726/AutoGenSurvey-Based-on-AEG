["", [17], {"from_entity": "Mikolov2013_SkipGram", "to_entity": "Wang2016_DimensionallyGuidedSynthesis", "relation_type": "Use", "structure": "Dimensionally Guided Synthesis uses the Skip-gram model to generate math word problems.", "detail": "Dimensionally Guided Synthesis employs the word vector representations and negative sampling techniques from the Skip-gram model to generate math word problems.", "problem_addressed": "Generate math word problems that conform to natural language expression.", "evidence": "Skip-gram model is used for learning high-quality word vector representations (see ]Distributed representations of words and phrases and their compositionality.pdf). Dimensionally Guided Synthesis uses word vector representations from the Skip-gram model (see tmpkwtol6qe.txt).", "confidence": 0.95}, [17], {"from_entity": "Mikolov2013_SkipGram", "to_entity": "Shi2015_SigmaDolphin", "relation_type": "Use", "structure": "SigmaDolphin uses word vector representations from the Skip-gram model to solve math word problems.", "detail": "SigmaDolphin employs word vector representations and negative sampling techniques from the Skip-gram model to generate math expressions.", "problem_addressed": "Convert natural language text into math expressions and solve them.", "evidence": "Skip-gram model is used for learning high-quality word vector representations (see ]Distributed representations of words and phrases and their compositionality.pdf). SigmaDolphin uses word vector representations from the Skip-gram model (see tmpkwtol6qe.txt).", "confidence": 0.95}, {"relations": [{"from_entity": "Slagle1965_DEDUCOM", "to_entity": "Yu2016_ImplicitQuantityRelationsExtractor", "relation_type": "Extend", "structure": "DEDUCOM is a deductive question-answering system, while Implicit Quantity Relations Extractor extends this concept to extract implicit quantity relations in Chinese arithmetic word problems.", "detail": "Implicit Quantity Relations Extractor builds on the idea of DEDUCOM by using more sophisticated semantic models and SVM classification to handle implicit quantity relations.", "problem_addressed": "Extracting implicit quantity relations is a critical step in solving complex arithmetic word problems, which DEDUCOM could not handle.", "evidence": "DEDUCOM uses simple logical deduction and recursive reasoning (see [17]Experiments with a deductive question-answering program.pdf). Implicit Quantity Relations Extractor uses more complex semantic models and SVM classification (see ction of implicit quantity relations for arithmetic word problems in chinese.pdf).", "confidence": 0.95}, {"from_entity": "Ma2010_FrameBasedCalculus", "to_entity": "Yu2016_ImplicitQuantityRelationsExtractor", "relation_type": "Comparison", "structure": "Both Frame-Based Calculus and Implicit Quantity Relations Extractor aim to solve arithmetic word problems but use different methods.", "detail": "Frame-Based Calculus uses frames and production rules, while Implicit Quantity Relations Extractor uses semantic models and SVM classification.", "problem_addressed": "Compare the effectiveness of the two methods in solving arithmetic word problems.", "evidence": "Implicit Quantity Relations Extractor uses semantic models and SVM classification (see ction of implicit quantity relations for arithmetic word problems in chinese.pdf). Frame-Based Calculus uses frames and production rules (see ulus of solving arithmetic multi-step addition and subtraction word problems.pdf).", "confidence": 0.95}, {"from_entity": "Mikolov2013_SkipGram", "to_entity": "Wang2016_DimensionallyGuidedSynthesis", "relation_type": "Use", "structure": "Dimensionally Guided Synthesis uses the Skip-gram model to generate math word problems.", "detail": "Dimensionally Guided Synthesis employs the word vector representations and negative sampling techniques from the Skip-gram model to generate math word problems.", "problem_addressed": "Generate math word problems that conform to natural language expression.", "evidence": "Skip-gram model is used for learning high-quality word vector representations (see ]Distributed representations of words and phrases and their compositionality.pdf). Dimensionally Guided Synthesis uses word vector representations from the Skip-gram model (see tmpkwtol6qe.txt).", "confidence": 0.95}, {"from_entity": "Slagle1965_DEDUCOM", "to_entity": "Ma2010_FrameBasedCalculus", "relation_type": "Extend", "structure": "DEDUCOM is a deductive question-answering system, while Frame-Based Calculus extends this concept to solve multi-step arithmetic addition and subtraction word problems.", "detail": "Frame-Based Calculus builds on the idea of DEDUCOM by introducing frames and production rules to handle multi-step arithmetic addition and subtraction word problems.", "problem_addressed": "Solve multi-step arithmetic addition and subtraction word problems, which DEDUCOM could not handle.", "evidence": "DEDUCOM uses simple logical deduction and recursive reasoning (see [17]Experiments with a deductive question-answering program.pdf). Frame-Based Calculus uses frames and production rules (see ulus of solving arithmetic multi-step addition and subtraction word problems.pdf).", "confidence": 0.95}, {"from_entity": "Yu2016_ImplicitQuantityRelationsExtractor", "to_entity": "Ma2010_FrameBasedCalculus", "relation_type": "Comparison", "structure": "Both Implicit Quantity Relations Extractor and Frame-Based Calculus aim to solve arithmetic word problems but use different methods.", "detail": "Implicit Quantity Relations Extractor uses semantic models and SVM classification, while Frame-Based Calculus uses frames and production rules.", "problem_addressed": "Compare the effectiveness of the two methods in solving arithmetic word problems.", "evidence": "Implicit Quantity Relations Extractor uses semantic models and SVM classification (see ction of implicit quantity relations for arithmetic word problems in chinese.pdf). Frame-Based Calculus uses frames and production rules (see ulus of solving arithmetic multi-step addition and subtraction word problems.pdf).", "confidence": 0.95}, {"from_entity": "Mikolov2013_SkipGram", "to_entity": "Chen2014_EfficientDependencyParser", "relation_type": "Improve", "structure": "Efficient Dependency Parser improves upon the Skip-gram model in dependency parsing tasks.", "detail": "Efficient Dependency Parser uses bidirectional LSTM and neural network classifiers, which outperform the Skip-gram model in dependency parsing tasks.", "problem_addressed": "Improve the accuracy of dependency parsing.", "evidence": "Skip-gram model is used for learning high-quality word vector representations (see ]Distributed representations of words and phrases and their compositionality.pdf). Efficient Dependency Parser uses bidirectional LSTM and neural network classifiers (see tmpkwtol6qe.txt).", "confidence": 0.95}, {"from_entity": "Mikolov2013_SkipGram", "to_entity": "Shi2015_SigmaDolphin", "relation_type": "Use", "structure": "SigmaDolphin uses word vector representations from the Skip-gram model to solve math word problems.", "detail": "SigmaDolphin employs word vector representations and negative sampling techniques from the Skip-gram model to generate math expressions.", "problem_addressed": "Convert natural language text into math expressions and solve them.", "evidence": "Skip-gram model is used for learning high-quality word vector representations (see ]Distributed representations of words and phrases and their compositionality.pdf). SigmaDolphin uses word vector representations from the Skip-gram model (see tmpkwtol6qe.txt).", "confidence": 0.95}], "extraction_info": {"is_complete": false}}]