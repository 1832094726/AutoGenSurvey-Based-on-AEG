[
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Lin2017_StructuredSelfAttention",
    "relation_type": "Extend",
    "structure": "Architecture.Component",
    "detail": "Structured Self-attentive Sentence Embedding extends to include multi-hop attention mechanism",
    "problem_addressed": "Limited representation of sentence semantics with single vector embedding",
    "evidence": "We propose a self-attention mechanism for these sequential models to replace the max pooling or averaging step. Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
    "confidence": 0.95
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Age_Dataset_2017",
    "relation_type": "Use",
    "structure": "Dataset.Task",
    "detail": "Used for author profiling task",
    "problem_addressed": "Need for effective sentence embedding for author profiling",
    "evidence": "We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment.",
    "confidence": 0.9
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Yelp_Dataset_2017",
    "relation_type": "Use",
    "structure": "Dataset.Task",
    "detail": "Used for sentiment classification task",
    "problem_addressed": "Need for effective sentence embedding for sentiment classification",
    "evidence": "We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment.",
    "confidence": 0.9
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "SNLI_Corpus_2015",
    "relation_type": "Use",
    "structure": "Dataset.Task",
    "detail": "Used for textual entailment task",
    "problem_addressed": "Need for effective sentence embedding for textual entailment",
    "evidence": "We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment.",
    "confidence": 0.9
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Accuracy_Classification",
    "relation_type": "Use",
    "structure": "Metric.Task",
    "detail": "Classification accuracy used as evaluation metric",
    "problem_addressed": "Need for a suitable evaluation metric for classification tasks",
    "evidence": "We use classification accuracy as a measurement.",
    "confidence": 0.9
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Chen2014_NeuralNetworkParser",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon neural network parser by incorporating self-attention mechanism",
    "problem_addressed": "Limited ability to capture long-range dependencies in neural network parsers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation. Thus the LSTM doesnâ€™t need to carry every piece of information towards its last hidden state.",
    "confidence": 0.85
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Hochreiter1997_LongShortTermMemory",
    "relation_type": "Extend",
    "structure": "Architecture.Component",
    "detail": "Extends LSTM with self-attention mechanism",
    "problem_addressed": "Difficulty in capturing long-term dependencies in LSTM",
    "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations. It is performed on top of an LSTM in our sentence embedding model.",
    "confidence": 0.9
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "DosSantos2016_AttentivePoolingNetworks",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon attentive pooling networks by introducing a penalization term",
    "problem_addressed": "Redundancy in sentence embeddings",
    "evidence": "The embedding matrix M can suffer from redundancy problems if the attention mechanism always provides similar summation weights for all the r hops. Thus we need a penalization term to encourage the diversity of summation weight vectors across different hops of attention.",
    "confidence": 0.85
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Kiros2015_SkipThoughtVectors",
    "relation_type": "Improve",
    "structure": "Architecture.Component",
    "detail": "Improves upon skip-thought vectors by using self-attention mechanism",
    "problem_addressed": "Limited effectiveness of skip-thought vectors in capturing sentence semantics",
    "evidence": "We hypothesize that carrying the semantics along all time steps of a recurrent model is relatively hard and not necessary. We propose a self-attention mechanism for these sequential models to replace the max pooling or averaging step.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Lee2016_SequentialShortTextClassification",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon sequential short-text classification by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in sequential models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.85
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Bowman2015_FastUnifiedModel",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon fast unified model by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in unified models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "DosSantos2014_DeepConvolutionalNeuralNetworks",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon deep convolutional neural networks by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in convolutional models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Kim2014_ConvolutionalNeuralNetworks",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon convolutional neural networks by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in convolutional models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Tai2015_ImprovedSemanticRepresentations",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon improved semantic representations by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in tree-structured LSTM models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Palangi2016_DeepSentenceEmbedding",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon deep sentence embedding by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in deep sentence embedding models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Feng2015_ApplyingDeepLearning",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon applying deep learning for answer selection by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in deep learning models for answer selection",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Munkhdalai2016_NeuralTreeIndexers",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon neural tree indexers by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in neural tree indexers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Munkhdalai2016_NeuralSemanticEncoders",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon neural semantic encoders by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in neural semantic encoders",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Parikh2016_DecomposableAttentionModel",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon decomposable attention model by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in decomposable attention models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Yin2015_ConvolutionalNeuralNetwork",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon convolutional neural network for paraphrase identification by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in convolutional models for paraphrase identification",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Socher2013_RecursiveDeepModels",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon recursive deep models by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in recursive deep models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Tan2016_ImprovedRepresentationLearning",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon improved representation learning by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in representation learning models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "DosSantos2014_DeepConvolutionalNeuralNetworks",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon deep convolutional neural networks by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in deep convolutional models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "DosSantos2016_AttentivePoolingNetworks",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon attentive pooling networks by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in attentive pooling networks",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "DosSantos2014_DeepConvolutionalNeuralNetworks",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon deep convolutional neural networks by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in deep convolutional models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Mou2015_NaturalLanguageInference",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon natural language inference models by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in natural language inference models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Mou2015_DiscriminativeNeuralSentenceModeling",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon discriminative neural sentence modeling by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in discriminative neural sentence models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Li2016_OpenDomainFactoidQA",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon open-domain factoid QA models by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in open-domain factoid QA models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Chang2013_ConstrainedLatentLeftLinkingModel",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon constrained latent left linking model by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in constrained latent left linking models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Chang2013_ProbabilisticLatentLeftLinkingModel",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon probabilistic latent left linking model by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in probabilistic latent left linking models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Chang2013_LatentLeftLinkingModel",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon latent left linking model by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in latent left linking models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Raghunathan2010_MultiPassSieve",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon multi-pass sieve for coreference resolution by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in multi-pass sieve models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Charniak2000_MaximumEntropyParser",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon maximum entropy parser by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in maximum entropy parsers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Lin1998_MINIPAR",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon MINIPAR dependency parser by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in dependency parsers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Sleator1993_LinkParser",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon link parser by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in link parsers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Hajishirzi2013_NECO",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon NECO joint coreference resolution and named-entity linking by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in joint coreference resolution and named-entity linking models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_QuantityExtraction",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon quantity extraction by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in quantity extraction models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_QuantityEntailment",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon quantity entailment by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in quantity entailment models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_MathWordProblemSolver",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon math word problem solver by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in math word problem solvers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2016_ExpressionTreeBasedSolver",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon expression tree-based solver by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in expression tree-based solvers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2017_UNITDEP",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon unit dependency graph by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in unit dependency graph models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_LCA++",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon LCA++ by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in LCA++ models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Kushman2014_TemplateBased",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon template-based method by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in template-based methods",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Koncel-Kedziorski2015_ALGES",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon ALGES by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in ALGES models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2018_KNOWLEDGE",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon KNOWLEDGE by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in KNOWLEDGE models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Wang2018_MathDQN",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon MathDQN by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in MathDQN models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Luong2016_MultiTaskSeq2Seq",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon multi-task sequence-to-sequence learning by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in multi-task sequence-to-sequence models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Wiseman2016_BeamSearchOptimization",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon beam search optimization by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in beam search optimization models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2016_ExpressionTreeBasedSolver",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon expression tree-based solver by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in expression tree-based solvers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Seo2015_GEOS",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon GEOS by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in GEOS models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Seo2014_G-ALIGNER",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon G-ALIGNER by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in G-ALIGNER models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_QuantityExtraction",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon quantity extraction by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in quantity extraction models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_QuantityEntailment",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon quantity entailment by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in quantity entailment models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_MathWordProblemSolver",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon math word problem solver by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in math word problem solvers",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2017_UNITDEP",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon UNITDEP by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in UNITDEP models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2015_LCA++",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon LCA++ by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in LCA++ models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Kushman2014_TemplateBased",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon template-based method by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in template-based methods",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Koncel-Kedziorski2015_ALGES",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon ALGES by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in ALGES models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Roy2018_KNOWLEDGE",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon KNOWLEDGE by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in KNOWLEDGE models",
    "evidence": "Introducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation.",
    "confidence": 0.8
  },
  {
    "from_entity": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
    "to_entity": "Wang2018_MathDQN",
    "relation_type": "Improve",
    "structure": "Architecture.Mechanism",
    "detail": "Improves upon MathDQN by using self-attention mechanism",
    "problem_addressed": "Limited ability to capture sentence semantics in MathDQN models",
    "evidence": "Introducing attention mechanism allows the final