```json
[
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Chen2014_NeuralNetworkDependencyParser",
    "relation_type": "Use",
    "structure": "Dataset.Mechanism",
    "detail": "Chen et al. (2014) used the Skip-gram model for word embeddings in their dependency parser.",
    "problem_addressed": "Need for high-quality word representations in dependency parsing",
    "evidence": "The Skip-gram model is used to learn high-quality vector representations of words from large amounts of unstructured text data (Mikolov et al., 2013). Chen et al. (2014) utilized these word embeddings in their neural network dependency parser.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mnih2009_HierarchicalSoftmax",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model extends the hierarchical softmax for efficient training.",
    "problem_addressed": "Inefficiency in training large-scale word representation models",
    "evidence": "We used the hierarchical softmax, dimensionality of 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy of 72% (Mikolov et al., 2013). Mnih and Hinton (2009) introduced the hierarchical softmax for efficient training of large-scale language models.",
    "confidence": 0.92
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Gutmann2012_NoiseContrastiveEstimation",
    "relation_type": "Compare",
    "structure": "Methodology.Metric",
    "detail": "The Skip-gram model compares its performance with Noise Contrastive Estimation.",
    "problem_addressed": "Evaluation of different training methods for word representation models",
    "evidence": "While NCE can be shown to approximately maximize the log probability of the softmax, the Skip-gram model is only concerned with learning high-quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality (Gutmann and Hyvärinen, 2012).",
    "confidence": 0.88
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_NegativeSampling",
    "relation_type": "Optimize",
    "structure": "Architecture.Mechanism",
    "detail": "Negative Sampling optimizes the Skip-gram model for faster training and better vector representations.",
    "problem_addressed": "Slow training speed and inefficiency in handling large vocabularies",
    "evidence": "We define Negative sampling (NEG) by the objective which is used to replace every log P(wO|wI) term in the Skip-gram objective. Thus the task is to distinguish the target word wO from draws from the noise distribution Pn(w) using logistic regression (Mikolov et al., 2013).",
    "confidence": 0.94
  },
  {
    "from_entity": "Mikolov2013_Subsampling",
    "to_entity": "Mikolov2013_SkipGram",
    "relation_type": "Optimize",
    "structure": "Methodology.Mechanism",
    "detail": "Subsampling of frequent words optimizes the Skip-gram model for faster training and better representations of uncommon words.",
    "problem_addressed": "Imbalance between frequent and rare words in large corpora",
    "evidence": "To counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word wi in the training set is discarded with probability computed by the formula P(wi)= 1− √ t f(wi) (Mikolov et al., 2013).",
    "confidence": 0.96
  },
  {
    "from_entity": "Ma2010_MSMPAS_CP",
    "to_entity": "Yu2016_ImplicitQuantityRelations",
    "relation_type": "Extend",
    "structure": "Architecture.Methodology",
    "detail": "Yu et al. (2016) extended the frame-based calculus approach of Ma et al. (2010) to extract implicit quantity relations in arithmetic word problems.",
    "problem_addressed": "Limited ability to extract implicit quantity relations in arithmetic word problems",
    "evidence": "Ma et al. (2010) proposed a frame-based calculus method for solving multi-step addition and subtraction word problems. Yu et al. (2016) extended this approach to extract implicit quantity relations in arithmetic word problems.",
    "confidence": 0.93
  },
  {
    "from_entity": "Ma2010_MSMPAS_CP",
    "to_entity": "Elementary_school_arithmetic_application_problem_2011",
    "relation_type": "Use",
    "structure": "Dataset.Architecture",
    "detail": "Ma et al. (2010) used the Elementary school arithmetic application problem dataset for evaluating their frame-based calculus method.",
    "problem_addressed": "Evaluation of multi-step arithmetic word problem solving methods",
    "evidence": "The targeting classification dataset were provided by Elementary school arithmetic application problem, People's Education Press, 2011 (Ma et al., 2010).",
    "confidence": 0.95
  },
  {
    "from_entity": "Yu2016_ImplicitQuantityRelations",
    "to_entity": "Elementary_school_arithmetic_application_problem_2011",
    "relation_type": "Use",
    "structure": "Dataset.Methodology",
    "detail": "Yu et al. (2016) used the Elementary school arithmetic application problem dataset for evaluating their method of extracting implicit quantity relations.",
    "problem_addressed": "Evaluation of implicit quantity relation extraction methods",
    "evidence": "The word problems are gathered from four publishers in Chinese(one book from People’s Education Press, one book from Beijing Normal University Press, and two books from DONGBEI Normal University Press).",
    "confidence": 0.94
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "NewsArticles_2013",
    "relation_type": "Use",
    "structure": "Dataset.Architecture",
    "detail": "Mikolov et al. (2013) used the News Articles dataset for training the Skip-gram model.",
    "problem_addressed": "Training high-quality word and phrase representations",
    "evidence": "For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words).",
    "confidence": 0.97
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Syntactic_Analogy",
    "relation_type": "Evaluate",
    "structure": "Metric.Architecture",
    "detail": "The Skip-gram model was evaluated on syntactic analogy tasks.",
    "problem_addressed": "Evaluation of syntactic relationship capturing in word representations",
    "evidence": "The performance of various Skip-gram models on the word analogy test set is reported in Table 1. The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical reasoning task.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Semantic_Analogy",
    "relation_type": "Evaluate",
    "structure": "Metric.Architecture",
    "detail": "The Skip-gram model was evaluated on semantic analogy tasks.",
    "problem_addressed": "Evaluation of semantic relationship capturing in word representations",
    "evidence": "The performance of various Skip-gram models on the word analogy test set is reported in Table 1. The table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical reasoning task.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_NegativeSampling",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model extends its training methodology by incorporating Negative Sampling.",
    "problem_addressed": "Inefficiency in training large-scale word representation models",
    "evidence": "An alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE) which was introduced by Gutmann and Hyvarinen (2012) and applied to language modeling by Mnih and Teh (2012). We simplified NCE as Negative Sampling (NEG) for faster training and better vector representations.",
    "confidence": 0.94
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_HierarchicalSoftmax",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model extends its training methodology by incorporating Hierarchical Softmax.",
    "problem_addressed": "Inefficiency in training large-scale word representation models",
    "evidence": "A computationally efficient approximation of the full softmax is the hierarchical softmax. In the context of neural network language models, it was first introduced by Morin and Bengio (2005).",
    "confidence": 0.94
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_Subsampling",
    "relation_type": "Extend",
    "structure": "Methodology.Mechanism",
    "detail": "The Skip-gram model extends its training methodology by incorporating Subsampling of frequent words.",
    "problem_addressed": "Imbalance between frequent and rare words in large corpora",
    "evidence": "To counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word wi in the training set is discarded with probability computed by the formula P(wi)= 1− √ t f(wi).",
    "confidence": 0.96
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Methodology.Property",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes precise analogical reasoning possible.",
    "confidence": 0.93
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_PhraseSkipGram",
    "relation_type": "Extend",
    "structure": "Architecture.Mechanism",
    "detail": "The Skip-gram model was extended to handle phrases.",
    "problem_addressed": "Inability to represent idiomatic phrases",
    "evidence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
    "confidence": 0.95
  },
  {
    "from_entity": "Mikolov2013_SkipGram",
    "to_entity": "Mikolov2013_AdditiveCompositionality",
    "relation_type": "Discover",
    "structure": "Property.Methodology",
    "detail": "The Skip-gram model discovered the additive compositionality property of word vectors.",
    "problem_addressed": "Understanding the linear structure of word vectors",
    "evidence": "We demonstrated that the word and phrase representations learned by the Skip-gram model exhibit linear structure that makes