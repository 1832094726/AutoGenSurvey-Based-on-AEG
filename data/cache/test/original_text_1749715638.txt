```json
{
  "entities": [
    {
      "algorithm_entity": {
        "algorithm_id": "Cho2014_RNNEncoderDecoder",
        "entity_type": "Algorithm",
        "name": "RNN Encoder-Decoder",
        "year": 2014,
        "authors": ["Cho, K.", "van Merriënboer, B.", "Gulcehre, C.", "Bahdanau, D.", "Bougares, F.", "Schwenk, H.", "Bengio, Y."],
        "task": "Phrase-based Statistical Machine Translation",
        "dataset": ["WMT'14 English-French"],
        "metrics": ["BLEU Score", "Perplexity"],
        "architecture": {
          "components": ["Recurrent Neural Network (RNN) Encoder", "RNN Decoder", "Hidden Unit with Reset and Update Gates"],
          "connections": ["Sequence-to-Sequence Mapping", "Fixed-Length Vector Representation"],
          "mechanisms": ["Conditional Probability Maximization", "Adaptive Memory and Forget Control"]
        },
        "methodology": {
          "training_strategy": ["Gradient-Based Optimization", "Beam Search Inference"],
          "parameter_tuning": ["Weight Initialization with White Gaussian Distribution", "AdaDelta", "Stochastic Gradient Descent"]
        },
        "feature_processing": ["Continuous Space Representation", "Word Embedding", "Deep Neural Network Layers"]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "WMT'14_English-French_2014",
        "entity_type": "Dataset",
        "name": "WMT'14 English-French",
        "description": "Workshop on Machine Translation 2014 dataset for English-French translation",
        "domain": "Machine Translation",
        "size": 514,
        "year": 2014,
        "creators": ["Workshop on Machine Translation"]
      }
    },
    {
      "metric_entity": {
        "metric_id": "BLEU_Score_Translation",
        "entity_type": "Metric",
        "name": "BLEU Score",
        "description": "Bilingual Evaluation Understudy Score for evaluating translation quality",
        "category": "Translation Evaluation",
        "formula": "BLEU = BP * exp(Σw_i * log p_i)"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Perplexity_LanguageModel",
        "entity_type": "Metric",
        "name": "Perplexity",
        "description": "Measure of how well a probability distribution or probability model predicts a sample",
        "category": "Language Model Evaluation",
        "formula": "Perplexity = exp(-1/N * Σ log p(x_i))"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Goldwasser2014_LearningFromNaturalInstructions",
        "entity_type": "Algorithm",
        "name": "Learning from Natural Instructions",
        "year": 2014,
        "authors": ["Goldwasser, D.", "Roth, D."],
        "task": "Interpreting Natural Language Instructions",
        "dataset": ["Freecell Solitaire Card Game Rules", "Geoquery"],
        "metrics": ["Accuracy", "F1 Score"],
        "architecture": {
          "components": ["Semantic Interpreter", "Feedback Mechanism", "Integer Linear Programming (ILP)"],
          "connections": ["Input Sentence to Logical Formula", "Logical Formula to Game Actions"],
          "mechanisms": ["Behavioral Feedback Learning", "Loss Approximation"]
        },
        "methodology": {
          "training_strategy": ["Response-Driven Learning", "Binary Feedback"],
          "parameter_tuning": ["Initial Model Bootstrapping", "Loss Approximation"]
        },
        "feature_processing": ["Lexical Similarity Features", "Syntactic Dependency Features", "External Knowledge Resources"]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Geoquery_1996",
        "entity_type": "Dataset",
        "name": "Geoquery",
        "description": "Database of U.S. geographical information and natural language queries",
        "domain": "Geographical Information Retrieval",
        "size": 500,
        "year": 1996,
        "creators": ["Zelle, J.", "Mooney, R."]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Accuracy_Classification",
        "entity_type": "Metric",
        "name": "Accuracy",
        "description": "Proportion of correctly classified instances",
        "category": "Classification Evaluation",
        "formula": "Accuracy = TP + TN / TP + TN + FP + FN"
      }
    },
    {
      "metric_entity": {
        "metric_id": "F1_Score_Classification",
        "entity_type": "Metric",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Classification Evaluation",
        "formula": "F1 = 2 * (Precision * Recall) / (Precision + Recall)"
      }
    },
    {
      "algorithm_entity": {
        "algorithm_id": "Kushman2014_AlgebraWordProblemSolver",
        "entity_type": "Algorithm",
        "name": "Algebra Word Problem Solver",
        "year": 2014,
        "authors": ["Kushman, N.", "Artzi, Y.", "Zettlemoyer, L.", "Barzilay, R."],
        "task": "Automatically Solving Algebra Word Problems",
        "dataset": ["Algebra.com"],
        "metrics": ["Equation Accuracy", "Answer Accuracy"],
        "architecture": {
          "components": ["Equation Templates", "Log-Linear Model", "Beam Search Inference"],
          "connections": ["Text Alignment to Equations", "Equation System Construction"],
          "mechanisms": ["Canonicalization", "Dependency Parsing", "Mathematical Solver Integration"]
        },
        "methodology": {
          "training_strategy": ["Semi-Supervised Learning", "Supervised Learning"],
          "parameter_tuning": ["L-BFGS Optimization", "L2 Regularization"]
        },
        "feature_processing": ["Document-Level Features", "Single Slot Features", "Slot Pair Features", "Solution Features"]
      }
    },
    {
      "dataset_entity": {
        "dataset_id": "Algebra.com_2014",
        "entity_type": "Dataset",
        "name": "Algebra.com",
        "description": "Crowdsourced tutoring website dataset of algebra word problems",
        "domain": "Algebra Problem Solving",
        "size": 514,
        "year": 2014,
        "creators": ["Algebra.com Community"]
      }
    },
    {
      "metric_entity": {
        "metric_id": "Equation_Accuracy",
        "entity_type": "Metric",
        "name": "Equation Accuracy",
        "description": "Accuracy of generated equation systems",
        "category": "Algebra Word Problem Solving Evaluation",
        "formula": "Proportion of correctly generated equation systems"
      }
    },
    {
      "metric_entity": {
        "metric_id": "Answer_Accuracy",
        "entity_type": "Metric",
        "name": "Answer Accuracy",
        "description": "Accuracy of generated numerical answers",
        "category": "Algebra Word Problem Solving Evaluation",
        "formula": "Proportion of correctly generated numerical answers"
      }
    }
  ],
  "extraction_info": {
    "is_complete": false,
    "current_section": "EXPERIMENTAL SETUP",
    "next_section": "RESULTS"
  }
}
```