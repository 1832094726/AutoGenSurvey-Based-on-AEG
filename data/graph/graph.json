{
  "nodes": [
    {
      "id": "ALGES2015_EquationTree",
      "label": "ALGES",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "ALGES2015_EquationTree",
        "entity_id": "ALGES2015_EquationTree",
        "name": "ALGES",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": "2015",
        "authors": [
          "Koncel-Kedziorski",
          "Hajishirzi",
          "Sabharwal",
          "Etzioni",
          "Ang"
        ],
        "task": "[\"Algebraic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Tree",
            "Integer Linear Programming"
          ],
          "connections": [
            "Syntactic Validity Constraints"
          ],
          "mechanisms": [
            "Brute-Force Enumeration"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Validation"
          ],
          "parameter_tuning": [
            "ILP Parameters"
          ]
        },
        "feature_processing": [
          "Quantity Extraction",
          "Proposition Transformation"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Alvin2014_ProblemGenerationAlgorithm",
      "label": "Problem Generation Algorithm",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Alvin2014_ProblemGenerationAlgorithm",
        "entity_id": "Alvin2014_ProblemGenerationAlgorithm",
        "name": "Problem Generation Algorithm",
        "title": "Synthesis of Geometry Proof Problems",
        "year": "2014",
        "authors": [
          "Alvin, C.",
          "Gulwani, S.",
          "Majumdar, R.",
          "Mukhopadhyay, S."
        ],
        "task": "[\"Geometry Proof Problem Generation\"]",
        "dataset": [],
        "metrics": [
          "Number of Generated Problems",
          "Time Taken to Generate Problems"
        ],
        "architecture": {
          "components": [
            "Hypergraph Construction",
            "Minimal Assumption Generation",
            "Strictly Interesting Problem Synthesis"
          ],
          "connections": [
            "Derive Function",
            "Choose Operator"
          ],
          "mechanisms": [
            "First-order Logic",
            "Horn Clauses"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Hypergraph Reachability",
            "Fixed-point Procedure"
          ],
          "parameter_tuning": [
            "Minimal Sets Enumeration",
            "Strictly Interesting Problem Synthesis"
          ]
        },
        "feature_processing": [
          "Implicit Facts Extraction",
          "Explicit Facts Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Alvin2015_GeoTutor",
      "label": "GeoTutor",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Alvin2015_GeoTutor",
        "entity_id": "Alvin2015_GeoTutor",
        "name": "GeoTutor",
        "title": "Automatic Synthesis of Geometry Problems for an Intelligent Tutoring System",
        "year": "2015",
        "authors": [
          "Chris Alvin",
          "Sumit Gulwani",
          "Rupak Majumdar",
          "Supratik Mukhopadhyay"
        ],
        "task": "[\"Euclidean Geometry Problem Synthesis\"]",
        "dataset": [],
        "metrics": [
          "Proof Width",
          "Proof Length",
          "Deductive Steps"
        ],
        "architecture": {
          "components": [
            "Hypergraph Construction",
            "Pebbling Algorithm"
          ],
          "connections": [
            "Forward Edges",
            "Back-Edges"
          ],
          "mechanisms": [
            "Traversal Algorithm",
            "Coarse Problem Homomorphism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Breadth-First Pebbling",
            "Coordinate-Based Computation"
          ],
          "parameter_tuning": [
            "User Query Restrictions"
          ]
        },
        "feature_processing": [
          "Assumption Filtering",
          "Invariant Characteristics"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Alvin2017_GeoShader",
      "label": "GeoShader",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Alvin2017_GeoShader",
        "entity_id": "Alvin2017_GeoShader",
        "name": "GeoShader",
        "title": "Synthesis of Problems for Shaded Area Geometry Reasoning",
        "year": "2017",
        "authors": [
          "Alvin",
          "Gulwani",
          "Majumdar",
          "Mukhopadhyay"
        ],
        "task": "[\"Geometry Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Hypergraph",
            "Intermediate Facts"
          ],
          "connections": [
            "Deduction Paths"
          ],
          "mechanisms": [
            "Analysis Hypergraph"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Regularization Parameters"
          ]
        },
        "feature_processing": [
          "Diagram Parsing",
          "Fact Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Antol2015_VQA",
      "label": "VQA",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Antol2015_VQA",
        "entity_id": "Antol2015_VQA",
        "name": "VQA",
        "title": "VQA: Visual Question Answering",
        "year": "2015",
        "authors": [
          "Stanislaw Antol",
          "Aishwarya Agrawal",
          "Jiasen Lu",
          "Margaret Mitchell",
          "Dhruv Batra",
          "C. Lawrence Zitnick",
          "Devi Parikh"
        ],
        "task": "[\"Visual Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy_Open-Answer",
          "Accuracy_Multiple-Choice"
        ],
        "architecture": {
          "components": [
            "Multi-Layer Perceptron",
            "LSTM"
          ],
          "connections": [
            "Element-wise Multiplication"
          ],
          "mechanisms": [
            "Dropout",
            "Softmax"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Bag-of-Words",
            "Image Features"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Batch Size"
          ]
        },
        "feature_processing": [
          "Bag-of-Words",
          "Image Feature Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Bakman2023_ROBUST",
      "label": "ROBUST",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Bakman2023_ROBUST",
        "entity_id": "Bakman2023_ROBUST",
        "name": "ROBUST",
        "title": "ROBUST UNDERSTANDING OF WORD PROBLEMS WITH EXTRANEOUS INFORMATION",
        "year": "2023",
        "authors": [
          "Bakman, Y."
        ],
        "task": "[\"Understanding Arithmetic Word Problems with Extraneous Information\"]",
        "dataset": [],
        "metrics": [
          "Correct Solution Rate"
        ],
        "architecture": {
          "components": [
            "Change Schema Recognition",
            "Formula Instantiation",
            "Schema Instantiation"
          ],
          "connections": [
            "Change Verbs Categorization",
            "Natural Language Parsing"
          ],
          "mechanisms": [
            "Cautious Strategy for Schema Instantiation Creation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "None (Rule-Based System)"
          ],
          "parameter_tuning": [
            "None (Rule-Based System)"
          ]
        },
        "feature_processing": [
          "Parsing Natural Language Sentences",
          "Identifying Change Verbs"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "BasicSim",
      "label": "BasicSim",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "BasicSim",
        "entity_id": "BasicSim",
        "name": "BasicSim",
        "title": "Simple Statistical Method for Math Word Problem Solving",
        "year": "2015",
        "authors": [
          "Not Explicitly Mentioned"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Similarity Calculation"
          ],
          "connections": [
            "Training Set Problems"
          ],
          "mechanisms": [
            "Statistical Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Validation"
          ],
          "parameter_tuning": [
            "None Mentioned"
          ]
        },
        "feature_processing": [
          "Problem Similarity"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Bengio2003_NeuralProbabilisticLanguageModel",
      "label": "Neural Probabilistic Language Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Bengio2003_NeuralProbabilisticLanguageModel",
        "entity_id": "Bengio2003_NeuralProbabilisticLanguageModel",
        "name": "Neural Probabilistic Language Model",
        "title": "A Neural Probabilistic Language Model",
        "year": "2003",
        "authors": [
          "Yoshua Bengio",
          "Rejean Ducharme",
          "Pascal Vincent",
          "Christian Jauvin"
        ],
        "task": "[\"Language Modeling\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Bengtson2008_PairwiseCoreferenceModel",
      "label": "Pairwise Coreference Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Bengtson2008_PairwiseCoreferenceModel",
        "entity_id": "Bengtson2008_PairwiseCoreferenceModel",
        "name": "Pairwise Coreference Model",
        "title": "Understanding the Value of Features for Coreference Resolution",
        "year": "2008",
        "authors": [
          "Eric Bengtson",
          "Dan Roth"
        ],
        "task": "[\"Coreference Resolution\"]",
        "dataset": [],
        "metrics": [
          "B-Cubed F-Score",
          "MUC F-Score"
        ],
        "architecture": {
          "components": [
            "Pairwise Coreference Function",
            "Document-Level Decision Model"
          ],
          "connections": [
            "Best-Link decision model",
            "Pairwise coreference function"
          ],
          "mechanisms": [
            "Averaged Perceptron Learning Algorithm"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Regularized Average Perceptron",
            "Threshold Optimization"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Regularization Parameter"
          ]
        },
        "feature_processing": [
          "Mention Types",
          "String Relation Features",
          "Semantic Features",
          "Relative Location Features",
          "Learned Features",
          "Aligned Modifiers",
          "Memorization Features",
          "Predicted Entity Types"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Blei2003_LatentDirichletAllocation",
      "label": "Latent Dirichlet Allocation (LDA)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Blei2003_LatentDirichletAllocation",
        "entity_id": "Blei2003_LatentDirichletAllocation",
        "name": "Latent Dirichlet Allocation (LDA)",
        "title": "Latent Dirichlet Allocation",
        "year": "2003",
        "authors": [
          "David M Blei",
          "Andrew Y Ng",
          "Michael I Jordan"
        ],
        "task": "[\"Topic Modeling\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Bulko1988_BEATRIX",
      "label": "BEATRIX",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Bulko1988_BEATRIX",
        "entity_id": "Bulko1988_BEATRIX",
        "name": "BEATRIX",
        "title": "Understanding Text With an Accompanying Diagram",
        "year": "1988",
        "authors": [
          "William C. Bulko"
        ],
        "task": "[\"Physics Problem Solving\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Graphic Interface",
            "Blackboard System",
            "Knowledge Sources"
          ],
          "connections": [
            "Coreference Resolution",
            "Parsing"
          ],
          "mechanisms": [
            "Blackboard Control Structure",
            "Opportunistic Control"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Text Parsing",
          "Picture Element Identification",
          "Touch Relation Analysis"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "CASS2018_ReinforcementLearningSolver",
      "label": "CASS",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "CASS2018_ReinforcementLearningSolver",
        "entity_id": "CASS2018_ReinforcementLearningSolver",
        "name": "CASS",
        "title": "Neural Math Word Problem Solver with Reinforcement Learning",
        "year": "2018",
        "authors": [
          "Huang",
          "Liu",
          "Lin",
          "Yin"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Copy Mechanism",
            "Alignment Mechanism"
          ],
          "connections": [
            "Policy Gradient"
          ],
          "mechanisms": [
            "Reinforcement Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Policy Gradient"
          ],
          "parameter_tuning": [
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Quantity Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "CeruttiDavis1969_FORMAC",
      "label": "FORMAC",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "CeruttiDavis1969_FORMAC",
        "entity_id": "CeruttiDavis1969_FORMAC",
        "name": "FORMAC",
        "title": "",
        "year": "1969",
        "authors": [
          "Cerutti",
          "Davis"
        ],
        "task": "[\"Elementary Analytic Geometry Theorem Proving\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Symbolic Manipulation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Assign Coordinates to Points"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "label": "Constrained Latent Left Linking Model (CL3M)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chang2013_ConstrainedLatentLeftLinkingModel",
        "entity_id": "Chang2013_ConstrainedLatentLeftLinkingModel",
        "name": "Constrained Latent Left Linking Model (CL3M)",
        "title": "A Constrained Latent Variable Model for Coreference Resolution",
        "year": "2013",
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": "[\"Coreference Resolution\"]",
        "dataset": [],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Pairwise Mention Scorer",
            "Left-Linking Model",
            "Domain Knowledge-Based Constraints"
          ],
          "connections": [
            "Best-Left-Link"
          ],
          "mechanisms": [
            "Latent Structural SVM",
            "Stochastic Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Approach",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Threshold Tuning",
            "Temperature Parameter Tuning"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Compatibility Score"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chang2013_L3M",
      "label": "Latent Left Linking model (L3M)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chang2013_L3M",
        "entity_id": "Chang2013_L3M",
        "name": "Latent Left Linking model (L3M)",
        "title": "A Constrained Latent Variable Model for Coreference Resolution",
        "year": "2013",
        "authors": [
          "Chang, K.-W.",
          "Samdani, R.",
          "Roth, D."
        ],
        "task": "[\"Coreference Resolution\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Latent Variables",
            "Stochastic Gradient"
          ],
          "connections": [
            "Knowledge-based Constraints"
          ],
          "mechanisms": [
            "Efficient Inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chang2013_LatentLeftLinkingModel",
      "label": "Latent Left Linking Model (L3M)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chang2013_LatentLeftLinkingModel",
        "entity_id": "Chang2013_LatentLeftLinkingModel",
        "name": "Latent Left Linking Model (L3M)",
        "title": "A Constrained Latent Variable Model for Coreference Resolution",
        "year": "2013",
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": "[\"Coreference Resolution\"]",
        "dataset": [],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Pairwise Mention Scorer",
            "Left-Linking Model"
          ],
          "connections": [
            "Best-Left-Link"
          ],
          "mechanisms": [
            "Latent Structural SVM",
            "Stochastic Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Approach",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Threshold Tuning",
            "Temperature Parameter Tuning"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Compatibility Score"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "label": "Probabilistic Latent Left Linking Model (PL3M)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chang2013_ProbabilisticLatentLeftLinkingModel",
        "entity_id": "Chang2013_ProbabilisticLatentLeftLinkingModel",
        "name": "Probabilistic Latent Left Linking Model (PL3M)",
        "title": "A Constrained Latent Variable Model for Coreference Resolution",
        "year": "2013",
        "authors": [
          "Kai-Wei Chang",
          "Rajhans Samdani",
          "Dan Roth"
        ],
        "task": "[\"Coreference Resolution\"]",
        "dataset": [],
        "metrics": [
          "MUC",
          "BCUB",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Pairwise Mention Scorer",
            "Left-Linking Model",
            "Probabilistic Model"
          ],
          "connections": [
            "Best-Left-Link"
          ],
          "mechanisms": [
            "Latent Structural SVM",
            "Stochastic Gradient Descent"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Likelihood-Based Approach",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Threshold Tuning",
            "Temperature Parameter Tuning"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Pairwise Compatibility Score"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Charniak2000_MaximumEntropyParser",
      "label": "Maximum-Entropy Parser",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Charniak2000_MaximumEntropyParser",
        "entity_id": "Charniak2000_MaximumEntropyParser",
        "name": "Maximum-Entropy Parser",
        "title": "A maximum-entropy-inspired parser",
        "year": "2000",
        "authors": [
          "Eugene Charniak"
        ],
        "task": "[\"Parsing\"]",
        "dataset": [],
        "metrics": [
          "Parsing Accuracy"
        ],
        "architecture": {
          "components": [
            "Maximum Entropy Model"
          ],
          "connections": [
            "Feature Functions"
          ],
          "mechanisms": [
            "Log-linear Model"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Entropy Estimation"
          ],
          "parameter_tuning": [
            "Feature Selection"
          ]
        },
        "feature_processing": [
          "Feature Engineering"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chen2014_NeuralDependencyParser",
      "label": "Neural Dependency Parser",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chen2014_NeuralDependencyParser",
        "entity_id": "Chen2014_NeuralDependencyParser",
        "name": "Neural Dependency Parser",
        "title": "A Fast and Accurate Dependency Parser using Neural Networks",
        "year": "2014",
        "authors": [
          "Danqi Chen",
          "Christopher D. Manning"
        ],
        "task": "[\"Dependency Parsing\"]",
        "dataset": [],
        "metrics": [
          "Unlabeled Attachment Score",
          "Labeled Attachment Score"
        ],
        "architecture": {
          "components": [
            "Neural network classifier"
          ],
          "connections": [],
          "mechanisms": [
            "Greedy transition-based parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Feature learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Sparse indicator features"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chen2014_NeuralNetworkParser",
      "label": "Neural Network Parser",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chen2014_NeuralNetworkParser",
        "entity_id": "Chen2014_NeuralNetworkParser",
        "name": "Neural Network Parser",
        "title": "A Fast and Accurate Dependency Parser using Neural Networks",
        "year": "2014",
        "authors": [
          "Danqi Chen",
          "Christopher D. Manning"
        ],
        "task": "[\"Dependency Parsing\"]",
        "dataset": [],
        "metrics": [
          "Unlabeled Attachment Score (UAS)",
          "Labeled Attachment Score (LAS)"
        ],
        "architecture": {
          "components": [
            "Word Embeddings",
            "POS Tag Embeddings",
            "Arc Label Embeddings",
            "Hidden Layer",
            "Softmax Layer"
          ],
          "connections": [
            "Cube Activation Function"
          ],
          "mechanisms": [
            "Dense Representations",
            "Transition-based Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Entropy Loss",
            "L2 Regularization",
            "Mini-batched AdaGrad",
            "Dropout"
          ],
          "parameter_tuning": [
            "Pre-trained Word Embeddings",
            "Random Initialization"
          ]
        },
        "feature_processing": [
          "Word Embeddings",
          "POS Tag Embeddings",
          "Arc Label Embeddings"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chen2014_NeuralParser",
      "label": "Neural Dependency Parser",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chen2014_NeuralParser",
        "entity_id": "Chen2014_NeuralParser",
        "name": "Neural Dependency Parser",
        "title": "A Fast and Accurate Dependency Parser using Neural Networks",
        "year": "2014",
        "authors": [
          "Chen, D.",
          "Manning, C.D."
        ],
        "task": "[\"Dependency Parsing\"]",
        "dataset": [],
        "metrics": [
          "Unlabeled Attachment Score",
          "Labeled Attachment Score"
        ],
        "architecture": {
          "components": [
            "Neural Network Classifier"
          ],
          "connections": [],
          "mechanisms": [
            "Greedy Transition-based Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Dense Feature Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Chiang2018_SemanticallyAligned",
      "label": "Semantically-Aligned Equation Generation",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chiang2018_SemanticallyAligned",
        "entity_id": "Chiang2018_SemanticallyAligned",
        "name": "Semantically-Aligned Equation Generation",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": "2018",
        "authors": [
          "Chiang",
          "Chen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "LSTM",
            "Recursive Neural Networks"
          ],
          "connections": [
            "Self-Attention"
          ],
          "mechanisms": [
            "Recursive Inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "Quantity Representation",
          "Recursive Inference"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Chiang2018_SemanticallyAlignedEquationGeneration",
      "label": "Semantically-Aligned Equation Generation",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chiang2018_SemanticallyAlignedEquationGeneration",
        "entity_id": "Chiang2018_SemanticallyAlignedEquationGeneration",
        "name": "Semantically-Aligned Equation Generation",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": "2018",
        "authors": [
          "T. Chiang",
          "Y. Chen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "Long Short-Term Memory"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss"
          ],
          "parameter_tuning": [
            "Adam"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Dependency Parsing"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Chiang2018_StackDecoder",
      "label": "StackDecoder",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chiang2018_StackDecoder",
        "entity_id": "Chiang2018_StackDecoder",
        "name": "StackDecoder",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": "2018",
        "authors": [
          "Chiang",
          "Chen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Stack"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "Gated Recurrent Unit"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Stopword Removal"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Chiang2019_SemanticallyAlignedEquationGenerator",
      "label": "Semantically-Aligned Equation Generator",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Chiang2019_SemanticallyAlignedEquationGenerator",
        "entity_id": "Chiang2019_SemanticallyAlignedEquationGenerator",
        "name": "Semantically-Aligned Equation Generator",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": "2019",
        "authors": [
          "Ting-Rui Chiang",
          "Yun-Nung Chen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Stack",
            "Semantic Transformer"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "BLSTM",
            "LSTM",
            "Gated Mechanisms"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Semantic Representation Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Cho2014_RNNEncoderDecoder",
      "label": "RNN Encoder–Decoder",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Cho2014_RNNEncoderDecoder",
        "entity_id": "Cho2014_RNNEncoderDecoder",
        "name": "RNN Encoder–Decoder",
        "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
        "year": "2014",
        "authors": [
          "Kyunghyun Cho",
          "Bart van Merriënboer",
          "Caglar Gulcehre",
          "Dzmitry Bahdanau",
          "Fethi Bougares",
          "Holger Schwenk",
          "Yoshua Bengio"
        ],
        "task": "[\"Statistical Machine Translation\"]",
        "dataset": [],
        "metrics": [
          "BLEU"
        ],
        "architecture": {
          "components": [
            "Encoder RNN",
            "Decoder RNN"
          ],
          "connections": [
            "Conditional Probability"
          ],
          "mechanisms": [
            "Reset Gate",
            "Update Gate"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximize Conditional Log-Likelihood",
            "Gradient-Based Algorithm"
          ],
          "parameter_tuning": [
            "Adadelta",
            "Stochastic Gradient Descent"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Phrase Pair Scoring"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Clark2016_Aristo",
      "label": "Aristo",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Clark2016_Aristo",
        "entity_id": "Clark2016_Aristo",
        "name": "Aristo",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": "2016",
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "[\"Elementary Science Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "IR Solver",
            "PMI Solver",
            "SVM Solver",
            "RULE Solver",
            "ILP Solver"
          ],
          "connections": [
            "Logistic Regression Combiner"
          ],
          "mechanisms": [
            "Information Retrieval",
            "Pointwise Mutual Information",
            "Support Vector Machine",
            "Rule-based Reasoning",
            "Integer Linear Programming"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Logistic Regression Calibration"
          ],
          "parameter_tuning": [
            "TF-IDF Scoring",
            "Window Size for PMI"
          ]
        },
        "feature_processing": [
          "Text Parsing",
          "N-gram Extraction",
          "Word Embeddings",
          "Lexical Entailment"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Clark2016_ILPSolver",
      "label": "ILP Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Clark2016_ILPSolver",
        "entity_id": "Clark2016_ILPSolver",
        "name": "ILP Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": "2016",
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "[\"Elementary Science Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Integer Linear Programming Model"
          ],
          "connections": [],
          "mechanisms": [
            "Structured Knowledge Representation",
            "Table Joins"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Table Construction",
          "Relation Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Clark2016_IRSolver",
      "label": "IR Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Clark2016_IRSolver",
        "entity_id": "Clark2016_IRSolver",
        "name": "IR Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": "2016",
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "[\"Elementary Science Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Lucene Search Engine"
          ],
          "connections": [],
          "mechanisms": [
            "Information Retrieval"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Query Construction",
          "Stopword Filtering"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Clark2016_PMISolver",
      "label": "PMI Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Clark2016_PMISolver",
        "entity_id": "Clark2016_PMISolver",
        "name": "PMI Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": "2016",
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "[\"Elementary Science Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Pointwise Mutual Information Calculation"
          ],
          "connections": [],
          "mechanisms": [
            "Statistical Association"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": [
            "Window Size for PMI"
          ]
        },
        "feature_processing": [
          "N-gram Extraction",
          "Stopword Filtering"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Clark2016_RULESolver",
      "label": "RULE Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Clark2016_RULESolver",
        "entity_id": "Clark2016_RULESolver",
        "name": "RULE Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": "2016",
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "[\"Elementary Science Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Probabilistic First-Order Logic Rules"
          ],
          "connections": [],
          "mechanisms": [
            "Rule-based Reasoning",
            "Lexical Entailment"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Text Parsing",
          "Syntactic Pattern Matching"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Clark2016_SVMSolver",
      "label": "SVM Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Clark2016_SVMSolver",
        "entity_id": "Clark2016_SVMSolver",
        "name": "SVM Solver",
        "title": "Combining Retrieval, Statistics, and Inference to Answer Elementary Science Questions",
        "year": "2016",
        "authors": [
          "Peter Clark",
          "Oren Etzioni",
          "Tushar Khot",
          "Ashish Sabharwal",
          "Oyvind Tafjord",
          "Peter Turney",
          "Daniel Khashabi"
        ],
        "task": "[\"Elementary Science Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Support Vector Machine"
          ],
          "connections": [],
          "mechanisms": [
            "Word Embeddings"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SVM Ranker"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Word Embedding Generation",
          "Cosine Similarity Calculation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Collins1999_HeadDrivenStatisticalModel",
      "label": "Head-Driven Statistical Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Collins1999_HeadDrivenStatisticalModel",
        "entity_id": "Collins1999_HeadDrivenStatisticalModel",
        "name": "Head-Driven Statistical Model",
        "title": "Head-Driven Statistical Models for Natural Language Parsing",
        "year": "1999",
        "authors": [
          "Michael Collins"
        ],
        "task": "[\"Natural Language Parsing\"]",
        "dataset": [],
        "metrics": [
          "Parsing Accuracy"
        ],
        "architecture": {
          "components": [
            "Statistical Models"
          ],
          "connections": [
            "Head Rules"
          ],
          "mechanisms": [
            "Probabilistic Context-Free Grammar"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "Parameter Estimation"
          ]
        },
        "feature_processing": [
          "Syntactic Analysis"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Collobert2008_UnifiedArchitectureForNLP",
      "label": "Unified Architecture for Natural Language Processing",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Collobert2008_UnifiedArchitectureForNLP",
        "entity_id": "Collobert2008_UnifiedArchitectureForNLP",
        "name": "Unified Architecture for Natural Language Processing",
        "title": "A Unified Architecture for Natural Language Processing",
        "year": "2008",
        "authors": [
          "Ronan Collobert",
          "Jason Weston"
        ],
        "task": "[\"Natural Language Processing\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "deMarneffe2006_TypedDependencyExtractor",
      "label": "Typed Dependency Extractor",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "deMarneffe2006_TypedDependencyExtractor",
        "entity_id": "deMarneffe2006_TypedDependencyExtractor",
        "name": "Typed Dependency Extractor",
        "title": "Generating Typed Dependency Parses from Phrase Structure Parses",
        "year": "2006",
        "authors": [
          "Marie-Catherine de Marneffe",
          "Bill MacCartney",
          "Christopher D. Manning"
        ],
        "task": "[\"Dependency Parsing\"]",
        "dataset": [],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Phrase Structure Parser",
            "Dependency Extraction Rules"
          ],
          "connections": [
            "Head Identification Rules"
          ],
          "mechanisms": [
            "Collapsing Dependencies"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Statistical Parsing"
          ],
          "parameter_tuning": [
            "Collins Head Rules"
          ]
        },
        "feature_processing": [
          "Semantic Head Retrieval",
          "Dependency Typing"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Deng2009_AdaBoostBasedClassifier",
      "label": "AdaBoost-based Classifier",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Deng2009_AdaBoostBasedClassifier",
        "entity_id": "Deng2009_AdaBoostBasedClassifier",
        "name": "AdaBoost-based Classifier",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": "2009",
        "authors": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": "[\"Object Classification\"]",
        "dataset": [],
        "metrics": [
          "AUC"
        ],
        "architecture": {
          "components": [
            "AdaBoost"
          ],
          "connections": [
            "Hierarchical Structure"
          ],
          "mechanisms": [
            "Synset Nodes"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Random Sampling"
          ],
          "parameter_tuning": [
            "AdaBoost"
          ]
        },
        "feature_processing": [
          "Image Descriptors"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Deng2009_NonParametricObjectRecognition",
      "label": "Non-Parametric Object Recognition",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Deng2009_NonParametricObjectRecognition",
        "entity_id": "Deng2009_NonParametricObjectRecognition",
        "name": "Non-Parametric Object Recognition",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": "2009",
        "authors": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": "[\"Object Recognition\"]",
        "dataset": [],
        "metrics": [
          "Accuracy",
          "Precision",
          "Recall"
        ],
        "architecture": {
          "components": [
            "Nearest Neighbor"
          ],
          "connections": [
            "Hierarchical Structure"
          ],
          "mechanisms": [
            "Synset Nodes"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Random Sampling"
          ],
          "parameter_tuning": [
            "SSD Pixel Distance"
          ]
        },
        "feature_processing": [
          "Image Descriptors"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Deng2009_TreeMaxClassifier",
      "label": "Tree-Max Classifier",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Deng2009_TreeMaxClassifier",
        "entity_id": "Deng2009_TreeMaxClassifier",
        "name": "Tree-Max Classifier",
        "title": "ImageNet: A Large-Scale Hierarchical Image Database",
        "year": "2009",
        "authors": [
          "Deng, J.",
          "Dong, W.",
          "Socher, R.",
          "Li, L.-J.",
          "Li, K.",
          "Fei-Fei, L."
        ],
        "task": "[\"Object Classification\"]",
        "dataset": [],
        "metrics": [
          "AUC"
        ],
        "architecture": {
          "components": [
            "AdaBoost-based Classifier"
          ],
          "connections": [
            "Hierarchical Structure"
          ],
          "mechanisms": [
            "Synset Nodes"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Random Sampling"
          ],
          "parameter_tuning": [
            "AdaBoost"
          ]
        },
        "feature_processing": [
          "Image Descriptors"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Deng2017_DimensionallyGuidedSynthesis",
      "label": "Dimensionally Guided Synthesis",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Deng2017_DimensionallyGuidedSynthesis",
        "entity_id": "Deng2017_DimensionallyGuidedSynthesis",
        "name": "Dimensionally Guided Synthesis",
        "title": "Dimensionally Guided Synthesis of Mathematical Word Problems",
        "year": "2017",
        "authors": [
          "Wang, K.",
          "Su, Z."
        ],
        "task": "[\"Mathematical Word Problem Generation\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Dimensionally Consistent Equation Generator",
            "Natural Language Story Composer"
          ],
          "connections": [
            "Equation Tree"
          ],
          "mechanisms": [
            "Bottom-Up Traversal"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Efficient Generation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Dimensional Units"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "DNS2017_DeepNeuralSolver",
      "label": "Deep Neural Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "DNS2017_DeepNeuralSolver",
        "entity_id": "DNS2017_DeepNeuralSolver",
        "name": "Deep Neural Solver",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": "2017",
        "authors": [
          "Wang",
          "Liu",
          "Shi"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "GRU"
          ],
          "connections": [
            "Word Embedding"
          ],
          "mechanisms": [
            "Equation Normalization"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Entropy Loss"
          ],
          "parameter_tuning": [
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Quantity Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Duda1972_HoughTransformation",
      "label": "Hough Transformation",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Duda1972_HoughTransformation",
        "entity_id": "Duda1972_HoughTransformation",
        "name": "Hough Transformation",
        "title": "Use of the Hough Transformation To Detect Lines and Curves in Pictures",
        "year": "1972",
        "authors": [
          "Richard O. Duda",
          "Peter E. Hart"
        ],
        "task": "[\"Line Detection\", \"Curve Detection\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Point-Line Transformation",
            "Parameter Space"
          ],
          "connections": [
            "Normal Parameterization"
          ],
          "mechanisms": [
            "Accumulator Array"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Quantization",
          "Thresholding"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Earley1970_EfficientParsingAlgorithm",
      "label": "Efficient Context-Free Parsing Algorithm",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Earley1970_EfficientParsingAlgorithm",
        "entity_id": "Earley1970_EfficientParsingAlgorithm",
        "name": "Efficient Context-Free Parsing Algorithm",
        "title": "An Efficient Context-Free Parsing Algorithm",
        "year": "1970",
        "authors": [
          "Jay Earley"
        ],
        "task": "[\"Syntax Analysis\", \"Parsing\"]",
        "dataset": [],
        "metrics": [
          "Time Complexity",
          "Space Complexity"
        ],
        "architecture": {
          "components": [
            "Predictor",
            "Completer",
            "Scanner"
          ],
          "connections": [
            "State Sets",
            "Look-ahead"
          ],
          "mechanisms": [
            "LR(k) Parsing",
            "Top-down Parsing",
            "Bottom-up Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Context-Free Grammars",
          "Derivation Trees"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Ferguson1994_MAGI",
      "label": "MAGI",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Ferguson1994_MAGI",
        "entity_id": "Ferguson1994_MAGI",
        "name": "MAGI",
        "title": "MAGI: Analogy-based encoding using symmetry and regularity",
        "year": "1994",
        "authors": [
          "Ronald W. Ferguson"
        ],
        "task": "[\"Symmetry Detection\", \"Analogical Encoding\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Symmetry Detector",
            "Regular Pattern Recognizer"
          ],
          "connections": [
            "Visual Relation Integration"
          ],
          "mechanisms": [
            "Symmetry Judgment",
            "Pattern Matching"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Shape Analysis",
          "Boundary Detection"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Ferguson1995_JUXTA",
      "label": "JUXTA",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Ferguson1995_JUXTA",
        "entity_id": "Ferguson1995_JUXTA",
        "name": "JUXTA",
        "title": "Understanding illustrations of physical laws by integrating differences in visual and textual representations",
        "year": "1995",
        "authors": [
          "Ronald W. Ferguson",
          "Kenneth D. Forbus"
        ],
        "task": "[\"Diagram Understanding\", \"Critiquing Simplified Diagrams\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Visual Level Representation",
            "Physical Level Representation",
            "Process Level Representation"
          ],
          "connections": [
            "Visual Operation Library",
            "Domain-Specific Rules"
          ],
          "mechanisms": [
            "Difference Detection",
            "Caption Interpretation",
            "Contextual Analysis"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Visual Relation Detection",
          "Textual Analysis"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Ferguson1999_GeoRep",
      "label": "GeoRep",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Ferguson1999_GeoRep",
        "entity_id": "Ferguson1999_GeoRep",
        "name": "GeoRep",
        "title": "GeoRep: A Flexible Tool for Spatial Representation of Line Drawings",
        "year": "1999",
        "authors": [
          "Ronald W. Ferguson",
          "Kenneth D. Forbus"
        ],
        "task": "[\"Spatial Representation\", \"Diagrammatic Reasoning\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Low-Level Relational Describer (LLRD)",
            "High-Level Relational Describer (HLRD)"
          ],
          "connections": [
            "Visual Operations Library",
            "Domain-Specific Rules"
          ],
          "mechanisms": [
            "Proximity Detection",
            "Reference Frame Relations",
            "Parallel Lines Detection",
            "Connectivity Detection",
            "Polygon and Polyline Detection",
            "Boundary Description",
            "Interval Relations",
            "Grouping"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Primitive Shape Detection",
          "Proximity Calculation",
          "Visual Relation Detection"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Fletcher1985_WORDPRO",
      "label": "WORDPRO",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Fletcher1985_WORDPRO",
        "entity_id": "Fletcher1985_WORDPRO",
        "name": "WORDPRO",
        "title": "Understanding and solving arithmetic word problems: A computer simulation",
        "year": "1985",
        "authors": [
          "Fletcher, C. R."
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Run-Time Statistics"
        ],
        "architecture": {
          "components": [
            "Production Rules",
            "Set Schema",
            "Transfer Schema",
            "Superset Schema",
            "More-Than/Less-Than Schema"
          ],
          "connections": [
            "Short-Term Memory (STM)",
            "Long-Term Memory (LTM)"
          ],
          "mechanisms": [
            "Meaning Postulates",
            "Arithmetic Strategies",
            "Problem-Solving Procedures"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Production System"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Propositional Representation",
          "Bilevel Representation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Fukui2016_MultimodalCompactBilinearPooling",
      "label": "Multimodal Compact Bilinear Pooling",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Fukui2016_MultimodalCompactBilinearPooling",
        "entity_id": "Fukui2016_MultimodalCompactBilinearPooling",
        "name": "Multimodal Compact Bilinear Pooling",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "year": "2016",
        "authors": [
          "Yash Goyal",
          "Tejas Khot",
          "Douglas Summers-Stay",
          "Dhruv Batra",
          "Devi Parikh"
        ],
        "task": "[\"Visual Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Multimodal compact bilinear pooling"
          ],
          "connections": [
            "Image features",
            "Language features"
          ],
          "mechanisms": [
            "Fully-connected layer"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Gelernter1959_GeometryMachine",
      "label": "Geometry Machine",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Gelernter1959_GeometryMachine",
        "entity_id": "Gelernter1959_GeometryMachine",
        "name": "Geometry Machine",
        "title": "",
        "year": "1959",
        "authors": [
          "Gelernter"
        ],
        "task": "[\"Geometry Theorem Proving\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Backward Chaining Search Strategy"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Specific Heuristic Knowledge about Geometry Domain"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Goldberg2010_EasyFirstNonDirectionalParser",
      "label": "Easy-First Non-Directional Dependency Parsing",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Goldberg2010_EasyFirstNonDirectionalParser",
        "entity_id": "Goldberg2010_EasyFirstNonDirectionalParser",
        "name": "Easy-First Non-Directional Dependency Parsing",
        "title": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing",
        "year": "2010",
        "authors": [
          "Yoav Goldberg",
          "Michael Elhadad"
        ],
        "task": "[\"Dependency Parsing\"]",
        "dataset": [],
        "metrics": [
          "Accuracy",
          "Root",
          "Complete"
        ],
        "architecture": {
          "components": [
            "ATTACHLEFT",
            "ATTACHRIGHT"
          ],
          "connections": [
            "Dependency Edges"
          ],
          "mechanisms": [
            "Score Function",
            "Feature Extraction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Structured Perceptron",
            "Parameter Averaging"
          ],
          "parameter_tuning": [
            "Weight Vector Updates"
          ]
        },
        "feature_processing": [
          "POS Tagging",
          "Feature Templates"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Goldwasser2013_NECO",
      "label": "NECO",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Goldwasser2013_NECO",
        "entity_id": "Goldwasser2013_NECO",
        "name": "NECO",
        "title": "Joint Coreference Resolution and Named-Entity Linking with Multi-pass Sieves",
        "year": "2013",
        "authors": [
          "Hajishirzi, H.",
          "Zilles, L.",
          "Weld, D.S.",
          "Zettlemoyer, L."
        ],
        "task": "[\"Coreference Resolution\", \"Named-Entity Linking\"]",
        "dataset": [],
        "metrics": [
          "MUC Coreference Error",
          "NEL Error"
        ],
        "architecture": {
          "components": [
            "Deterministic Coreference System",
            "Wikipedia Linking"
          ],
          "connections": [
            "NEL-informed Mention-Merging Sieves"
          ],
          "mechanisms": [
            "Improved Mention-Detection",
            "Context Modeling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Joint Training"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Goldwasser2014_BinaryPerceptron",
      "label": "Binary Perceptron",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Goldwasser2014_BinaryPerceptron",
        "entity_id": "Goldwasser2014_BinaryPerceptron",
        "name": "Binary Perceptron",
        "title": "Learning from Natural Instructions",
        "year": "2014",
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": "[\"Semantic Parsing\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Feature Function",
            "Weight Vector"
          ],
          "connections": [
            "Feature Representation"
          ],
          "mechanisms": [
            "Binary Classification"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Online Learning",
            "Error-Driven Updates"
          ],
          "parameter_tuning": [
            "Weight Vector Updates"
          ]
        },
        "feature_processing": [
          "Lexical Mapping",
          "Syntactic Information"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Goldwasser2014_CombinedFeedbackPerceptron",
      "label": "Combined Feedback Perceptron",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Goldwasser2014_CombinedFeedbackPerceptron",
        "entity_id": "Goldwasser2014_CombinedFeedbackPerceptron",
        "name": "Combined Feedback Perceptron",
        "title": "Learning from Natural Instructions",
        "year": "2014",
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": "[\"Semantic Parsing\", \"Natural Language Instruction Interpretation\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Perceptron",
            "Structured Perceptron"
          ],
          "connections": [
            "Feature Function",
            "Weight Vector"
          ],
          "mechanisms": [
            "Loss Approximation",
            "Binary Update",
            "Structural Update"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Online Learning",
            "Error-Driven Updates"
          ],
          "parameter_tuning": [
            "Weight Vector Updates"
          ]
        },
        "feature_processing": [
          "Lexical Mapping",
          "Syntactic Information"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Goldwasser2014_LossApproximation",
      "label": "Loss Approximation",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Goldwasser2014_LossApproximation",
        "entity_id": "Goldwasser2014_LossApproximation",
        "name": "Loss Approximation",
        "title": "Learning from Natural Instructions",
        "year": "2014",
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": "[\"Semantic Parsing\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Feature Function",
            "Weight Vector"
          ],
          "connections": [
            "Feature Representation"
          ],
          "mechanisms": [
            "Loss Approximation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Online Learning",
            "Error-Driven Updates"
          ],
          "parameter_tuning": [
            "Weight Vector Updates"
          ]
        },
        "feature_processing": [
          "Lexical Mapping",
          "Syntactic Information"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Goldwasser2014_StructuredPerceptron",
      "label": "Structured Perceptron",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Goldwasser2014_StructuredPerceptron",
        "entity_id": "Goldwasser2014_StructuredPerceptron",
        "name": "Structured Perceptron",
        "title": "Learning from Natural Instructions",
        "year": "2014",
        "authors": [
          "Goldwasser, D.",
          "Roth, D."
        ],
        "task": "[\"Semantic Parsing\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Feature Function",
            "Weight Vector"
          ],
          "connections": [
            "Feature Representation"
          ],
          "mechanisms": [
            "Structured Prediction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Online Learning",
            "Error-Driven Updates"
          ],
          "parameter_tuning": [
            "Weight Vector Updates"
          ]
        },
        "feature_processing": [
          "Lexical Mapping",
          "Syntactic Information"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Goyal2016_DeepLSTM",
      "label": "Deeper LSTM Question + norm Image",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Goyal2016_DeepLSTM",
        "entity_id": "Goyal2016_DeepLSTM",
        "name": "Deeper LSTM Question + norm Image",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "year": "2016",
        "authors": [
          "Yash Goyal",
          "Tejas Khot",
          "Douglas Summers-Stay",
          "Dhruv Batra",
          "Devi Parikh"
        ],
        "task": "[\"Visual Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "CNN embedding",
            "LSTM embedding",
            "Point-wise multiplication",
            "Multi-layer perceptron classifier"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Haghighi2009_SimpleCoreferenceResolution",
      "label": "Simple Coreference Resolution",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Haghighi2009_SimpleCoreferenceResolution",
        "entity_id": "Haghighi2009_SimpleCoreferenceResolution",
        "name": "Simple Coreference Resolution",
        "title": "Simple Coreference Resolution with Rich Syntactic and Semantic Features",
        "year": "2009",
        "authors": [
          "Aria Haghighi",
          "Dan Klein"
        ],
        "task": "[\"Coreference Resolution\"]",
        "dataset": [],
        "metrics": [
          "Pairwise F1",
          "b3",
          "MUC",
          "CEAF"
        ],
        "architecture": {
          "components": [
            "Syntactic Module",
            "Semantic Module",
            "Selection Module"
          ],
          "connections": [
            "Syntactic Paths",
            "Compatibility Lists"
          ],
          "mechanisms": [
            "Syntactic Constraints",
            "Semantic Compatibility",
            "Tree Distance"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Unsupervised Learning",
            "Treebank Parsing"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Syntactic Parsing",
          "Semantic Compatibility Mining",
          "Appositive Annotation",
          "Predicate Nominative Annotation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hajishirzi2013_NECO",
      "label": "NECO",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hajishirzi2013_NECO",
        "entity_id": "Hajishirzi2013_NECO",
        "name": "NECO",
        "title": "Joint Coreference Resolution and Named-Entity Linking with Multi-pass Sieves",
        "year": "2013",
        "authors": [
          "Hannaneh Hajishirzi",
          "Leila Zilles",
          "Daniel S. Weld",
          "Luke Zettlemoyer"
        ],
        "task": "[\"Coreference Resolution\", \"Named-Entity Linking\"]",
        "dataset": [],
        "metrics": [
          "MUC",
          "B3",
          "Pairwise",
          "F1"
        ],
        "architecture": {
          "components": [
            "Sieve-based model",
            "NEL-informed sieves",
            "Mention detection",
            "Cluster merging"
          ],
          "connections": [
            "Coreference rules",
            "NEL constraints"
          ],
          "mechanisms": [
            "Automatic mention detection",
            "Entity link propagation",
            "Fine-grained attributes"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Deterministic rules",
            "High precision sieves first"
          ],
          "parameter_tuning": [
            "Confidence thresholds for NEL systems"
          ]
        },
        "feature_processing": [
          "Mention pruning",
          "Attribute extraction from Freebase and Wikipedia"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hochreiter1997_BackPropagationThroughTime",
      "label": "Back-Propagation Through Time (BPTT)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hochreiter1997_BackPropagationThroughTime",
        "entity_id": "Hochreiter1997_BackPropagationThroughTime",
        "name": "Back-Propagation Through Time (BPTT)",
        "title": "Long Short-Term Memory",
        "year": "1997",
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": "[\"Sequence Modeling\"]",
        "dataset": [],
        "metrics": [
          "Error Signal Stability"
        ],
        "architecture": {
          "components": [
            "Hidden Units",
            "Output Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": [
            "Gradient-Based Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Full Backpropagation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hochreiter1997_ConstantErrorFlow",
      "label": "Constant Error Flow",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hochreiter1997_ConstantErrorFlow",
        "entity_id": "Hochreiter1997_ConstantErrorFlow",
        "name": "Constant Error Flow",
        "title": "Long Short-Term Memory",
        "year": "1997",
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": "[\"Sequence Modeling\"]",
        "dataset": [],
        "metrics": [
          "Error Signal Stability"
        ],
        "architecture": {
          "components": [
            "Single Unit with Self-Connection"
          ],
          "connections": [
            "Self-Connection"
          ],
          "mechanisms": [
            "Fixed Time Constants"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Naive Approach"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hochreiter1997_ElmanNets",
      "label": "Elman Nets",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hochreiter1997_ElmanNets",
        "entity_id": "Hochreiter1997_ElmanNets",
        "name": "Elman Nets",
        "title": "Long Short-Term Memory",
        "year": "1997",
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": "[\"Sequence Modeling\"]",
        "dataset": [],
        "metrics": [
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Hidden Units",
            "Output Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": [
            "Gradient-Based Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Elman's Training Procedure"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hochreiter1997_LongShortTermMemory",
      "label": "Long Short-Term Memory (LSTM)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hochreiter1997_LongShortTermMemory",
        "entity_id": "Hochreiter1997_LongShortTermMemory",
        "name": "Long Short-Term Memory (LSTM)",
        "title": "Long Short-Term Memory",
        "year": "1997",
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": "[\"Sequence Modeling\", \"Long Time Lag Problems\"]",
        "dataset": [],
        "metrics": [
          "Success Rate",
          "Training Time"
        ],
        "architecture": {
          "components": [
            "Memory Cells",
            "Input Gates",
            "Output Gates",
            "Constant Error Carousels (CECs)"
          ],
          "connections": [
            "Fully Connected Hidden Layer",
            "Self-Connections"
          ],
          "mechanisms": [
            "Multiplicative Gates",
            "Truncated Backpropagation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Truncated Backpropagation Through Time",
            "Real-Time Recurrent Learning (RTRL)"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Bias Initialization"
          ]
        },
        "feature_processing": [
          "Local Input/Output Representations",
          "Distributed Representations"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hochreiter1997_NeuralSequenceChunker",
      "label": "Neural Sequence Chunker",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hochreiter1997_NeuralSequenceChunker",
        "entity_id": "Hochreiter1997_NeuralSequenceChunker",
        "name": "Neural Sequence Chunker",
        "title": "Long Short-Term Memory",
        "year": "1997",
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": "[\"Sequence Modeling\"]",
        "dataset": [],
        "metrics": [
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Two Networks"
          ],
          "connections": [
            "Inter-Network Connections"
          ],
          "mechanisms": [
            "Chunking"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Chunking"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hochreiter1997_RealTimeRecurrentLearning",
      "label": "Real-Time Recurrent Learning (RTRL)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hochreiter1997_RealTimeRecurrentLearning",
        "entity_id": "Hochreiter1997_RealTimeRecurrentLearning",
        "name": "Real-Time Recurrent Learning (RTRL)",
        "title": "Long Short-Term Memory",
        "year": "1997",
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": "[\"Sequence Modeling\"]",
        "dataset": [],
        "metrics": [
          "Error Signal Stability"
        ],
        "architecture": {
          "components": [
            "Hidden Units",
            "Output Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": [
            "Gradient-Based Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Online Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hochreiter1997_RecurrentCascadeCorrelation",
      "label": "Recurrent Cascade-Correlation",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hochreiter1997_RecurrentCascadeCorrelation",
        "entity_id": "Hochreiter1997_RecurrentCascadeCorrelation",
        "name": "Recurrent Cascade-Correlation",
        "title": "Long Short-Term Memory",
        "year": "1997",
        "authors": [
          "Sepp Hochreiter",
          "Jürgen Schmidhuber"
        ],
        "task": "[\"Sequence Modeling\"]",
        "dataset": [],
        "metrics": [
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Hidden Units",
            "Output Units"
          ],
          "connections": [
            "Fully Connected"
          ],
          "mechanisms": [
            "Incremental Network Construction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Sequential Network Construction"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hosseini2014_ARIS",
      "label": "ARIS",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hosseini2014_ARIS",
        "entity_id": "Hosseini2014_ARIS",
        "name": "ARIS",
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "year": "2014",
        "authors": [
          "Mohammad Javad Hosseini",
          "Hannaneh Hajishirzi",
          "Oren Etzioni",
          "Nate Kushman"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy_Classification"
        ],
        "architecture": {
          "components": [
            "Entity Recognition",
            "Container Identification",
            "Verb Categorization",
            "State Transition Modeling",
            "Equation Formation"
          ],
          "connections": [
            "Dependency Parsing",
            "Coreference Resolution"
          ],
          "mechanisms": [
            "Support Vector Machines",
            "Circumscription Assumption"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-validation",
            "Feature Extraction"
          ],
          "parameter_tuning": [
            "Regularization",
            "Similarity-based Feature Selection"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Coreference Resolution",
          "Named Entity Recognition"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Hosseini2014_VerbCategorization",
      "label": "Verb Categorization",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Hosseini2014_VerbCategorization",
        "entity_id": "Hosseini2014_VerbCategorization",
        "name": "Verb Categorization",
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "year": "2014",
        "authors": [
          "Hosseini, M. J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Verb Categories",
            "Semantic Parsing"
          ],
          "connections": [
            "Rule-Based Filtering"
          ],
          "mechanisms": [
            "Dependency Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Dependency Parsing",
          "Rule-Based Filtering"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2012_MultiSenseWordEmbedding",
      "label": "Multi-Sense Word Embedding",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2012_MultiSenseWordEmbedding",
        "entity_id": "Huang2012_MultiSenseWordEmbedding",
        "name": "Multi-Sense Word Embedding",
        "title": "Improving Word Representations via Global Context and Multiple Word Prototypes",
        "year": "2012",
        "authors": [
          "Eric H Huang",
          "Richard Socher",
          "Christopher D Manning",
          "Andrew Y Ng"
        ],
        "task": "[\"Word Representation\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2016_LargeScaleDataset",
      "label": "Ranking SVM Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2016_LargeScaleDataset",
        "entity_id": "Huang2016_LargeScaleDataset",
        "name": "Ranking SVM Model",
        "title": "How Well Do Computers Solve Math Word Problems?",
        "year": "2016",
        "authors": [
          "Huang, D.",
          "Shi, S.",
          "Lin, C.Y.",
          "Yin, J.",
          "Ma, W.Y."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Precision",
          "Recall"
        ],
        "architecture": {
          "components": [
            "Ranking SVM"
          ],
          "connections": [
            "Equation System Extraction"
          ],
          "mechanisms": [
            "Gold Answer Extraction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2016_SIM",
      "label": "SIM",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2016_SIM",
        "entity_id": "Huang2016_SIM",
        "name": "SIM",
        "title": "How Well Do Computers Solve Math Word Problems?",
        "year": "2016",
        "authors": [
          "Huang",
          "Shi",
          "Lin",
          "Yin",
          "Ma"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Similarity-based Method"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Lexical Similarity Calculation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "TF-IDF",
          "Weighted Jaccard Similarity"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2016_Solver",
      "label": "Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2016_Solver",
        "entity_id": "Huang2016_Solver",
        "name": "Solver",
        "title": "How Well Do Computers Solve Math Word Problems?",
        "year": "2016",
        "authors": [
          "Danqing Huang",
          "Shuming Shi",
          "Chin-Yew Lin",
          "Jian Yin",
          "Wei-Ying Ma"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Precision",
          "Recall"
        ],
        "architecture": {
          "components": [
            "Ranking SVM model"
          ],
          "connections": [
            "Answer extraction"
          ],
          "mechanisms": [
            "Gold answers extraction",
            "Equation systems extraction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Logistic regression classifier"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Post classification",
          "Problem cleaning",
          "Gold answers extraction",
          "Equation systems extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2017_FeatureBasedModel",
      "label": "Feature-Based Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2017_FeatureBasedModel",
        "entity_id": "Huang2017_FeatureBasedModel",
        "name": "Feature-Based Model",
        "title": "Learning Fine-Grained Expressions to Solve Math Word Problems",
        "year": "2017",
        "authors": [
          "Danqing Huang",
          "Shuming Shi",
          "Chin-Yew Lin",
          "Jian Yin"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Template Retrieval",
            "Equation Ranking"
          ],
          "connections": [
            "Feature Vectors"
          ],
          "mechanisms": [
            "Ranking Algorithms"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Feature Engineering"
          ]
        },
        "feature_processing": [
          "Feature Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2017_FGExpression",
      "label": "FG-Expression",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2017_FGExpression",
        "entity_id": "Huang2017_FGExpression",
        "name": "FG-Expression",
        "title": "Learning Fine-Grained Expressions to Solve Math Word Problems",
        "year": "2017",
        "authors": [
          "Huang, D.",
          "Shi, S.",
          "Yin, J.",
          "Lin, C.Y."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Template Fragment",
            "RankSVM"
          ],
          "connections": [
            "Sub-tree"
          ],
          "mechanisms": [
            "Fine-Grained Units"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Objective",
            "RankSVM"
          ],
          "parameter_tuning": [
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Template Fragment",
          "Textual Features"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Huang2017_FineGrainedExpressions",
      "label": "Fine-Grained Expressions",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2017_FineGrainedExpressions",
        "entity_id": "Huang2017_FineGrainedExpressions",
        "name": "Fine-Grained Expressions",
        "title": "Learning Fine-Grained Expressions to Solve Math Word Problems",
        "year": "2017",
        "authors": [
          "Huang, D.",
          "Shi, S.",
          "Lin, C.Y.",
          "Yin, J."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Trees"
          ],
          "connections": [
            "Equation Normalization"
          ],
          "mechanisms": [
            "Ensemble Model"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2017_NeuralMathWordProblemSolver",
      "label": "Neural Math Word Problem Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2017_NeuralMathWordProblemSolver",
        "entity_id": "Huang2017_NeuralMathWordProblemSolver",
        "name": "Neural Math Word Problem Solver",
        "title": "Learning Fine-grained Expressions to Solve Math Word Problems",
        "year": "2017",
        "authors": [
          "D. Huang",
          "S. Shi",
          "J. Yin",
          "C.-Y. Lin"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "Gated Recurrent Unit"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Policy Gradient"
          ],
          "parameter_tuning": [
            "Adam"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Dependency Parsing"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Huang2018_CASS",
      "label": "CASS (Copy and Alignment Sequence-to-Sequence)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2018_CASS",
        "entity_id": "Huang2018_CASS",
        "name": "CASS (Copy and Alignment Sequence-to-Sequence)",
        "title": "Neural Math Word Problem Solver with Reinforcement Learning",
        "year": "2018",
        "authors": [
          "Danqing Huang",
          "Jing Liu",
          "Chin-Yew Lin",
          "Jian Yin"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder",
            "Copy Mechanism",
            "Alignment Mechanism"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "Gated Recurrent Unit (GRU)",
            "Policy Gradient"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Reinforcement Learning",
            "Policy Gradient",
            "Pre-training with Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "SGD Optimizer",
            "Decaying Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Post-processing for Number Recovery"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Huang2018_ReinforcementLearning",
      "label": "Neural Math Word Problem Solver with Reinforcement Learning",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Huang2018_ReinforcementLearning",
        "entity_id": "Huang2018_ReinforcementLearning",
        "name": "Neural Math Word Problem Solver with Reinforcement Learning",
        "title": "Neural Math Word Problem Solver with Reinforcement Learning",
        "year": "2018",
        "authors": [
          "Huang, D.",
          "Liu, J.",
          "Lin, C.Y.",
          "Yin, J."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Sequence-to-Sequence Model",
            "Copy Mechanism",
            "Alignment Mechanism"
          ],
          "connections": [
            "Deep Q-Network"
          ],
          "mechanisms": [
            "Reinforcement Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Reinforcement Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Jung2004_WindowedHoughTransform",
      "label": "Windowed Hough Transform",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Jung2004_WindowedHoughTransform",
        "entity_id": "Jung2004_WindowedHoughTransform",
        "name": "Windowed Hough Transform",
        "title": "Rectangle Detection based on a Windowed Hough Transform",
        "year": "2004",
        "authors": [
          "Claudio Rosito Jung",
          "Rodrigo Schramm"
        ],
        "task": "[\"Rectangle Detection\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Sliding Window",
            "Hough Transform",
            "Peak Detection"
          ],
          "connections": [
            "Geometric Constraints"
          ],
          "mechanisms": [
            "Butterfly Pattern Analysis"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": [
            "Tθ",
            "Tρ",
            "TL",
            "Tα"
          ]
        },
        "feature_processing": [
          "Edge Detection",
          "Canny Operator"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Knuth1968_ContextFreeSemanticsAlgorithm",
      "label": "Context-Free Semantics Algorithm",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Knuth1968_ContextFreeSemanticsAlgorithm",
        "entity_id": "Knuth1968_ContextFreeSemanticsAlgorithm",
        "name": "Context-Free Semantics Algorithm",
        "title": "Semantics of Context-Free Languages",
        "year": "1968",
        "authors": [
          "Donald E. Knuth"
        ],
        "task": "[\"Semantic Analysis of Context-Free Grammars\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Directed Graph Construction"
          ],
          "connections": [
            "Graph Pasting"
          ],
          "mechanisms": [
            "Cycle Detection"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Koncel-Kedziorski2015_ALGES",
      "label": "ALGES",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Koncel-Kedziorski2015_ALGES",
        "entity_id": "Koncel-Kedziorski2015_ALGES",
        "name": "ALGES",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": "2015",
        "authors": [
          "Koncel-Kedziorski, R.",
          "Hajishirzi, H.",
          "Sabharwal, A.",
          "Etzioni, O.",
          "Ang, S. D."
        ],
        "task": "[\"Algebraic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Integer Linear Programming (ILP)",
            "Local Discriminative Models",
            "Global Discriminative Models"
          ],
          "connections": [
            "Equation Tree Generation",
            "Bottom-Up Scoring"
          ],
          "mechanisms": [
            "Quantified Sets (Qsets)",
            "Reordering Rules",
            "Feature Extraction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Weak Supervision",
            "Discriminative Models"
          ],
          "parameter_tuning": [
            "LIBSVM with RBF Kernel"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Qset Reordering",
          "Semantic Features"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Koncel-Kedziorski2015_SINGLEEQ",
      "label": "SINGLEEQ",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Koncel-Kedziorski2015_SINGLEEQ",
        "entity_id": "Koncel-Kedziorski2015_SINGLEEQ",
        "name": "SINGLEEQ",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": "2015",
        "authors": [
          "Rik Koncel-Kedziorski",
          "Hannaneh Hajishirzi",
          "Ashish Sabharwal",
          "Oren Etzioni",
          "Siena Ang"
        ],
        "task": "[\"Single Equation Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Single Equation Solver"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "KoncelKedziorski2015_EquationParser",
      "label": "Equation Parser",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "KoncelKedziorski2015_EquationParser",
        "entity_id": "KoncelKedziorski2015_EquationParser",
        "name": "Equation Parser",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": "2015",
        "authors": [
          "Koncel-Kedziorski",
          "Hajishirzi",
          "Sabharwal",
          "Etzioni",
          "Ang"
        ],
        "task": "[\"Algebraic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Semantic Parser",
            "Logic Representation"
          ],
          "connections": [
            "Dependency Parsing"
          ],
          "mechanisms": [
            "Rule-Based Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "Syntactic Parsing",
          "Coreference Resolution"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Kushman2014_EquationSetSolver",
      "label": "EquationSetSolver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Kushman2014_EquationSetSolver",
        "entity_id": "Kushman2014_EquationSetSolver",
        "name": "EquationSetSolver",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": "2014",
        "authors": [
          "Kushman",
          "Zettlemoyer",
          "Barzilay",
          "Artzi"
        ],
        "task": "[\"Equation Set Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Templates",
            "Number Slots"
          ],
          "connections": [
            "Template Matching"
          ],
          "mechanisms": [
            "RankSVM"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Regularization Parameters"
          ]
        },
        "feature_processing": [
          "Template Matching",
          "Feature Selection"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Kushman2014_EquationSystemSolver",
      "label": "Equation System Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Kushman2014_EquationSystemSolver",
        "entity_id": "Kushman2014_EquationSystemSolver",
        "name": "Equation System Solver",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": "2014",
        "authors": [
          "Nate Kushman",
          "Yoav Artzi",
          "Luke Zettlemoyer",
          "Regina Barzilay"
        ],
        "task": "[\"Algebra Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Equation Accuracy",
          "Answer Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Templates",
            "Log-linear Distribution",
            "Beam Search"
          ],
          "connections": [
            "Alignment between Equations and Text"
          ],
          "mechanisms": [
            "Latent Variable Modeling",
            "Marginal Data Log-Likelihood Optimization"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Weak Supervision",
            "L-BFGS Optimization"
          ],
          "parameter_tuning": [
            "L2 Regularization"
          ]
        },
        "feature_processing": [
          "Part-of-Speech Tagging",
          "Lematization",
          "Dependency Parsing",
          "Canonicalization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Kushman2014_KAZB",
      "label": "KAZB",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Kushman2014_KAZB",
        "entity_id": "Kushman2014_KAZB",
        "name": "KAZB",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": "2014",
        "authors": [
          "N. Kushman",
          "Y. Artzi",
          "L. Zettlemoyer",
          "R. Barzilay"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Templates",
            "Mapping to Templates"
          ],
          "connections": [
            "Training Data Equations"
          ],
          "mechanisms": [
            "Statistical Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "ALLEQ Version"
          ],
          "parameter_tuning": [
            "Linear Equations"
          ]
        },
        "feature_processing": [
          "Problem Similarity"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Kushman2014_TEMPLATE",
      "label": "TEMPLATE",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Kushman2014_TEMPLATE",
        "entity_id": "Kushman2014_TEMPLATE",
        "name": "TEMPLATE",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": "2014",
        "authors": [
          "Nathan Kushman",
          "Luke Zettlemoyer",
          "Regina Barzilay",
          "Yoav Artzi"
        ],
        "task": "[\"Algebra Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Template-based Solver"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Kushman2014_TemplateBased",
      "label": "Template-Based Method",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Kushman2014_TemplateBased",
        "entity_id": "Kushman2014_TemplateBased",
        "name": "Template-Based Method",
        "title": "Learning to Automatically Solve Algebra Word Problems",
        "year": "2014",
        "authors": [
          "Kushman, N.",
          "Artzi, Y.",
          "Zettlemoyer, L.",
          "Barzilay, R."
        ],
        "task": "[\"Algebraic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Templates",
            "Global and Local Features"
          ],
          "connections": [
            "Template Matching"
          ],
          "mechanisms": [
            "Dependency Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Fully Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Dependency Parsing",
          "Template Matching"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Kwiatkowski2013_SemanticParser",
      "label": "Semantic Parser with On-the-fly Ontology Matching",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Kwiatkowski2013_SemanticParser",
        "entity_id": "Kwiatkowski2013_SemanticParser",
        "name": "Semantic Parser with On-the-fly Ontology Matching",
        "title": "Scaling Semantic Parsers with On-the-fly Ontology Matching",
        "year": "2013",
        "authors": [
          "Tom Kwiatkowski",
          "Eunsol Choi",
          "Yoav Artzi",
          "Luke Zettlemoyer"
        ],
        "task": "[\"Question Answering\", \"Semantic Parsing\"]",
        "dataset": [],
        "metrics": [
          "Recall",
          "Precision",
          "F1"
        ],
        "architecture": {
          "components": [
            "Probabilistic CCG",
            "Ontology Matching Model"
          ],
          "connections": [
            "CCG Parsing",
            "Logical Form Transformation"
          ],
          "mechanisms": [
            "Domain-independent Parsing",
            "Structure Matching",
            "Constant Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Perceptron",
            "Latent Variable Learning"
          ],
          "parameter_tuning": [
            "Feature Weights"
          ]
        },
        "feature_processing": [
          "CCG Lexical Categories",
          "Wiktionary Definitions",
          "Knowledge Base Constraints"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Kwiatkowski2013_SemanticParsers",
      "label": "Semantic Parsers with On-the-fly Ontology Matching",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Kwiatkowski2013_SemanticParsers",
        "entity_id": "Kwiatkowski2013_SemanticParsers",
        "name": "Semantic Parsers with On-the-fly Ontology Matching",
        "title": "Scaling Semantic Parsers with On-the-fly Ontology Matching",
        "year": "2013",
        "authors": [
          "Kwiatkowski, T.",
          "Choi, E.",
          "Artzi, Y.",
          "Zettlemoyer, L."
        ],
        "task": "[\"Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Probabilistic CCG"
          ],
          "connections": [
            "Logical Forms"
          ],
          "mechanisms": [
            "Ontology Matching"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Lagunovsky1999_StraightLineExtraction",
      "label": "Straight-line-based Primitive Extraction",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Lagunovsky1999_StraightLineExtraction",
        "entity_id": "Lagunovsky1999_StraightLineExtraction",
        "name": "Straight-line-based Primitive Extraction",
        "title": "Straight-line-based Primitive Extraction in Grey-scale Object Recognition",
        "year": "1999",
        "authors": [
          "D. Lagunovsky",
          "S. Ablameyko"
        ],
        "task": "[\"Primitive Extraction\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Linear Primitives",
            "Grouping"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Liang2016_TagBasedSolver",
      "label": "Tag-based English Math Word Problem Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Liang2016_TagBasedSolver",
        "entity_id": "Liang2016_TagBasedSolver",
        "name": "Tag-based English Math Word Problem Solver",
        "title": "A Tag-based English Math Word Problem Solver with Understanding, Reasoning and Explanation",
        "year": "2016",
        "authors": [
          "Chao-Chun Liang",
          "Kuang-Yi Hsu",
          "Chien-Tsung Huang",
          "Chung-Min Li",
          "Shen-Yu Miao",
          "Keh-Yih Su"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy_Classification",
          "Solution_Type_Accuracy"
        ],
        "architecture": {
          "components": [
            "Language Analyzer",
            "Solution Type Classifier",
            "Logic Form Converter",
            "Inference Engine",
            "Explanation Generator"
          ],
          "connections": [
            "Pipeline"
          ],
          "mechanisms": [
            "Tag-based annotation",
            "First-order logic predicates",
            "Logic inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised learning",
            "SVM with linear kernel"
          ],
          "parameter_tuning": [
            "Feature extraction",
            "Pattern matching"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "POS tagging",
          "Named entity recognition",
          "Dependency parsing",
          "Co-reference resolution"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Lin1998_BuildingDetection",
      "label": "Building Detection and Description",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Lin1998_BuildingDetection",
        "entity_id": "Lin1998_BuildingDetection",
        "name": "Building Detection and Description",
        "title": "Building Detection and Description from a Single Intensity Image",
        "year": "1998",
        "authors": [
          "C. Lin",
          "R. Nevatia"
        ],
        "task": "[\"Building Detection\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Line Detection",
            "Anti-parallel Lines"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Lin1998_MINIPAR",
      "label": "MINIPAR",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Lin1998_MINIPAR",
        "entity_id": "Lin1998_MINIPAR",
        "name": "MINIPAR",
        "title": "Dependency-based evaluation of MINIPAR",
        "year": "1998",
        "authors": [
          "Dekang Lin"
        ],
        "task": "[\"Dependency Parsing\"]",
        "dataset": [],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Dependency Parser"
          ],
          "connections": [
            "Dependency Relations"
          ],
          "mechanisms": [
            "Rule-Based Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Rule-Based"
          ],
          "parameter_tuning": [
            "Dependency Rules"
          ]
        },
        "feature_processing": [
          "Dependency Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Lin2017_SelfAttentiveEmbedding",
      "label": "Structured Self-attentive Sentence Embedding",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Lin2017_SelfAttentiveEmbedding",
        "entity_id": "Lin2017_SelfAttentiveEmbedding",
        "name": "Structured Self-attentive Sentence Embedding",
        "title": "A Structured Self-attentive Sentence Embedding",
        "year": "2017",
        "authors": [
          "Lin, Z.",
          "Feng, M.",
          "dos Santos, C.N.",
          "Yu, M.",
          "Xiang, B.",
          "Zhou, B.",
          "Bengio, Y."
        ],
        "task": "[\"Sentence Embedding\"]",
        "dataset": [],
        "metrics": [
          "Performance Gain"
        ],
        "architecture": {
          "components": [
            "Self-Attention Mechanism",
            "Regularization Term"
          ],
          "connections": [
            "2-D Matrix Representation"
          ],
          "mechanisms": [
            "Interpretable Embedding"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Unsupervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Lin2017_StructuredSelfAttention",
      "label": "Structured Self-Attention",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Lin2017_StructuredSelfAttention",
        "entity_id": "Lin2017_StructuredSelfAttention",
        "name": "Structured Self-Attention",
        "title": "Data-Driven Methods for Solving Algebra Word Problems",
        "year": "2018",
        "authors": [
          "Robaidek, Benjamin",
          "Koncel-Kedziorski, Rik",
          "Hajishirzi, Hannaneh"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "BiLSTM",
            "Multi-Hop Attention"
          ],
          "connections": [
            "Self-Attention"
          ],
          "mechanisms": [
            "Fixed Size Embedding"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-End Training"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "label": "Structured Self-attentive Sentence Embedding",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
        "entity_id": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
        "name": "Structured Self-attentive Sentence Embedding",
        "title": "A Structured Self-attentive Sentence Embedding",
        "year": "2017",
        "authors": [
          "Zhouhan Lin",
          "Minwei Feng",
          "Cicero Nogueira dos Santos",
          "Mo Yu",
          "Bing Xiang",
          "Bowen Zhou",
          "Yoshua Bengio"
        ],
        "task": "[\"Author Profiling\", \"Sentiment Classification\", \"Textual Entailment\"]",
        "dataset": [],
        "metrics": [
          "Classification Accuracy"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM",
            "Self-attention Mechanism"
          ],
          "connections": [
            "Weighted Sum of Hidden States"
          ],
          "mechanisms": [
            "Softmax",
            "Tanh",
            "Penalization Term"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Stochastic Gradient Descent",
            "AdaGrad"
          ],
          "parameter_tuning": [
            "Dropout",
            "L2 Regularization",
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Embeddings Initialization",
          "Tokenization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Lu2016_HierarchicalCoAttention",
      "label": "Hierarchical Co-attention",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Lu2016_HierarchicalCoAttention",
        "entity_id": "Lu2016_HierarchicalCoAttention",
        "name": "Hierarchical Co-attention",
        "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "year": "2016",
        "authors": [
          "Yash Goyal",
          "Tejas Khot",
          "Douglas Summers-Stay",
          "Dhruv Batra",
          "Devi Parikh"
        ],
        "task": "[\"Visual Question Answering\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Word-level attention",
            "Phrase-level attention",
            "Entire question-level attention"
          ],
          "connections": [
            "Co-attention mechanism"
          ],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Luong2016_MultiTaskSeq2Seq",
      "label": "Multi-task Sequence to Sequence Learning",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Luong2016_MultiTaskSeq2Seq",
        "entity_id": "Luong2016_MultiTaskSeq2Seq",
        "name": "Multi-task Sequence to Sequence Learning",
        "title": "MULTI-TASK SEQUENCE TO SEQUENCE LEARNING",
        "year": "2016",
        "authors": [
          "Minh-Thang Luong",
          "Quoc V. Le",
          "Ilya Sutskever",
          "Oriol Vinyals",
          "Lukasz Kaiser"
        ],
        "task": "[\"Machine Translation\", \"Constituency Parsing\", \"Image Caption Generation\"]",
        "dataset": [],
        "metrics": [
          "BLEU",
          "F1 Score",
          "Perplexity"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "One-to-Many",
            "Many-to-One",
            "Many-to-Many"
          ],
          "mechanisms": [
            "Recurrent Neural Networks",
            "Long Short-Term Memory (LSTM)",
            "Attention Mechanism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Alternating Training",
            "Parameter Updates Allocation"
          ],
          "parameter_tuning": [
            "SGD",
            "Learning Rate Adjustment"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Embeddings Initialization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Ma2010_FrameBasedCalculus",
      "label": "Frame-Based Calculus",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Ma2010_FrameBasedCalculus",
        "entity_id": "Ma2010_FrameBasedCalculus",
        "name": "Frame-Based Calculus",
        "title": "Frame-Based Calculus of solving Arithmetic Multi-Step Addition and Subtraction word problems",
        "year": "2010",
        "authors": [
          "Ma Yuhui",
          "Zhou Ying",
          "Cui Guangzuo",
          "Ren Yun",
          "Huang Ronghuai"
        ],
        "task": "[\"Solving Multi-Step Addition and Subtraction Word Problems\"]",
        "dataset": [],
        "metrics": [
          "Correctness of Solution"
        ],
        "architecture": {
          "components": [
            "Problem Frames",
            "Production Rules"
          ],
          "connections": [
            "Means-end Analysis"
          ],
          "mechanisms": [
            "Frame Identification",
            "Working Memory",
            "Rule Base",
            "Executive Controlling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Means-end Analysis"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Natural Language Processing",
          "Semantic Frame Construction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Mikolov2013_HierarchicalSoftmax",
      "label": "Hierarchical Softmax",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Mikolov2013_HierarchicalSoftmax",
        "entity_id": "Mikolov2013_HierarchicalSoftmax",
        "name": "Hierarchical Softmax",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": "2013",
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "[\"Word Representation Learning\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Tree",
            "Inner Nodes",
            "Leaf Nodes"
          ],
          "connections": [
            "Paths in Binary Tree"
          ],
          "mechanisms": [
            "Logarithmic Evaluation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Binary Huffman Tree"
          ],
          "parameter_tuning": [
            "Tree Structure"
          ]
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Mikolov2013_NegativeSampling",
      "label": "Negative Sampling",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Mikolov2013_NegativeSampling",
        "entity_id": "Mikolov2013_NegativeSampling",
        "name": "Negative Sampling",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": "2013",
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "[\"Word Representation Learning\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Noise Distribution",
            "Target Word",
            "Negative Samples"
          ],
          "connections": [
            "Logistic Regression"
          ],
          "mechanisms": [
            "Log-Sigmoid Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Logistic Regression"
          ],
          "parameter_tuning": [
            "Number of Negative Samples"
          ]
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Mikolov2013_SkipGram",
      "label": "Skip-gram",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Mikolov2013_SkipGram",
        "entity_id": "Mikolov2013_SkipGram",
        "name": "Skip-gram",
        "title": "Distributed Representations of Words and Phrases and their Compositionality",
        "year": "2013",
        "authors": [
          "Tomas Mikolov",
          "Ilya Sutskever",
          "Kai Chen",
          "Greg Corrado",
          "Jeffrey Dean"
        ],
        "task": "[\"Word Representation Learning\", \"Phrase Representation Learning\"]",
        "dataset": [],
        "metrics": [
          "Accuracy",
          "Syntactic Accuracy",
          "Semantic Accuracy"
        ],
        "architecture": {
          "components": [
            "Input Layer",
            "Hidden Layer",
            "Output Layer"
          ],
          "connections": [
            "Weight Matrices"
          ],
          "mechanisms": [
            "Negative Sampling",
            "Hierarchical Softmax"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Subsampling of Frequent Words",
            "Negative Sampling",
            "Hierarchical Softmax"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Context Window Size",
            "Vector Dimensionality"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Subsampling of Frequent Words"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Mitra2016_FormulaBasedSolver",
      "label": "Formula-Based Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Mitra2016_FormulaBasedSolver",
        "entity_id": "Mitra2016_FormulaBasedSolver",
        "name": "Formula-Based Solver",
        "title": "Learning To Use Formulas To Solve Simple Arithmetic Problems",
        "year": "2016",
        "authors": [
          "Arindam Mitra",
          "Chitta Baral"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Variable Identification",
            "Formula Recognition",
            "Equation Generation"
          ],
          "connections": [
            "Part-Whole",
            "Change",
            "Comparison"
          ],
          "mechanisms": [
            "Log-linear Model",
            "Feature Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Parameter Vector θ"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Machine Readable Dictionaries",
          "Semantic Relations Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Mitra2016_FormulaSolver",
      "label": "FormulaSolver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Mitra2016_FormulaSolver",
        "entity_id": "Mitra2016_FormulaSolver",
        "name": "FormulaSolver",
        "title": "Learning To Use Formulas To Solve Simple Arithmetic Problems",
        "year": "2016",
        "authors": [
          "Arindam Mitra",
          "Chitta Baral"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Higher-level representation"
          ],
          "connections": [
            "Formula recognition"
          ],
          "mechanisms": [
            "Equation generation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Scoring <formula, variables> pairs"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Variables and attributes extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Novak1985_BEATRIX",
      "label": "BEATRIX",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Novak1985_BEATRIX",
        "entity_id": "Novak1985_BEATRIX",
        "name": "BEATRIX",
        "title": "Understanding Natural Language with Diagrams",
        "year": "1985",
        "authors": [
          "Novak, G."
        ],
        "task": "[\"Physics Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Correctness"
        ],
        "architecture": {
          "components": [
            "English Parser",
            "Diagram Parser",
            "Blackboard Architecture"
          ],
          "connections": [
            "Coreference Resolution"
          ],
          "mechanisms": [
            "Opportunistic Co-parsers"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Rule-Based"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Propositional Representation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Novak1990_BEATRIX",
      "label": "BEATRIX",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Novak1990_BEATRIX",
        "entity_id": "Novak1990_BEATRIX",
        "name": "BEATRIX",
        "title": "Understanding Natural Language with Diagrams",
        "year": "1990",
        "authors": [
          "Novak, G. S.",
          "Bulko, W."
        ],
        "task": "[\"Physics Problem Solving\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "English Parser",
            "Diagram Parser",
            "Coreference Resolver"
          ],
          "connections": [
            "Blackboard Architecture"
          ],
          "mechanisms": [
            "Opportunistic Co-parsers",
            "ATN Parser",
            "Knowledge Sources"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Text Parsing",
          "Diagram Parsing",
          "Coreference Resolution"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Pennington2014_GloVe",
      "label": "GloVe",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Pennington2014_GloVe",
        "entity_id": "Pennington2014_GloVe",
        "name": "GloVe",
        "title": "GloVe: Global Vectors for Word Representation",
        "year": "2014",
        "authors": [
          "Jeffrey Pennington",
          "Richard Socher",
          "Christopher D. Manning"
        ],
        "task": "[\"Word Representation\", \"Word Analogy\", \"Word Similarity\", \"Named Entity Recognition\"]",
        "dataset": [],
        "metrics": [
          "Accuracy",
          "Spearman Rank Correlation"
        ],
        "architecture": {
          "components": [
            "Global Log-Bilinear Regression Model",
            "Weighted Least Squares Model"
          ],
          "connections": [
            "Dot Product",
            "Bias Terms"
          ],
          "mechanisms": [
            "Co-occurrence Matrix Factorization",
            "Weighting Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "AdaGrad",
            "Stochastic Sampling"
          ],
          "parameter_tuning": [
            "xmax=100",
            "α=3/4",
            "Initial Learning Rate=0.05"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Lowercasing",
          "Context Window Construction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Raghunathan2010_MultiPassSieve",
      "label": "Multi-Pass Sieve",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Raghunathan2010_MultiPassSieve",
        "entity_id": "Raghunathan2010_MultiPassSieve",
        "name": "Multi-Pass Sieve",
        "title": "A Multi-Pass Sieve for Coreference Resolution",
        "year": "2010",
        "authors": [
          "Karthik Raghunathan",
          "Heeyoung Lee",
          "Sudarshan Rangarajan",
          "Nathanael Chambers",
          "Mihai Surdeanu",
          "Dan Jurafsky",
          "Christopher Manning"
        ],
        "task": "[\"Coreference Resolution\"]",
        "dataset": [],
        "metrics": [
          "Pairwise F1",
          "MUC",
          "B3"
        ],
        "architecture": {
          "components": [
            "Pass 1- Exact Match",
            "Pass 2- Precise Constructs",
            "Pass 3- Strict Head Matching",
            "Pass 4- Variants of Strict Head",
            "Pass 5- Variants of Strict Head",
            "Pass 6- Relaxed Head Matching",
            "Pass 7- Pronouns"
          ],
          "connections": [
            "Attribute Sharing",
            "Cluster Information Propagation"
          ],
          "mechanisms": [
            "Deterministic Models",
            "Tiered Model"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Unsupervised Learning"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Syntactic Information",
          "Attribute Detection",
          "Modifier Information",
          "Discourse Salience"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Robaidek2018_BiLSTMClassifier",
      "label": "BiLSTM Classifier",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Robaidek2018_BiLSTMClassifier",
        "entity_id": "Robaidek2018_BiLSTMClassifier",
        "name": "BiLSTM Classifier",
        "title": "Data-Driven Methods for Solving Algebra Word Problems",
        "year": "2018",
        "authors": [
          "Robaidek, Benjamin",
          "Koncel-Kedziorski, Rik",
          "Hajishirzi, Hannaneh"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "BiLSTM"
          ],
          "connections": [
            "Softmax"
          ],
          "mechanisms": [
            "Cross Entropy Loss"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-End Training"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2015_ALGES",
      "label": "ALGES",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_ALGES",
        "entity_id": "Roy2015_ALGES",
        "name": "ALGES",
        "title": "Parsing Algebraic Word Problems into Equations",
        "year": "2015",
        "authors": [
          "Koncel-Kedziorski",
          "Hajishirzi",
          "Sabharwal",
          "Etzioni",
          "Ang"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Equation Tree",
            "Integer Linear Programming"
          ],
          "connections": [
            "Syntactic Validity",
            "Type Consistency"
          ],
          "mechanisms": [
            "Beam Search"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-validation"
          ],
          "parameter_tuning": [
            "Beam Size"
          ]
        },
        "feature_processing": [
          "Syntactic Parsing",
          "Coreference Resolution"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Roy2015_ARIS",
      "label": "ARIS",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_ARIS",
        "entity_id": "Roy2015_ARIS",
        "name": "ARIS",
        "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization",
        "year": "2015",
        "authors": [
          "Hosseini, M.J.",
          "Hajishirzi, H.",
          "Etzioni, O.",
          "Kushman, N."
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Verb Categorization Accuracy",
          "Problem Solving Accuracy"
        ],
        "architecture": {
          "components": [
            "Verb Categorization"
          ],
          "connections": [
            "Equation Generation"
          ],
          "mechanisms": [
            "Semantic Mapping"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2015_ExpressionTree",
      "label": "Expression Tree",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_ExpressionTree",
        "entity_id": "Roy2015_ExpressionTree",
        "name": "Expression Tree",
        "title": "Solving General Arithmetic Word Problems",
        "year": "2015",
        "authors": [
          "Roy, S.",
          "Roth, D."
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Classifier",
            "Expression Tree"
          ],
          "connections": [
            "Beam Search"
          ],
          "mechanisms": [
            "Binary Classification"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossValidation"
          ],
          "parameter_tuning": [
            "Beam Size"
          ]
        },
        "feature_processing": [
          "Quantity Extraction",
          "Verb Categorization"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Roy2015_GeneralArithmetic",
      "label": "General Arithmetic Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_GeneralArithmetic",
        "entity_id": "Roy2015_GeneralArithmetic",
        "name": "General Arithmetic Solver",
        "title": "Solving General Arithmetic Word Problems",
        "year": "2015",
        "authors": [
          "Roy, S.",
          "Roth, D."
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Trees"
          ],
          "connections": [
            "Constrained Inference Framework"
          ],
          "mechanisms": [
            "Quantity Schemas"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2015_LCA++",
      "label": "LCA++",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_LCA++",
        "entity_id": "Roy2015_LCA++",
        "name": "LCA++",
        "title": "Solving General Arithmetic Word Problems",
        "year": "2015",
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Irrelevance Classifier",
            "LCA Operation Classifier"
          ],
          "connections": [
            "Monotonic Expression Tree"
          ],
          "mechanisms": [
            "Feature Augmentation",
            "Positive Answer Constraint"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Context Features"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2015_MathWordProblemSolver",
      "label": "MathWordProblemSolver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_MathWordProblemSolver",
        "entity_id": "Roy2015_MathWordProblemSolver",
        "name": "MathWordProblemSolver",
        "title": "Reasoning about Quantities in Natural Language",
        "year": "2015",
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Quantity Pair Classifier",
            "Operation Classifier",
            "Order Classifier"
          ],
          "connections": [
            "Cascade of Classifiers"
          ],
          "mechanisms": [
            "Sparse Averaged Perceptron"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Gold Annotations"
          ],
          "parameter_tuning": [
            "Feature Engineering"
          ]
        },
        "feature_processing": [
          "Unigrams and Bigrams",
          "POS Tags",
          "Quantity Units Matching"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2015_QuantityEntailment",
      "label": "QuantityEntailment",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_QuantityEntailment",
        "entity_id": "Roy2015_QuantityEntailment",
        "name": "QuantityEntailment",
        "title": "Reasoning about Quantities in Natural Language",
        "year": "2015",
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": "[\"Quantity Entailment\"]",
        "dataset": [],
        "metrics": [
          "F1 Score",
          "Precision",
          "Recall"
        ],
        "architecture": {
          "components": [
            "Extraction Phase",
            "Reasoning Phase"
          ],
          "connections": [
            "Implicit Quantity Productions",
            "Quantity Comparisons"
          ],
          "mechanisms": [
            "Monotonicity Verification",
            "Coreference Resolution",
            "Semantic Role Labeling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Rule-based Standardization",
            "Coreference Resolution",
            "Semantic Role Labeling"
          ],
          "parameter_tuning": [
            "Monotonicity Verification",
            "Coreference Resolution",
            "Semantic Role Labeling"
          ]
        },
        "feature_processing": [
          "WordNet Synsets",
          "POS Tags",
          "Contextual Cues"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2015_QuantityExtraction",
      "label": "QuantityExtraction",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_QuantityExtraction",
        "entity_id": "Roy2015_QuantityExtraction",
        "name": "QuantityExtraction",
        "title": "Reasoning about Quantities in Natural Language",
        "year": "2015",
        "authors": [
          "Subhro Roy",
          "Tim Vieira",
          "Dan Roth"
        ],
        "task": "[\"Quantity Recognition\", \"Quantity Standardization\"]",
        "dataset": [],
        "metrics": [
          "F1 Score",
          "Precision",
          "Recall"
        ],
        "architecture": {
          "components": [
            "Segmentation",
            "Standardization"
          ],
          "connections": [
            "Semi-CRF",
            "Bank of Classifiers"
          ],
          "mechanisms": [
            "Parameter Averaging",
            "Perceptron Algorithm"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Structured Perceptron",
            "Parameter Averaging"
          ],
          "parameter_tuning": [
            "Semi-CRF",
            "Bank of Classifiers"
          ]
        },
        "feature_processing": [
          "Word Class Features",
          "Character-based Features",
          "Part of Speech Tags"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2015_SolvingGeneralArithmetic",
      "label": "Solving General Arithmetic Word Problems",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_SolvingGeneralArithmetic",
        "entity_id": "Roy2015_SolvingGeneralArithmetic",
        "name": "Solving General Arithmetic Word Problems",
        "title": "Solving General Arithmetic Word Problems",
        "year": "2015",
        "authors": [
          "Roy",
          "Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Classifier",
            "Expression Tree"
          ],
          "connections": [
            "Beam Search"
          ],
          "mechanisms": [
            "Local Classifier"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "Feature Extraction",
          "Quantity Relevance"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Roy2015_UnitDependencyGraph",
      "label": "Unit Dependency Graph (UDG)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2015_UnitDependencyGraph",
        "entity_id": "Roy2015_UnitDependencyGraph",
        "name": "Unit Dependency Graph (UDG)",
        "title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
        "year": "2015",
        "authors": [
          "Roy, S.",
          "Roth, D."
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Unit Dependency Graph"
          ],
          "connections": [
            "Constrained Inference Framework"
          ],
          "mechanisms": [
            "Unit Compatibility"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Decomposed Model"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2016_ExpressionTreeBasedSolver",
      "label": "Expression Tree Based Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2016_ExpressionTreeBasedSolver",
        "entity_id": "Roy2016_ExpressionTreeBasedSolver",
        "name": "Expression Tree Based Solver",
        "title": "Solving General Arithmetic Word Problems",
        "year": "2016",
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Tree",
            "Monotonic Expression Tree",
            "Quantity Schema"
          ],
          "connections": [
            "LCA Operation Prediction",
            "Relevance Classification"
          ],
          "mechanisms": [
            "Constrained Inference Framework",
            "Multiclass Classification"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search",
            "Pairwise Conjunction of Features"
          ],
          "parameter_tuning": [
            "wIRR"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Shallow Parsing",
          "Unit Extraction",
          "Noun Phrase Extraction",
          "Rate Detection"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2017_UNITDEP",
      "label": "UNITDEP",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2017_UNITDEP",
        "entity_id": "Roy2017_UNITDEP",
        "name": "UNITDEP",
        "title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
        "year": "2017",
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Vertex Classifier",
            "Edge Classifier",
            "Constrained Inference Module"
          ],
          "connections": [
            "Joint Inference with Arithmetic Solver"
          ],
          "mechanisms": [
            "Decomposed Model",
            "Monotonic Expression Tree"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search",
            "Scaling Parameters Tuning"
          ],
          "parameter_tuning": [
            "λIRR",
            "λVERTEX",
            "λEDGE"
          ]
        },
        "feature_processing": [
          "Context Features",
          "Rule-based Extraction Features"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2017_UnitDependencyGraph",
      "label": "UnitDependencyGraph",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2017_UnitDependencyGraph",
        "entity_id": "Roy2017_UnitDependencyGraph",
        "name": "UnitDependencyGraph",
        "title": "Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving",
        "year": "2017",
        "authors": [
          "Roy",
          "Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Unit Dependency Graph",
            "Context Features"
          ],
          "connections": [
            "Graph Construction"
          ],
          "mechanisms": [
            "Node Classification",
            "Edge Classification"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Regularization Parameters"
          ]
        },
        "feature_processing": [
          "Unit Extraction",
          "Context Feature Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Roy2018_DeclarativeKnowledge",
      "label": "Declarative Knowledge Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2018_DeclarativeKnowledge",
        "entity_id": "Roy2018_DeclarativeKnowledge",
        "name": "Declarative Knowledge Solver",
        "title": "Mapping to Declarative Knowledge for Word Problem Solving",
        "year": "2018",
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "[\"Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Declarative rules"
          ],
          "connections": [
            "Equation generation"
          ],
          "mechanisms": [
            "Constrained inference framework"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Variables and attributes extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2018_DeclarativeMapping",
      "label": "Declarative Mapping",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2018_DeclarativeMapping",
        "entity_id": "Roy2018_DeclarativeMapping",
        "name": "Declarative Mapping",
        "title": "Mapping to Declarative Knowledge for Word Problem Solving",
        "year": "2018",
        "authors": [
          "Roy, S.",
          "Roth, D."
        ],
        "task": "[\"Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Declarative Rules"
          ],
          "connections": [
            "Equation Generation"
          ],
          "mechanisms": [
            "Semantic Alignment"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2018_KNOWLEDGE",
      "label": "KNOWLEDGE",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2018_KNOWLEDGE",
        "entity_id": "Roy2018_KNOWLEDGE",
        "name": "KNOWLEDGE",
        "title": "Mapping to Declarative Knowledge for Word Problem Solving",
        "year": "2018",
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Concept Selection",
            "Declarative Rule Selection"
          ],
          "connections": [
            "Latent Variable Modeling"
          ],
          "mechanisms": [
            "Transfer",
            "Dimensional Analysis",
            "Part-Whole Relation",
            "Explicit Math"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Latent Structured SVM",
            "Two Stage Learning"
          ],
          "parameter_tuning": [
            "Beam Search"
          ]
        },
        "feature_processing": [
          "Dependency Parsing",
          "Coreference Resolution",
          "Verb Classification",
          "Rate Component Detection"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Roy2018_UnitDependencyGraph",
      "label": "Unit Dependency Graph",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Roy2018_UnitDependencyGraph",
        "entity_id": "Roy2018_UnitDependencyGraph",
        "name": "Unit Dependency Graph",
        "title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
        "year": "2018",
        "authors": [
          "Subhro Roy",
          "Dan Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Error Reduction"
        ],
        "architecture": {
          "components": [
            "Unit Dependency Graphs"
          ],
          "connections": [
            "Constrained Inference Framework"
          ],
          "mechanisms": [
            "Domain Knowledge"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameters"
          ]
        },
        "feature_processing": [
          "Feature Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Seo2014_G-ALIGNER",
      "label": "G-ALIGNER",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Seo2014_G-ALIGNER",
        "entity_id": "Seo2014_G-ALIGNER",
        "name": "G-ALIGNER",
        "title": "Diagram Understanding in Geometry Questions",
        "year": "2014",
        "authors": [
          "Min Joon Seo",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Oren Etzioni"
        ],
        "task": "[\"Diagram Understanding\", \"Geometry Question Solving\"]",
        "dataset": [],
        "metrics": [
          "F1 Score"
        ],
        "architecture": {
          "components": [
            "Primitive Detection",
            "Alignment with Textual Mentions"
          ],
          "connections": [
            "Submodular Optimization",
            "Greedy Approximation"
          ],
          "mechanisms": [
            "Hough Transform",
            "Corner Detection"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Submodular Function Maximization"
          ],
          "parameter_tuning": [
            "No Parameter Tuning Required"
          ]
        },
        "feature_processing": [
          "OCR for Label Positioning",
          "Textual Mention Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Seo2014_GALINGER",
      "label": "G-ALINGER",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Seo2014_GALINGER",
        "entity_id": "Seo2014_GALINGER",
        "name": "G-ALINGER",
        "title": "Diagram Understanding in Geometry Questions",
        "year": "2014",
        "authors": [
          "Seo, M.J.",
          "Hajishirzi, H.",
          "Farhadi, A.",
          "Etzioni, O."
        ],
        "task": "[\"Geometry Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Hough Transform",
            "Objective Function"
          ],
          "connections": [
            "Pixel Coverage",
            "Visual Coherence",
            "Textual-Visual Alignment"
          ],
          "mechanisms": [
            "Greedy Algorithm"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Primitive Detection"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Diagram Parsing",
          "Text Parsing"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Seo2014_GeoSolver",
      "label": "GeoSolver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Seo2014_GeoSolver",
        "entity_id": "Seo2014_GeoSolver",
        "name": "GeoSolver",
        "title": "Diagram Understanding in Geometry Questions",
        "year": "2014",
        "authors": [
          "Seo",
          "Hajishirzi",
          "Farhadi",
          "Etzioni"
        ],
        "task": "[\"Geometry Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Diagram Parser",
            "Text Parser"
          ],
          "connections": [
            "Visual-Text Alignment"
          ],
          "mechanisms": [
            "Hough Transform"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Thresholds"
          ]
        },
        "feature_processing": [
          "Visual Features",
          "Textual Features"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Seo2015_GEOS",
      "label": "GEOS",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Seo2015_GEOS",
        "entity_id": "Seo2015_GEOS",
        "name": "GEOS",
        "title": "Solving Geometry Problems: Combining Text and Diagram Interpretation",
        "year": "2015",
        "authors": [
          "Minjoon Seo",
          "Hannaneh Hajishirzi",
          "Ali Farhadi",
          "Oren Etzioni",
          "Clint Malcolm"
        ],
        "task": "[\"Geometry Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "SAT Score"
        ],
        "architecture": {
          "components": [
            "Text Parser",
            "Diagram Parser",
            "Optimization Module",
            "Solver"
          ],
          "connections": [
            "Text-Diagram Integration"
          ],
          "mechanisms": [
            "Submodular Optimization",
            "Greedy Algorithm"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning",
            "Log-linear Model"
          ],
          "parameter_tuning": [
            "Trade-off Parameter λ"
          ]
        },
        "feature_processing": [
          "Concept Identification",
          "Relation Identification",
          "Relation Completion"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Shi2015_DOL",
      "label": "DOL",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Shi2015_DOL",
        "entity_id": "Shi2015_DOL",
        "name": "DOL",
        "title": "Automatically Solving Number Word Problems by Semantic Parsing and Reasoning",
        "year": "2015",
        "authors": [
          "Shi",
          "Wang",
          "Lin",
          "Liu",
          "Rui"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Semantic Parser",
            "CFG Parser"
          ],
          "connections": [
            "Context-Free Grammar"
          ],
          "mechanisms": [
            "Dependency Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Lexicon Construction"
          ]
        },
        "feature_processing": [
          "Lexical Features",
          "Grammar Rules"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Shi2015_NumberWordProblemSolver",
      "label": "NumberWordProblemSolver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Shi2015_NumberWordProblemSolver",
        "entity_id": "Shi2015_NumberWordProblemSolver",
        "name": "NumberWordProblemSolver",
        "title": "Automatically Solving Number Word Problems by Semantic Parsing and Reasoning",
        "year": "2015",
        "authors": [
          "Shi",
          "Wang",
          "Lin",
          "Liu",
          "Rui"
        ],
        "task": "[\"Number Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Semantic Parser",
            "Reasoning Module"
          ],
          "connections": [
            "Text-Diagram Alignment"
          ],
          "mechanisms": [
            "Dependency Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Thresholds"
          ]
        },
        "feature_processing": [
          "Verb Categorization",
          "Quantity Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Shi2015_SigmaDolphin",
      "label": "SigmaDolphin",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Shi2015_SigmaDolphin",
        "entity_id": "Shi2015_SigmaDolphin",
        "name": "SigmaDolphin",
        "title": "Automatically Solving Number Word Problems by Semantic Parsing and Reasoning",
        "year": "2015",
        "authors": [
          "Shuming Shi",
          "Yuehui Wang",
          "Chin-Yew Lin",
          "Xiaojiang Liu",
          "Yong Rui"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Precision",
          "Recall",
          "F1"
        ],
        "architecture": {
          "components": [
            "CFG Parser",
            "Reasoning Module"
          ],
          "connections": [
            "Semantic Parsing",
            "Math Expression Derivation"
          ],
          "mechanisms": [
            "Context-Free Grammar",
            "Semantic Representation Language (DOL)"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Top-down Parsing",
            "Earley Algorithm"
          ],
          "parameter_tuning": [
            "Type Compatibility Checking"
          ]
        },
        "feature_processing": [
          "Lexical String Handling",
          "Entity Variable Assignment"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Siyam2017_ArabicArithmetic",
      "label": "Arabic Arithmetic Word Problem Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Siyam2017_ArabicArithmetic",
        "entity_id": "Siyam2017_ArabicArithmetic",
        "name": "Arabic Arithmetic Word Problem Solver",
        "title": "Solving Arabic Arithmetic Word Problems",
        "year": "2017",
        "authors": [
          "Siyam",
          "Saa",
          "Alqaryouti",
          "Shaalan"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Verb Categorization"
          ],
          "connections": [
            "Syntactic Parser",
            "Named Entity Recognition"
          ],
          "mechanisms": [
            "Customization for Arabic Language"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "Verb Categorization",
          "Syntactic Parsing"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Siyam2017_AxiomaticKnowledge",
      "label": "AxiomaticKnowledge",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Siyam2017_AxiomaticKnowledge",
        "entity_id": "Siyam2017_AxiomaticKnowledge",
        "name": "AxiomaticKnowledge",
        "title": "From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems",
        "year": "2017",
        "authors": [
          "Sachan",
          "Dubey",
          "Xing"
        ],
        "task": "[\"Geometry Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Axiomatic Knowledge Base",
            "Reasoning Engine"
          ],
          "connections": [
            "Knowledge Integration"
          ],
          "mechanisms": [
            "Logical Inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Regularization Parameters"
          ]
        },
        "feature_processing": [
          "Axiomatic Knowledge Extraction",
          "Logical Inference"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Slagle1965_DEDUCOM",
      "label": "DEDUCOM",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Slagle1965_DEDUCOM",
        "entity_id": "Slagle1965_DEDUCOM",
        "name": "DEDUCOM",
        "title": "Experiments with a Deductive Question-Answering Program",
        "year": "1965",
        "authors": [
          "James R. Slagle"
        ],
        "task": "[\"Deductive Question-Answering\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Fact Interpreter",
            "Question Reducer",
            "Search Procedure"
          ],
          "connections": [
            "Depth-First Search"
          ],
          "mechanisms": [
            "Predicate Calculus Deductions",
            "Logical Deductions"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Heuristic Programming"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Fact Input Processing",
          "Question Parsing"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Sleator1993_LinkParser",
      "label": "Link Parser",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Sleator1993_LinkParser",
        "entity_id": "Sleator1993_LinkParser",
        "name": "Link Parser",
        "title": "Parsing English with a link grammar",
        "year": "1993",
        "authors": [
          "Daniel D. Sleator",
          "Davy Temperley"
        ],
        "task": "[\"Dependency Parsing\"]",
        "dataset": [],
        "metrics": [
          "Dependency Accuracy"
        ],
        "architecture": {
          "components": [
            "Link Grammar"
          ],
          "connections": [
            "Link Relations"
          ],
          "mechanisms": [
            "Constraint-Based Parsing"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Constraint-Based"
          ],
          "parameter_tuning": [
            "Link Rules"
          ]
        },
        "feature_processing": [
          "Dependency Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Socher2013_CompositionalVectorGrammar",
      "label": "Compositional Vector Grammar (CVG)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Socher2013_CompositionalVectorGrammar",
        "entity_id": "Socher2013_CompositionalVectorGrammar",
        "name": "Compositional Vector Grammar (CVG)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": "2013",
        "authors": [
          "Richard Socher",
          "John Bauer",
          "Christopher D. Manning",
          "Andrew Y. Ng"
        ],
        "task": "[\"Syntactic Parsing\"]",
        "dataset": [],
        "metrics": [
          "Labeled F1 Score"
        ],
        "architecture": {
          "components": [
            "Probabilistic Context-Free Grammar (PCFG)",
            "Recursive Neural Network (RNN)",
            "Syntactically Untied Recursive Neural Network (SU-RNN)"
          ],
          "connections": [
            "Composition Function",
            "Parent-Child Relationships"
          ],
          "mechanisms": [
            "Continuous Vector Representations",
            "Syntactic Categories"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Training Objective",
            "Backpropagation Through Structure (BTS)",
            "AdaGrad"
          ],
          "parameter_tuning": [
            "Regularization",
            "Mini-batch Size",
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Vector Representations",
          "POS Tags"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Socher2013_StandardRecursiveNeuralNetwork",
      "label": "Standard Recursive Neural Network (RNN)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Socher2013_StandardRecursiveNeuralNetwork",
        "entity_id": "Socher2013_StandardRecursiveNeuralNetwork",
        "name": "Standard Recursive Neural Network (RNN)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": "2013",
        "authors": [
          "Richard Socher",
          "John Bauer",
          "Christopher D. Manning",
          "Andrew Y. Ng"
        ],
        "task": "[\"Syntactic Parsing\"]",
        "dataset": [],
        "metrics": [
          "Labeled F1 Score"
        ],
        "architecture": {
          "components": [
            "Neural Network",
            "Composition Function"
          ],
          "connections": [
            "Parent-Child Relationships"
          ],
          "mechanisms": [
            "Continuous Vector Representations"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Training Objective"
          ],
          "parameter_tuning": [
            "Regularization"
          ]
        },
        "feature_processing": [
          "Word Vector Representations"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Socher2013_SyntacticallyUntiedRecursiveNeuralNetwork",
      "label": "Syntactically Untied Recursive Neural Network (SU-RNN)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Socher2013_SyntacticallyUntiedRecursiveNeuralNetwork",
        "entity_id": "Socher2013_SyntacticallyUntiedRecursiveNeuralNetwork",
        "name": "Syntactically Untied Recursive Neural Network (SU-RNN)",
        "title": "Parsing with Compositional Vector Grammars",
        "year": "2013",
        "authors": [
          "Richard Socher",
          "John Bauer",
          "Christopher D. Manning",
          "Andrew Y. Ng"
        ],
        "task": "[\"Syntactic Parsing\"]",
        "dataset": [],
        "metrics": [
          "Labeled F1 Score"
        ],
        "architecture": {
          "components": [
            "Neural Network",
            "Composition Function"
          ],
          "connections": [
            "Parent-Child Relationships"
          ],
          "mechanisms": [
            "Continuous Vector Representations",
            "Syntactic Categories"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Training Objective",
            "Backpropagation Through Structure (BTS)",
            "AdaGrad"
          ],
          "parameter_tuning": [
            "Regularization",
            "Mini-batch Size",
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Vector Representations",
          "POS Tags"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Sundaram2015_WordProblemSolver",
      "label": "Word Problem Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Sundaram2015_WordProblemSolver",
        "entity_id": "Sundaram2015_WordProblemSolver",
        "name": "Word Problem Solver",
        "title": "Natural Language Processing for Solving Simple Word Problems",
        "year": "2015",
        "authors": [
          "Sundaram, S. S.",
          "Khemani, D."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Simplification Phase",
            "Analysis Phase",
            "Knowledge Representation",
            "Temporal Schemas"
          ],
          "connections": [
            "Dependency Parsing",
            "Co-reference Resolution",
            "Conjunction Resolution"
          ],
          "mechanisms": [
            "Stanford CoreNLP Suite",
            "Common Sense Law of Inertia"
          ]
        },
        "methodology": {
          "training_strategy": [
            "None"
          ],
          "parameter_tuning": [
            "None"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Conjunction Resolution",
          "Co-reference Resolution",
          "Entity Resolution"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Sutskever2014_SequenceToSequence",
      "label": "Sequence-to-Sequence Learning",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Sutskever2014_SequenceToSequence",
        "entity_id": "Sutskever2014_SequenceToSequence",
        "name": "Sequence-to-Sequence Learning",
        "title": "Sequence to Sequence Learning with Neural Networks",
        "year": "2014",
        "authors": [
          "Sutskever, I.",
          "Vinyals, O.",
          "Le, Q."
        ],
        "task": "[\"Translation\"]",
        "dataset": [],
        "metrics": [
          "BLEU Score"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "Long Short-Term Memory"
          ],
          "mechanisms": [
            "Multilayered LSTM"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Backpropagation Through Time"
          ],
          "parameter_tuning": [
            "Gradient Descent"
          ]
        },
        "feature_processing": [
          "Word Embeddings"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Sutskever2014_SequenceToSequenceLearning",
      "label": "Sequence to Sequence Learning with Neural Networks",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Sutskever2014_SequenceToSequenceLearning",
        "entity_id": "Sutskever2014_SequenceToSequenceLearning",
        "name": "Sequence to Sequence Learning with Neural Networks",
        "title": "Sequence to Sequence Learning with Neural Networks",
        "year": "2014",
        "authors": [
          "Ilya Sutskever",
          "Oriol Vinyals",
          "Quoc V. Le"
        ],
        "task": "[\"Machine Translation\"]",
        "dataset": [],
        "metrics": [
          "BLEU score"
        ],
        "architecture": {
          "components": [
            "Multilayered LSTM",
            "Deep LSTM"
          ],
          "connections": [
            "Input Sequence Encoder",
            "Output Sequence Decoder"
          ],
          "mechanisms": [
            "Long Short-Term Memory (LSTM)",
            "Reversed Input Sentences"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Backpropagation",
            "Stochastic Gradient Descent"
          ],
          "parameter_tuning": [
            "Fixed Learning Rate",
            "Gradient Clipping"
          ]
        },
        "feature_processing": [
          "Word Embeddings",
          "Reversed Source Sentences"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Tao2002_RectangleBuildingExtraction",
      "label": "Rectangle Building Extraction",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Tao2002_RectangleBuildingExtraction",
        "entity_id": "Tao2002_RectangleBuildingExtraction",
        "name": "Rectangle Building Extraction",
        "title": "A New Approach to Extract Rectangle Building from Aerial Urban Images",
        "year": "2002",
        "authors": [
          "W.-B. Tao",
          "J.-W. Tian",
          "J. Liu"
        ],
        "task": "[\"Building Extraction\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "Edge Elements",
            "Linear Elements"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "UnitDep2017_UnitDependencyGraph",
      "label": "UnitDep",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "UnitDep2017_UnitDependencyGraph",
        "entity_id": "UnitDep2017_UnitDependencyGraph",
        "name": "UnitDep",
        "title": "Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving",
        "year": "2017",
        "authors": [
          "Roy",
          "Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Unit Dependency Graph",
            "Classifier"
          ],
          "connections": [
            "Rate Consistency"
          ],
          "mechanisms": [
            "Graph Likelihood Calculation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Cross-Validation"
          ],
          "parameter_tuning": [
            "Graph Parameters"
          ]
        },
        "feature_processing": [
          "Node Classification",
          "Edge Classification"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Upadhyay2016_MixedSP",
      "label": "MixedSP",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Upadhyay2016_MixedSP",
        "entity_id": "Upadhyay2016_MixedSP",
        "name": "MixedSP",
        "title": "Learning from Explicit and Implicit Supervision Jointly for Algebra Word Problems",
        "year": "2016",
        "authors": [
          "Upadhyay",
          "Chang",
          "Chang",
          "Yih"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "RankSVM",
            "Log-linear Model"
          ],
          "connections": [
            "Max-Margin Objective"
          ],
          "mechanisms": [
            "Constraint Generation Algorithm"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Joint Training"
          ],
          "parameter_tuning": [
            "Regularization Parameters"
          ]
        },
        "feature_processing": [
          "Textual Features",
          "Quantity Features"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Wang2016_DimensionallyGuidedSynthesis",
      "label": "Dimensionally Guided Synthesis",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2016_DimensionallyGuidedSynthesis",
        "entity_id": "Wang2016_DimensionallyGuidedSynthesis",
        "name": "Dimensionally Guided Synthesis",
        "title": "Dimensionally Guided Synthesis of Mathematical Word Problems",
        "year": "2016",
        "authors": [
          "Ke Wang",
          "Zhendong Su"
        ],
        "task": "[\"Math Word Problem Generation\"]",
        "dataset": [],
        "metrics": [
          "Statistical Indistinguishability",
          "Error Rate"
        ],
        "architecture": {
          "components": [
            "Equation Generator",
            "Narrative Generator"
          ],
          "connections": [
            "Binary Expression Tree",
            "Dimensional Units"
          ],
          "mechanisms": [
            "Dimensional Consistency",
            "Semantic Instantiation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Random Equation Synthesis",
            "Variable Unrolling"
          ],
          "parameter_tuning": [
            "SMT Solver",
            "Predefined Operational Rules"
          ]
        },
        "feature_processing": [
          "Keyword Assignment",
          "Numerical Value Generation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2016_RelationKnowledgePoweredModel",
      "label": "Relation Knowledge Powered Model (RK)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2016_RelationKnowledgePoweredModel",
        "entity_id": "Wang2016_RelationKnowledgePoweredModel",
        "name": "Relation Knowledge Powered Model (RK)",
        "title": "Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding",
        "year": "2016",
        "authors": [
          "Huazheng Wang",
          "Fei Tian",
          "Bin Gao",
          "Chengjieren Zhu",
          "Jiang Bian",
          "Tie-Yan Liu"
        ],
        "task": "[\"Verbal Comprehension Questions Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Question Classifier",
            "Word-Sense Pair Embedding",
            "Relation Embedding"
          ],
          "connections": [
            "Co-learning of Word-Sense Pairs and Relations"
          ],
          "mechanisms": [
            "Multi-Sense Clustering",
            "Relational Knowledge Integration"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Skip-gram",
            "Negative Sampling",
            "Back Propagation"
          ],
          "parameter_tuning": [
            "Window Size",
            "Embedding Dimension",
            "Epoch Number"
          ]
        },
        "feature_processing": [
          "TF-IDF",
          "Context Window Representation",
          "Spherical k-means Clustering"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_DeepNeuralSolver",
      "label": "Deep Neural Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_DeepNeuralSolver",
        "entity_id": "Wang2017_DeepNeuralSolver",
        "name": "Deep Neural Solver",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": "2017",
        "authors": [
          "Wang, Y.",
          "Liu, X.",
          "Shi, S."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Recurrent Neural Network (RNN)"
          ],
          "connections": [
            "Equation Templates"
          ],
          "mechanisms": [
            "Direct Translation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_HybridModel",
      "label": "Hybrid Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_HybridModel",
        "entity_id": "Wang2017_HybridModel",
        "name": "Hybrid Model",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": "2017",
        "authors": [
          "Wang, Yan",
          "Liu, Xiaojiang",
          "Shi, Shuming"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Retrieval Model"
          ],
          "connections": [
            "Jaccard Similarity"
          ],
          "mechanisms": [
            "Threshold-based Selection"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Tokenization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_MathDQN",
      "label": "MathDQN",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_MathDQN",
        "entity_id": "Wang2017_MathDQN",
        "name": "MathDQN",
        "title": "MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning",
        "year": "2017",
        "authors": [
          "Wang, L.",
          "Zhang, D.",
          "Gao, L.",
          "Song, J.",
          "Guo, L.",
          "Shen, H.T."
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Average Precision"
        ],
        "architecture": {
          "components": [
            "Deep Q-Network"
          ],
          "connections": [
            "State Representation",
            "Action Selection"
          ],
          "mechanisms": [
            "Reward Function"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Deep Reinforcement Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_RNNBasedSeq2SeqModel",
      "label": "RNN-based Seq2Seq Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_RNNBasedSeq2SeqModel",
        "entity_id": "Wang2017_RNNBasedSeq2SeqModel",
        "name": "RNN-based Seq2Seq Model",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": "2017",
        "authors": [
          "Wang, Yan",
          "Liu, Xiaojiang",
          "Shi, Shuming"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Word Embedding Layer",
            "GRU Encoder",
            "LSTM Decoder"
          ],
          "connections": [
            "GRU",
            "LSTM"
          ],
          "mechanisms": [
            "Gated Recurrent Unit",
            "Long Short-Term Memory"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Mapping",
          "Tokenization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_SemanticallyAligned",
      "label": "Semantically-Aligned Equation Generation",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_SemanticallyAligned",
        "entity_id": "Wang2017_SemanticallyAligned",
        "name": "Semantically-Aligned Equation Generation",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": "2017",
        "authors": [
          "Chiang, T.R.",
          "Chen, Y.N."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder-Decoder Framework"
          ],
          "connections": [
            "Symbolic World",
            "Semantic World"
          ],
          "mechanisms": [
            "Stack Actions"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-End Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_Seq2Seq",
      "label": "Seq2Seq Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_Seq2Seq",
        "entity_id": "Wang2017_Seq2Seq",
        "name": "Seq2Seq Model",
        "title": "Data-Driven Methods for Solving Algebra Word Problems",
        "year": "2018",
        "authors": [
          "Robaidek, Benjamin",
          "Koncel-Kedziorski, Rik",
          "Hajishirzi, Hannaneh"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "LSTM",
            "CNN"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embedding"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_Seq2SeqAttn",
      "label": "Seq2SeqAttn",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_Seq2SeqAttn",
        "entity_id": "Wang2017_Seq2SeqAttn",
        "name": "Seq2SeqAttn",
        "title": "Deep Neural Model for Math Word Problem Solving",
        "year": "2017",
        "authors": [
          "Yan Wang",
          "Xiaojiang Liu",
          "Shuming Shi"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Solution Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "Attention"
          ],
          "mechanisms": [
            "Gated Recurrent Unit (GRU)"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": [
            "SGD Optimizer"
          ]
        },
        "feature_processing": [
          "Number Tokenization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_SignificantNumberIdentification",
      "label": "Significant Number Identification (SNI)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_SignificantNumberIdentification",
        "entity_id": "Wang2017_SignificantNumberIdentification",
        "name": "Significant Number Identification (SNI)",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": "2017",
        "authors": [
          "Wang, Yan",
          "Liu, Xiaojiang",
          "Shi, Shuming"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "LSTM"
          ],
          "connections": [
            "LSTM"
          ],
          "mechanisms": [
            "Long Short-Term Memory"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Binary Classification"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Context Window"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_TemplateBased",
      "label": "Template-Based Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_TemplateBased",
        "entity_id": "Wang2017_TemplateBased",
        "name": "Template-Based Solver",
        "title": "Template-Based Math Word Problem Solvers with Recursive Neural Networks",
        "year": "2017",
        "authors": [
          "Wang, L.",
          "Zhang, D.",
          "Zhang, J.",
          "Xu, X.",
          "Gao, L.",
          "Dai, B.T.",
          "Shen, H.T."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Recursive Neural Network"
          ],
          "connections": [
            "Tree-Structure Template"
          ],
          "mechanisms": [
            "Bottom-Up Inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_TemplateBasedSolver",
      "label": "Template-Based Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_TemplateBasedSolver",
        "entity_id": "Wang2017_TemplateBasedSolver",
        "name": "Template-Based Solver",
        "title": "Template-Based Math Word Problem Solvers with Recursive Neural Networks",
        "year": "2017",
        "authors": [
          "Wang, L.",
          "Zhang, D.",
          "Zhang, J.",
          "Xu, X.",
          "Gao, L.",
          "Dai, B.T.",
          "Shen, H.T."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Recursive Neural Network"
          ],
          "connections": [
            "Tree-Structure Template"
          ],
          "mechanisms": [
            "Bottom-Up Inference"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2017_TranslationModel",
      "label": "Translation Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2017_TranslationModel",
        "entity_id": "Wang2017_TranslationModel",
        "name": "Translation Model",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": "2017",
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Expression Tree"
          ],
          "connections": [
            "Equation Normalization"
          ],
          "mechanisms": [
            "Maximum Likelihood Estimation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameters"
          ]
        },
        "feature_processing": [
          "Feature Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_BiLSTM",
      "label": "BiLSTM",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_BiLSTM",
        "entity_id": "Wang2018_BiLSTM",
        "name": "BiLSTM",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": "2018",
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Bidirectional LSTM"
          ],
          "connections": [
            "Global Attention Mechanism"
          ],
          "mechanisms": [
            "LSTM Cells"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Adam Optimizer",
            "Beam Search"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Equation Template Transformation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_CASS",
      "label": "CASS",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_CASS",
        "entity_id": "Wang2018_CASS",
        "name": "CASS",
        "title": "Neural Math Word Problem Solver with Reinforcement Learning",
        "year": "2018",
        "authors": [
          "Huang, D.",
          "Liu, J.",
          "Lin, C.",
          "Yin, J."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Copy Mechanism",
            "Alignment Mechanism",
            "Sequence-to-Sequence Model"
          ],
          "connections": [
            "Reinforcement Learning"
          ],
          "mechanisms": [
            "Deep Q-Network"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Reinforcement Learning"
          ],
          "parameter_tuning": [
            "Deep Q-Network"
          ]
        },
        "feature_processing": [
          "Copy Mechanism",
          "Alignment Mechanism"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_ConvS2S",
      "label": "ConvS2S",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_ConvS2S",
        "entity_id": "Wang2018_ConvS2S",
        "name": "ConvS2S",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": "2018",
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Convolutional Layers"
          ],
          "connections": [
            "Gate Linear Units"
          ],
          "mechanisms": [
            "Convolutional Architecture"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Early Stopping",
            "Learning Rate Annealing"
          ],
          "parameter_tuning": [
            "Max-Epochs",
            "Hidden Size"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Equation Template Transformation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_EnsembleModel",
      "label": "Ensemble Model",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_EnsembleModel",
        "entity_id": "Wang2018_EnsembleModel",
        "name": "Ensemble Model",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": "2018",
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "BiLSTM",
            "ConvS2S",
            "Transformer"
          ],
          "connections": [
            "Generation Probability Selection"
          ],
          "mechanisms": [
            "Model Combination"
          ]
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Number Tokenization",
          "Equation Template Transformation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_EquationNormalization",
      "label": "Equation Normalization",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_EquationNormalization",
        "entity_id": "Wang2018_EquationNormalization",
        "name": "Equation Normalization",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": "2018",
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Expression Tree"
          ],
          "connections": [
            "Normalization Rules"
          ],
          "mechanisms": [
            "Order Duplication Handling",
            "Bracket Duplication Handling"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Maximum Likelihood Estimation"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [
          "Number Tokenization",
          "Equation Template Transformation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_MathDQN",
      "label": "MathDQN",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_MathDQN",
        "entity_id": "Wang2018_MathDQN",
        "name": "MathDQN",
        "title": "MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning",
        "year": "2018",
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Lianli Gao",
          "Jingkuan Song",
          "Long Guo",
          "Heng Tao Shen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Deep Q-Network",
            "Feed-Forward Neural Network"
          ],
          "connections": [
            "Two-layer Feed-Forward Neural Network"
          ],
          "mechanisms": [
            "Reinforcement Learning",
            "Deep Q-Learning"
          ]
        },
        "methodology": {
          "training_strategy": [
            "ε-greedy Strategy",
            "Mini-Batch Gradient Descent",
            "Experience Replay"
          ],
          "parameter_tuning": [
            "Discount Factor γ=0.9",
            "Learning Rate 0.0001",
            "Replay Memory Size 15,000"
          ]
        },
        "feature_processing": [
          "Quantity Schema Extraction",
          "Feature Concatenation",
          "Re-order Mechanism"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_Transformer",
      "label": "Transformer",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_Transformer",
        "entity_id": "Wang2018_Transformer",
        "name": "Transformer",
        "title": "Translating a Math Word Problem to an Expression Tree",
        "year": "2018",
        "authors": [
          "Lei Wang",
          "Yan Wang",
          "Deng Cai",
          "Dongxiang Zhang",
          "Xiaojiang Liu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Multi-Head Self-Attention Module",
            "Position-Wise Fully-Connected Feed-Forward Network"
          ],
          "connections": [
            "Self-Attention"
          ],
          "mechanisms": [
            "Attention Mechanism"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Adam Optimizer",
            "Dropout"
          ],
          "parameter_tuning": [
            "Learning Rate",
            "Number of Heads",
            "Dimension of Keys",
            "Dimension of Values",
            "Output Dimension"
          ]
        },
        "feature_processing": [
          "Number Tokenization",
          "Equation Template Transformation"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2018_TranslationToExpressionTree",
      "label": "Translation to Expression Tree",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2018_TranslationToExpressionTree",
        "entity_id": "Wang2018_TranslationToExpressionTree",
        "name": "Translation to Expression Tree",
        "title": "Translating Math Word Problem to Expression Tree",
        "year": "2018",
        "authors": [
          "L. Wang",
          "Y. Wang",
          "D. Cai",
          "D. Zhang",
          "X. Liu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder",
            "Decoder"
          ],
          "connections": [
            "Beam Search"
          ],
          "mechanisms": [
            "Recurrent Neural Network"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss"
          ],
          "parameter_tuning": [
            "Adam"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Dependency Parsing"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Wang2019_SemanticallyAligned",
      "label": "Semantically-Aligned Equation Generation",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2019_SemanticallyAligned",
        "entity_id": "Wang2019_SemanticallyAligned",
        "name": "Semantically-Aligned Equation Generation",
        "title": "Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems",
        "year": "2019",
        "authors": [
          "Ting-Rui Chiang",
          "Yun-Nung Chen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Encoder-Decoder Framework"
          ],
          "connections": [
            "Symbolic World",
            "Semantic World"
          ],
          "mechanisms": [
            "Stack Actions"
          ]
        },
        "methodology": {
          "training_strategy": [
            "End-to-End Training"
          ],
          "parameter_tuning": [
            "Hyperparameters"
          ]
        },
        "feature_processing": [
          "Feature Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2019_TemplateBased",
      "label": "Template-Based Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2019_TemplateBased",
        "entity_id": "Wang2019_TemplateBased",
        "name": "Template-Based Solver",
        "title": "Template-Based Math Word Problem Solvers with Recursive Neural Networks",
        "year": "2019",
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Jipeng Zhang",
          "Xing Xu",
          "Lianli Gao",
          "Bing Tian Dai",
          "Heng Tao Shen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Tree-Structure Template"
          ],
          "connections": [
            "Recursive Neural Network"
          ],
          "mechanisms": [
            "Bi-LSTM",
            "Self Attention"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameters"
          ]
        },
        "feature_processing": [
          "Feature Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wang2019_TemplateBasedSolver",
      "label": "Template-Based Math Word Problem Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wang2019_TemplateBasedSolver",
        "entity_id": "Wang2019_TemplateBasedSolver",
        "name": "Template-Based Math Word Problem Solver",
        "title": "Template-Based Math Word Problem Solvers with Recursive Neural Networks",
        "year": "2019",
        "authors": [
          "Lei Wang",
          "Dongxiang Zhang",
          "Jipeng Zhang",
          "Xing Xu",
          "Lianli Gao",
          "Bing Tian Dai",
          "Heng Tao Shen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Seq2Seq Model",
            "Bi-LSTM",
            "Self Attention",
            "Recursive Neural Network"
          ],
          "connections": [
            "Attention Layer",
            "Recursive Connections"
          ],
          "mechanisms": [
            "Operator Encapsulation",
            "Equation Normalization"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Adam Optimizer",
            "SGD Optimizer"
          ],
          "parameter_tuning": [
            "Dropout",
            "Batch Size",
            "Learning Rate"
          ]
        },
        "feature_processing": [
          "Word Embedding",
          "Quantity Embedding",
          "Suffix Expression Serialization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Watanabe1991_PBFDiagramUnderstandingFramework",
      "label": "PBF Diagram Understanding Framework",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Watanabe1991_PBFDiagramUnderstandingFramework",
        "entity_id": "Watanabe1991_PBFDiagramUnderstandingFramework",
        "name": "PBF Diagram Understanding Framework",
        "title": "Diagram Understanding Using Integration of Layout Information and Textual Information",
        "year": "1991",
        "authors": [
          "Yasuhiko Watanabe",
          "Makoto Nagao"
        ],
        "task": "[\"Diagram Understanding\"]",
        "dataset": [],
        "metrics": [
          "Accuracy",
          "Success Rate"
        ],
        "architecture": {
          "components": [
            "Layout Information Extraction",
            "Natural Language Information Extraction"
          ],
          "connections": [
            "Integration of Layout and Natural Language Information"
          ],
          "mechanisms": [
            "Semantic Interpretation Rules"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Pattern Matching",
            "Japanese Morphological Analysis"
          ],
          "parameter_tuning": [
            "Rule-based Classification"
          ]
        },
        "feature_processing": [
          "Symbol Detection",
          "Spatial Relationship Analysis",
          "Expression Pattern Matching"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Wiseman2016_BeamSearchOptimization",
      "label": "Beam Search Optimization (BSO)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Wiseman2016_BeamSearchOptimization",
        "entity_id": "Wiseman2016_BeamSearchOptimization",
        "name": "Beam Search Optimization (BSO)",
        "title": "Sequence-to-Sequence Learning as Beam-Search Optimization",
        "year": "2016",
        "authors": [
          "Sam Wiseman",
          "Alexander M. Rush"
        ],
        "task": "[\"Word Ordering\", \"Dependency Parsing\", \"Machine Translation\"]",
        "dataset": [],
        "metrics": [
          "BLEU",
          "UAS",
          "LAS"
        ],
        "architecture": {
          "components": [
            "LSTM Encoder",
            "LSTM Decoder",
            "Global Attention Model"
          ],
          "connections": [
            "Attention Mechanism",
            "Input Feeding"
          ],
          "mechanisms": [
            "Beam Search",
            "LaSO Framework"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Beam Search Training",
            "Curriculum Beam Strategy",
            "Pre-training with Cross-Entropy Loss"
          ],
          "parameter_tuning": [
            "Adagrad",
            "Gradient Clipping",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Word Embeddings Initialization",
          "Singleton Words Replacement",
          "Digit Normalization"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Yu2015_ChineseEquationSet",
      "label": "Chinese Equation Set Problem Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Yu2015_ChineseEquationSet",
        "entity_id": "Yu2015_ChineseEquationSet",
        "name": "Chinese Equation Set Problem Solver",
        "title": "Solving Equation Set Problems in Chinese",
        "year": "2015",
        "authors": [
          "Yu",
          "Wang",
          "Zeng",
          "Fan"
        ],
        "task": "[\"Equation Set Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Syntax-Semantics Model"
          ],
          "connections": [
            "Keyword Structure",
            "POS Pattern"
          ],
          "mechanisms": [
            "Quantity Relation Extraction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Hyperparameter Tuning"
          ]
        },
        "feature_processing": [
          "Keyword Structure",
          "POS Tagging"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Yu2015_ChineseEquationSetSolver",
      "label": "ChineseEquationSetSolver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Yu2015_ChineseEquationSetSolver",
        "entity_id": "Yu2015_ChineseEquationSetSolver",
        "name": "ChineseEquationSetSolver",
        "title": "Solving Directly-Stated Arithmetic Word Problems in Chinese",
        "year": "2015",
        "authors": [
          "Yu",
          "Wang",
          "Zeng",
          "Fan"
        ],
        "task": "[\"Chinese Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Syntax-Semantics Model",
            "Keyword Structure"
          ],
          "connections": [
            "Pattern Matching"
          ],
          "mechanisms": [
            "POS Tagging"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Supervised Learning"
          ],
          "parameter_tuning": [
            "Regularization Parameters"
          ]
        },
        "feature_processing": [
          "POS Tagging",
          "Quantity Relation Extraction"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Yu2016_ImplicitQuantityRelationExtractor",
      "label": "Implicit Quantity Relation Extractor",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Yu2016_ImplicitQuantityRelationExtractor",
        "entity_id": "Yu2016_ImplicitQuantityRelationExtractor",
        "name": "Implicit Quantity Relation Extractor",
        "title": "Extraction of Implicit Quantity Relations for Arithmetic Word Problems in Chinese",
        "year": "2016",
        "authors": [
          "Xinguo Yu",
          "Pengpeng Jian",
          "Mingshu Wang",
          "Shuang Wu"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Classification Accuracy"
        ],
        "architecture": {
          "components": [
            "Chinese Phrase Parser",
            "SVM Classifier",
            "Instantiation Method"
          ],
          "connections": [
            "Semantic Models"
          ],
          "mechanisms": [
            "Sequence Alignment",
            "Map List Construction"
          ]
        },
        "methodology": {
          "training_strategy": [
            "SVM with Slack Variable",
            "Bag of Words"
          ],
          "parameter_tuning": [
            "Lagrange Multiplier",
            "Weight Number for Slack Variable"
          ]
        },
        "feature_processing": [
          "Chinese Phrase Parsing",
          "Normalization of Common Units"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Zhang2015_ExpressionTree",
      "label": "ExpressionTree",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Zhang2015_ExpressionTree",
        "entity_id": "Zhang2015_ExpressionTree",
        "name": "ExpressionTree",
        "title": "The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers",
        "year": "2015",
        "authors": [
          "Roy",
          "Roth"
        ],
        "task": "[\"Arithmetic Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Binary Classifier",
            "Expression Tree"
          ],
          "connections": [
            "Beam Search"
          ],
          "mechanisms": [
            "Local Classifier"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Binary Classification"
          ],
          "parameter_tuning": [
            "Beam Search"
          ]
        },
        "feature_processing": [
          "Quantity Extraction",
          "Verb Categorization"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Zhang2017_DeepNeuralSolver",
      "label": "Deep Neural Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Zhang2017_DeepNeuralSolver",
        "entity_id": "Zhang2017_DeepNeuralSolver",
        "name": "Deep Neural Solver",
        "title": "Deep Neural Solver for Math Word Problems",
        "year": "2017",
        "authors": [
          "Wang, Y.",
          "Liu, X.",
          "Shi, S."
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "GRU",
            "Word Embedding"
          ],
          "connections": [
            "Seq2Seq"
          ],
          "mechanisms": [
            "Attention"
          ]
        },
        "methodology": {
          "training_strategy": [
            "CrossEntropyLoss",
            "Normalization"
          ],
          "parameter_tuning": [
            "Adam",
            "Dropout"
          ]
        },
        "feature_processing": [
          "Tokenization",
          "Word Embedding"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Zhou2015_EnhancedTemplate",
      "label": "EnhancedTemplate",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Zhou2015_EnhancedTemplate",
        "entity_id": "Zhou2015_EnhancedTemplate",
        "name": "EnhancedTemplate",
        "title": "Enhanced Algorithm for Template-Based Learning Framework",
        "year": "2015",
        "authors": [
          "Zhou",
          "et al."
        ],
        "task": "[\"Algebra Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Template Matching",
            "RankSVM"
          ],
          "connections": [
            "Canonicalized Ordering"
          ],
          "mechanisms": [
            "Beam Search"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-Margin Objective"
          ],
          "parameter_tuning": [
            "Beam Size"
          ]
        },
        "feature_processing": [
          "Number Slot Assignment",
          "Unknown Slot Alignment"
        ],
        "entity_type": "Algorithm",
        "source": "综述"
      }
    },
    {
      "id": "Zhou2015_QuadraticProgramming",
      "label": "Quadratic Programming (QP)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Zhou2015_QuadraticProgramming",
        "entity_id": "Zhou2015_QuadraticProgramming",
        "name": "Quadratic Programming (QP)",
        "title": "Learn to Solve Algebra Word Problems Using Quadratic Programming",
        "year": "2015",
        "authors": [
          "Zhou, L.",
          "Dai, S.",
          "Chen, L."
        ],
        "task": "[\"Algebra Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Log-linear Model"
          ],
          "connections": [
            "Equation Templates"
          ],
          "mechanisms": [
            "Robust Decision Surface"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Quadratic Programming"
          ],
          "parameter_tuning": [
            "liblinear"
          ]
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Zhou2015_QuadraticProgrammingSolver",
      "label": "Quadratic Programming Solver",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Zhou2015_QuadraticProgrammingSolver",
        "entity_id": "Zhou2015_QuadraticProgrammingSolver",
        "name": "Quadratic Programming Solver",
        "title": "Learn to Solve Algebra Word Problems Using Quadratic Programming",
        "year": "2015",
        "authors": [
          "Lipu Zhou",
          "Shuaixiang Dai",
          "Liwei Chen"
        ],
        "task": "[\"Algebra Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Log-linear Model",
            "Quadratic Programming"
          ],
          "connections": [
            "Max-margin Objective"
          ],
          "mechanisms": [
            "Constraint Generation"
          ]
        },
        "methodology": {
          "training_strategy": [
            "Max-margin Objective",
            "Quadratic Programming"
          ],
          "parameter_tuning": [
            "C Parameter"
          ]
        },
        "feature_processing": [
          "Single Slot Features",
          "Slot Pair Features",
          "Solution Features"
        ],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Zhou2015_ZDC",
      "label": "ZDC",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Zhou2015_ZDC",
        "entity_id": "Zhou2015_ZDC",
        "name": "ZDC",
        "title": "Learn to Solve Algebra Word Problems Using Quadratic Programming",
        "year": "2015",
        "authors": [
          "Zhou",
          "Dai",
          "Chen"
        ],
        "task": "[\"Math Word Problem Solving\"]",
        "dataset": [],
        "metrics": [
          "Accuracy"
        ],
        "architecture": {
          "components": [
            "Improved Template-based Statistical Learning"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [
            "Reduced Search Space"
          ],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Zhu2003_RectangularHoughTransform",
      "label": "Rectangular Hough Transform (RHT)",
      "type": "Algorithm",
      "entity_type": "Algorithm",
      "data": {
        "algorithm_id": "Zhu2003_RectangularHoughTransform",
        "entity_id": "Zhu2003_RectangularHoughTransform",
        "name": "Rectangular Hough Transform (RHT)",
        "title": "Automatic Particle Detection through Efficient Hough Transforms",
        "year": "2003",
        "authors": [
          "Y. Zhu",
          "B. Carragher",
          "F. Mouche",
          "C. Potter"
        ],
        "task": "[\"Particle Detection\"]",
        "dataset": [],
        "metrics": [],
        "architecture": {
          "components": [
            "2D Accumulator Array"
          ],
          "connections": [],
          "mechanisms": []
        },
        "methodology": {
          "training_strategy": [],
          "parameter_tuning": []
        },
        "feature_processing": [],
        "entity_type": "Algorithm",
        "source": "引文"
      }
    },
    {
      "id": "Abstract_Scenes_2015",
      "label": "Abstract Scenes",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Abstract_Scenes_2015",
        "name": "Abstract Scenes",
        "description": "Dataset of abstract scenes for VQA",
        "domain": "Computer Vision",
        "size": 50000,
        "year": "2015",
        "creators": "[\"Stanislaw Antol\", \"Aishwarya Agrawal\", \"Jiasen Lu\", \"Margaret Mitchell\", \"Dhruv Batra\", \"C. Lawrence Zitnick\", \"Devi Parikh\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Abstract_Scenes_2015"
      }
    },
    {
      "id": "ACE2004",
      "label": "ACE 2004",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004",
        "name": "ACE 2004",
        "description": "A dataset for coreference resolution containing 443 documents.",
        "domain": "Natural Language Processing",
        "size": 443,
        "year": "2004",
        "creators": "[\"NIST\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004"
      }
    },
    {
      "id": "ACE2004_2004",
      "label": "ACE 2004 English training data",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004_2004",
        "name": "ACE 2004 English training data",
        "description": "A dataset used for coreference resolution tasks",
        "domain": "Natural Language Processing",
        "size": 336,
        "year": "2004",
        "creators": "[\"NIST\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004_2004"
      }
    },
    {
      "id": "ACE2004_NWIRE_2004",
      "label": "ACE 2004 Newswire",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004_NWIRE_2004",
        "name": "ACE 2004 Newswire",
        "description": "Newswire subset of the ACE 2004 corpus",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": "2004",
        "creators": "[\"NIST\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004_NWIRE_2004"
      }
    },
    {
      "id": "ACE2004-CULOTTA-TEST_2004",
      "label": "ACE2004-CULOTTA-TEST",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004-CULOTTA-TEST_2004",
        "name": "ACE2004-CULOTTA-TEST",
        "description": "Partition of ACE 2004 corpus reserved for testing by several previous works",
        "domain": "Natural Language Processing",
        "size": 107,
        "year": "2004",
        "creators": "[\"Culotta et al.\", \"Bengston and Roth\", \"Haghighi and Klein\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004-CULOTTA-TEST_2004"
      }
    },
    {
      "id": "ACE2004-CULOTTA-TEST_2009",
      "label": "ACE2004-CULOTTA-TEST",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004-CULOTTA-TEST_2009",
        "name": "ACE2004-CULOTTA-TEST",
        "description": "Test set split of the ACE 2004 training set",
        "domain": "Natural Language Processing",
        "size": 107,
        "year": "2009",
        "creators": "[\"Culotta\", \"Bengston\", \"Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004-CULOTTA-TEST_2009"
      }
    },
    {
      "id": "ACE2004-NWIRE_2004",
      "label": "ACE2004-NWIRE",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004-NWIRE_2004",
        "name": "ACE2004-NWIRE",
        "description": "Newswire subset of the ACE 2004 corpus",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": "2004",
        "creators": "[\"Poon and Domingos\", \"Haghighi and Klein\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004-NWIRE_2004"
      }
    },
    {
      "id": "ACE2004-NWIRE_2009",
      "label": "ACE2004-NWIRE",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004-NWIRE_2009",
        "name": "ACE2004-NWIRE",
        "description": "ACE 2004 Newswire set",
        "domain": "Natural Language Processing",
        "size": 128,
        "year": "2009",
        "creators": "[\"Poon\", \"Domingos\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004-NWIRE_2009"
      }
    },
    {
      "id": "ACE2004-ROTH-DEV_2009",
      "label": "ACE2004-ROTH-DEV",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004-ROTH-DEV_2009",
        "name": "ACE2004-ROTH-DEV",
        "description": "Development set split of the ACE 2004 training set",
        "domain": "Natural Language Processing",
        "size": 68,
        "year": "2009",
        "creators": "[\"Bengston\", \"Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004-ROTH-DEV_2009"
      }
    },
    {
      "id": "ACE2004-ROTH-DEV2_2004",
      "label": "ACE2004-ROTH-DEV2",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ACE2004-ROTH-DEV2_2004",
        "name": "ACE2004-ROTH-DEV2",
        "description": "Development split of Bengston and Roth(2008) from the 2004 Automatic Content Extraction (ACE) evaluation",
        "domain": "Natural Language Processing",
        "size": 68,
        "year": "2004",
        "creators": "[\"Bengston and Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ACE2004-ROTH-DEV2_2004"
      }
    },
    {
      "id": "AddSub_2014",
      "label": "ADDSUB",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AddSub_2014",
        "name": "ADDSUB",
        "description": "Addition and subtraction word problems with irrelevant distractor quantities",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2014",
        "creators": "[\"Hosseini, M. J.\", \"Hajishirzi, H.\", \"Etzioni, O.\", \"Kushman, N.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AddSub_2014"
      }
    },
    {
      "id": "Age_Dataset_2017",
      "label": "Age Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Age_Dataset_2017",
        "name": "Age Dataset",
        "description": "Twitter tweets in English, Spanish, and Dutch with age and gender labels",
        "domain": "Natural Language Processing",
        "size": 76485,
        "year": "2017",
        "creators": "[\"PAN Conference\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Age_Dataset_2017"
      }
    },
    {
      "id": "Aggregate_2018",
      "label": "Aggregate",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Aggregate_2018",
        "name": "Aggregate",
        "description": "Combined dataset of AllArith and Perturb",
        "domain": "Mathematics",
        "size": 1492,
        "year": "2018",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Aggregate_2018"
      }
    },
    {
      "id": "AI2_2014",
      "label": "AI2",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AI2_2014",
        "name": "AI2",
        "description": "Single-step and multi-step arithmetic word problems involving addition and subtraction",
        "domain": "Arithmetic Word Problems",
        "size": 395,
        "year": "2014",
        "creators": "[\"Hosseini et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AI2_2014"
      }
    },
    {
      "id": "AI2_Dataset_2014",
      "label": "AI2 Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AI2_Dataset_2014",
        "name": "AI2 Dataset",
        "description": "A collection of 395 addition and subtraction problems",
        "domain": "Arithmetic Word Problems",
        "size": 395,
        "year": "2014",
        "creators": "[\"M. J. Hosseini\", \"H. Hajishirzi\", \"O. Etzioni\", \"N. Kushman\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AI2_Dataset_2014"
      }
    },
    {
      "id": "ALG514_2014",
      "label": "Alg514",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ALG514_2014",
        "name": "Alg514",
        "description": "A dataset of 514 linear algebra problems",
        "domain": "Natural Language Processing",
        "size": 514,
        "year": "2014",
        "creators": "[\"Kushman, Nate\", \"Artzi, Yoav\", \"Zettlemoyer, Luke\", \"Barzilay, Regina\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ALG514_2014"
      }
    },
    {
      "id": "Algebra_com_2015",
      "label": "Algebra.com",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Algebra_com_2015",
        "name": "Algebra.com",
        "description": "A website for users to post math problems and get help from tutors",
        "domain": "Mathematics",
        "size": 1878,
        "year": "2015",
        "creators": "[\"Various Contributors\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Algebra_com_2015"
      }
    },
    {
      "id": "Algebra.com_Dataset_2014",
      "label": "Algebra.com Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Algebra.com_Dataset_2014",
        "name": "Algebra.com Dataset",
        "description": "A dataset of algebra word problems collected from Algebra.com",
        "domain": "Natural Language Processing, Algebra",
        "size": 514,
        "year": "2014",
        "creators": "[\"Nate Kushman\", \"Yoav Artzi\", \"Luke Zettlemoyer\", \"Regina Barzilay\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Algebra.com_Dataset_2014"
      }
    },
    {
      "id": "AllArith_2016",
      "label": "AllArith",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AllArith_2016",
        "name": "AllArith",
        "description": "Mixed dataset of arithmetic word problems",
        "domain": "Mathematics",
        "size": 831,
        "year": "2016",
        "creators": "[\"UnitDep\"]",
        "entity_type": "Dataset",
        "task_id": "e6a5c6e0-87b5-49cf-a7c1-5ae80b441cb7",
        "source": "综述",
        "entity_id": "AllArith_2016"
      }
    },
    {
      "id": "AllArith_2017",
      "label": "AllArith",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AllArith_2017",
        "name": "AllArith",
        "description": "A comprehensive dataset of arithmetic word problems",
        "domain": "Mathematics",
        "size": 831,
        "year": "2017",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AllArith_2017"
      }
    },
    {
      "id": "AllArith_2018",
      "label": "AllArith",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AllArith_2018",
        "name": "AllArith",
        "description": "Arithmetic word problem dataset",
        "domain": "Mathematics",
        "size": 831,
        "year": "2018",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AllArith_2018"
      }
    },
    {
      "id": "AllArithLex_2017",
      "label": "AllArithLex",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AllArithLex_2017",
        "name": "AllArithLex",
        "description": "Subset of AllArith with low lexical overlap",
        "domain": "Mathematics",
        "size": 415,
        "year": "2017",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AllArithLex_2017"
      }
    },
    {
      "id": "AllArithLex_2018",
      "label": "AllArithLex",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AllArithLex_2018",
        "name": "AllArithLex",
        "description": "Subset of AllArith for testing robustness to new vocabulary",
        "domain": "Mathematics",
        "size": 831,
        "year": "2018",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AllArithLex_2018"
      }
    },
    {
      "id": "AllArithTmpl_2017",
      "label": "AllArithTmpl",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AllArithTmpl_2017",
        "name": "AllArithTmpl",
        "description": "Subset of AllArith with low template overlap",
        "domain": "Mathematics",
        "size": 415,
        "year": "2017",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AllArithTmpl_2017"
      }
    },
    {
      "id": "AllArithTmpl_2018",
      "label": "AllArithTmpl",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "AllArithTmpl_2018",
        "name": "AllArithTmpl",
        "description": "Subset of AllArith for testing robustness to new equation forms",
        "domain": "Mathematics",
        "size": 831,
        "year": "2018",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AllArithTmpl_2018"
      }
    },
    {
      "id": "ArithM_2018",
      "label": "ArithM",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ArithM_2018",
        "name": "ArithM",
        "description": "Subset of multi-step arithmetic problems involving at least two operators",
        "domain": "Arithmetic Word Problems",
        "size": 667,
        "year": "2018",
        "creators": "[\"Wang et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ArithM_2018"
      }
    },
    {
      "id": "ArithmeticWordProblems_2015",
      "label": "Arithmetic Word Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ArithmeticWordProblems_2015",
        "name": "Arithmetic Word Problems",
        "description": "A dataset of arithmetic word problems",
        "domain": "Arithmetic",
        "size": 1000,
        "year": "2015",
        "creators": "[\"Roy, S.\", \"Roth, D.\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "ArithmeticWordProblems_2015"
      }
    },
    {
      "id": "ArithS_2018",
      "label": "ArithS",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ArithS_2018",
        "name": "ArithS",
        "description": "Subset of single-step arithmetic problems involving only one operator",
        "domain": "Arithmetic Word Problems",
        "size": 890,
        "year": "2018",
        "creators": "[\"Wang et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ArithS_2018"
      }
    },
    {
      "id": "BLIPP_2009",
      "label": "BLIPP",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "BLIPP_2009",
        "name": "BLIPP",
        "description": "1.8 million sentences of newswire parsed with the Charniak parser",
        "domain": "Natural Language Processing",
        "size": 1800000,
        "year": "2009",
        "creators": "[\"Charniak\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "BLIPP_2009"
      }
    },
    {
      "id": "Caltech101_2004",
      "label": "Caltech101",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Caltech101_2004",
        "name": "Caltech101",
        "description": "一个包含101个类别的小规模图像数据集。",
        "domain": "计算机视觉",
        "size": 9146,
        "year": "2004",
        "creators": "[\"Fei-Fei, L.\", \"Fergus, R.\", \"Perona, P.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Caltech101_2004"
      }
    },
    {
      "id": "Caltech256_2007",
      "label": "Caltech256",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Caltech256_2007",
        "name": "Caltech256",
        "description": "一个包含256个类别的图像数据集。",
        "domain": "计算机视觉",
        "size": 30607,
        "year": "2007",
        "creators": "[\"Griffin, G.\", \"Holub, A.\", \"Perona, P.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Caltech256_2007"
      }
    },
    {
      "id": "CC_2014",
      "label": "CC",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "CC_2014",
        "name": "CC",
        "description": "Multi-step math problems",
        "domain": "Mathematics",
        "size": 600,
        "year": "2014",
        "creators": "[\"Roy, S.\", \"Roth, D.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "综述",
        "entity_id": "CC_2014"
      }
    },
    {
      "id": "CC_2015",
      "label": "CC",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "CC_2015",
        "name": "CC",
        "description": "Multi-step problems without irrelevant quantities, involving combinations of four types of operators",
        "domain": "Arithmetic Word Problems",
        "size": 600,
        "year": "2015",
        "creators": "[\"Roy and Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CC_2015"
      }
    },
    {
      "id": "Chinese_Penn_Treebank_2014",
      "label": "Chinese Penn Treebank",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Chinese_Penn_Treebank_2014",
        "name": "Chinese Penn Treebank",
        "description": "A dataset for Chinese syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 16091,
        "year": "2014",
        "creators": "[\"Zhang and Clark\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Chinese_Penn_Treebank_2014"
      }
    },
    {
      "id": "ChineseElementarySchoolWordProblems_2010",
      "label": "Chinese Elementary School Word Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ChineseElementarySchoolWordProblems_2010",
        "name": "Chinese Elementary School Word Problems",
        "description": "Word problems collected from Chinese elementary school textbooks",
        "domain": "Education",
        "size": 0,
        "year": "2010",
        "creators": "[\"People’s Education Press\", \"Beijing Normal University Press\", \"DONGBEI Normal University Press\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ChineseElementarySchoolWordProblems_2010"
      }
    },
    {
      "id": "COCO_2014",
      "label": "COCO",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "COCO_2014",
        "name": "COCO",
        "description": "Common Objects in Context dataset",
        "domain": "Computer Vision",
        "size": 204000,
        "year": "2014",
        "creators": "[\"T.-Y. Lin\", \"M. Maire\", \"S. Belongie\", \"J. Hays\", \"P. Perona\", \"D. Ramanan\", \"P. Dollar\", \"C. L. Zitnick\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "COCO_2014"
      }
    },
    {
      "id": "CollegeLevelPhysicsTextbooks_1988",
      "label": "College-Level Physics Textbooks",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "CollegeLevelPhysicsTextbooks_1988",
        "name": "College-Level Physics Textbooks",
        "description": "Collections of ready-made test cases in the form of college-level textbooks",
        "domain": "Physics",
        "size": 0,
        "year": "1988",
        "creators": "[]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CollegeLevelPhysicsTextbooks_1988"
      }
    },
    {
      "id": "Commoncore_Dataset_2016",
      "label": "Commoncore Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Commoncore_Dataset_2016",
        "name": "Commoncore Dataset",
        "description": "A new dataset of multi-step arithmetic problems",
        "domain": "Arithmetic Word Problems",
        "size": 600,
        "year": "2016",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Commoncore_Dataset_2016"
      }
    },
    {
      "id": "CoNLL_2007_English_dataset_2010",
      "label": "CoNLL 2007 English dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "CoNLL_2007_English_dataset_2010",
        "name": "CoNLL 2007 English dataset",
        "description": "English dataset from the CoNLL 2007 shared task",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2010",
        "creators": "[\"Various contributors\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CoNLL_2007_English_dataset_2010"
      }
    },
    {
      "id": "CONLL2011_2011",
      "label": "CoNLL 2011",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "CONLL2011_2011",
        "name": "CoNLL 2011",
        "description": "Coreference dataset with text from five different domains",
        "domain": "Natural Language Processing",
        "size": 625,
        "year": "2011",
        "creators": "[\"Pradhan et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CONLL2011_2011"
      }
    },
    {
      "id": "CrawledCorpora_2014",
      "label": "Crawled Corpora",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "CrawledCorpora_2014",
        "name": "Crawled Corpora",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 870000000,
        "year": "2014",
        "creators": "[\"Philipp Koehn\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CrawledCorpora_2014"
      }
    },
    {
      "id": "Custom_Word_Problems_2023",
      "label": "Custom Word Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Custom_Word_Problems_2023",
        "name": "Custom Word Problems",
        "description": "A collection of multi-step arithmetic word problems with extraneous information.",
        "domain": "Arithmetic Problem Solving",
        "size": 0,
        "year": "2023",
        "creators": "[\"Bakman, Y.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Custom_Word_Problems_2023"
      }
    },
    {
      "id": "Dolphin_S_2017",
      "label": "Dolphin-S",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Dolphin_S_2017",
        "name": "Dolphin-S",
        "description": "Subset of Dolphin18K with one or multiple equations",
        "domain": "Mathematics",
        "size": 7070,
        "year": "2017",
        "creators": "[\"Authors of the paper\"]",
        "entity_type": "Dataset",
        "task_id": "94afd1a4-87cf-461f-bf45-7e59af553b6e",
        "source": "综述",
        "entity_id": "Dolphin_S_2017"
      }
    },
    {
      "id": "Dolphin-S_2017",
      "label": "Dolphin-S",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Dolphin-S_2017",
        "name": "Dolphin-S",
        "description": "Subset of Dolphin18K with single and multi-step operations",
        "domain": "Mathematics",
        "size": 7070,
        "year": "2017",
        "creators": "[\"Huang\"]",
        "entity_type": "Dataset",
        "task_id": "e6a5c6e0-87b5-49cf-a7c1-5ae80b441cb7",
        "source": "综述",
        "entity_id": "Dolphin-S_2017"
      }
    },
    {
      "id": "Dolphin1878_2015",
      "label": "Dolphin1878",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Dolphin1878_2015",
        "name": "Dolphin1878",
        "description": "A dataset of 1,878 number word problems",
        "domain": "Mathematics",
        "size": 1878,
        "year": "2015",
        "creators": "[\"Shi\", \"Wang\", \"Lin\", \"Liu\", \"Rui\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Dolphin1878_2015"
      }
    },
    {
      "id": "Dolphin18K_2016",
      "label": "Dolphin18K",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Dolphin18K_2016",
        "name": "Dolphin18K",
        "description": "A large-scale dataset of 18,460 annotated math word problems",
        "domain": "Mathematics",
        "size": 18460,
        "year": "2016",
        "creators": "[\"Huang\", \"Shi\", \"Lin\", \"Yin\", \"Ma\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Dolphin18K_2016"
      }
    },
    {
      "id": "DRAW_2015",
      "label": "DRAW",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "DRAW_2015",
        "name": "DRAW",
        "description": "A diverse algebra word problem set",
        "domain": "Natural Language Processing",
        "size": 1000,
        "year": "2015",
        "creators": "[\"Upadhyay, Shyam\", \"Chang, Ming-Wei\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "DRAW_2015"
      }
    },
    {
      "id": "DRAW1K_2016",
      "label": "DRAW1K",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "DRAW1K_2016",
        "name": "DRAW1K",
        "description": "Dataset for equation set problems",
        "domain": "Mathematics",
        "size": 1000,
        "year": "2016",
        "creators": "[\"Upadhyay\", \"Chang\"]",
        "entity_type": "Dataset",
        "task_id": "94afd1a4-87cf-461f-bf45-7e59af553b6e",
        "source": "综述",
        "entity_id": "DRAW1K_2016"
      }
    },
    {
      "id": "DRAW1K_2017",
      "label": "DRAW1K",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "DRAW1K_2017",
        "name": "DRAW1K",
        "description": "Dataset constructed with 1000 linear equation problems crawled and filtered from algebra.com",
        "domain": "Mathematics",
        "size": 1000,
        "year": "2017",
        "creators": "[\"Upadhyay, S.\", \"Chang, M.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "综述",
        "entity_id": "DRAW1K_2017"
      }
    },
    {
      "id": "Elementary_Math_Word_Problems_2015",
      "label": "Elementary Math Word Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Elementary_Math_Word_Problems_2015",
        "name": "Elementary Math Word Problems",
        "description": "Elementary school math word problems",
        "domain": "Education",
        "size": 500,
        "year": "2015",
        "creators": "[\"Subhro Roy\", \"Tim Vieira\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Elementary_Math_Word_Problems_2015"
      }
    },
    {
      "id": "ElementarySchoolArithmeticApplicationProblem_2011",
      "label": "Elementary school arithmetic application problem",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ElementarySchoolArithmeticApplicationProblem_2011",
        "name": "Elementary school arithmetic application problem",
        "description": "Arithmetic word problems for elementary school students",
        "domain": "Education",
        "size": 627,
        "year": "2011",
        "creators": "[\"People's Education Press\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ElementarySchoolArithmeticApplicationProblem_2011"
      }
    },
    {
      "id": "EmbeddedReberGrammar_1997",
      "label": "Embedded Reber Grammar",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "EmbeddedReberGrammar_1997",
        "name": "Embedded Reber Grammar",
        "description": "A synthetic dataset for evaluating sequence modeling algorithms.",
        "domain": "Natural Language Processing",
        "size": 512,
        "year": "1997",
        "creators": "[\"Sepp Hochreiter\", \"Jürgen Schmidhuber\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "EmbeddedReberGrammar_1997"
      }
    },
    {
      "id": "English_Penn_Treebank_2014",
      "label": "English Penn Treebank",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "English_Penn_Treebank_2014",
        "name": "English Penn Treebank",
        "description": "A dataset for English syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 39832,
        "year": "2014",
        "creators": "[\"Johansson and Nugues\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "English_Penn_Treebank_2014"
      }
    },
    {
      "id": "ESP_2004",
      "label": "ESP",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ESP_2004",
        "name": "ESP",
        "description": "通过在线游戏收集的图像标签数据集。",
        "domain": "计算机视觉",
        "size": 60000,
        "year": "2004",
        "creators": "[\"von Ahn, L.\", \"Dabbish, L.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ESP_2004"
      }
    },
    {
      "id": "Europarl_2014",
      "label": "Europarl",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Europarl_2014",
        "name": "Europarl",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 61000000,
        "year": "2014",
        "creators": "[\"Philipp Koehn\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Europarl_2014"
      }
    },
    {
      "id": "FiguresFromGeometryTextbooks_2014",
      "label": "Figures from Geometry Textbooks",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "FiguresFromGeometryTextbooks_2014",
        "name": "Figures from Geometry Textbooks",
        "description": "A collection of 110 geometric figures taken from standard mathematics textbooks in India and the United States.",
        "domain": "High School Geometry",
        "size": 110,
        "year": "2014",
        "creators": "[\"Alvin, C.\", \"Gulwani, S.\", \"Majumdar, R.\", \"Mukhopadhyay, S.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "FiguresFromGeometryTextbooks_2014"
      }
    },
    {
      "id": "FreebaseQA_2013",
      "label": "Freebase QA",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "FreebaseQA_2013",
        "name": "Freebase QA",
        "description": "Questions to Freebase, a large community-authored database that spans many sub-domains.",
        "domain": "Open-domain Question Answering",
        "size": 0,
        "year": "2013",
        "creators": "[\"Cai, Q.\", \"Yates, A.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "FreebaseQA_2013"
      }
    },
    {
      "id": "Geometry_Questions_Dataset_2014",
      "label": "Geometry Questions Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Geometry_Questions_Dataset_2014",
        "name": "Geometry Questions Dataset",
        "description": "A dataset of high school plane geometry questions with textual descriptions and diagrams.",
        "domain": "Geometry",
        "size": 100,
        "year": "2014",
        "creators": "[\"Min Joon Seo\", \"Hannaneh Hajishirzi\", \"Ali Farhadi\", \"Oren Etzioni\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Geometry_Questions_Dataset_2014"
      }
    },
    {
      "id": "Geoquery_1996",
      "label": "GeoQuery",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Geoquery_1996",
        "name": "GeoQuery",
        "description": "Geography database with a small ontology and questions with relatively complex, compositional structure.",
        "domain": "Geography",
        "size": 0,
        "year": "1996",
        "creators": "[\"Zelle, J.\", \"Mooney, R.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Geoquery_1996"
      }
    },
    {
      "id": "HighConfidenceCorpusParsing_2015",
      "label": "High-Confidence Corpus Parsing",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "HighConfidenceCorpusParsing_2015",
        "name": "High-Confidence Corpus Parsing",
        "description": "Large corpus for syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 11000000,
        "year": "2015",
        "creators": "[\"Vinyals et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "HighConfidenceCorpusParsing_2015"
      }
    },
    {
      "id": "HighSchoolGeometryProblems_2015",
      "label": "High School Geometry Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "HighSchoolGeometryProblems_2015",
        "name": "High School Geometry Problems",
        "description": "A corpus of high school geometry problems from standard geometry textbooks.",
        "domain": "Education",
        "size": 155,
        "year": "2015",
        "creators": "[\"Sinclair et al.\", \"Boyd et al.\", \"Larson et al.\", \"Jurgensen et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "HighSchoolGeometryProblems_2015"
      }
    },
    {
      "id": "Hosseini2014_DS1",
      "label": "DS1",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Hosseini2014_DS1",
        "name": "DS1",
        "description": "Dataset with simple word problems",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2014",
        "creators": "[\"Hosseini, M. J.\", \"Hajishirzi, H.\", \"Etzioni, O.\", \"Kushman, N.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Hosseini2014_DS1"
      }
    },
    {
      "id": "Hosseini2014_DS2",
      "label": "DS2",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Hosseini2014_DS2",
        "name": "DS2",
        "description": "Dataset with moderate complexity word problems",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2014",
        "creators": "[\"Hosseini, M. J.\", \"Hajishirzi, H.\", \"Etzioni, O.\", \"Kushman, N.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Hosseini2014_DS2"
      }
    },
    {
      "id": "Hosseini2014_DS3",
      "label": "DS3",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Hosseini2014_DS3",
        "name": "DS3",
        "description": "Dataset with complex word problems",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2014",
        "creators": "[\"Hosseini, M. J.\", \"Hajishirzi, H.\", \"Etzioni, O.\", \"Kushman, N.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Hosseini2014_DS3"
      }
    },
    {
      "id": "IL_2014",
      "label": "IL",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "IL_2014",
        "name": "IL",
        "description": "Single-step word problems with one operator",
        "domain": "Mathematics",
        "size": 562,
        "year": "2014",
        "creators": "[\"Roy, S.\", \"Roth, D.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "综述",
        "entity_id": "IL_2014"
      }
    },
    {
      "id": "IL_2015",
      "label": "IL",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "IL_2015",
        "name": "IL",
        "description": "Single-step word problems with one operator (addition, subtraction, multiplication, division)",
        "domain": "Arithmetic Word Problems",
        "size": 562,
        "year": "2015",
        "creators": "[\"Roy, Vieira, and Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "IL_2015"
      }
    },
    {
      "id": "IL_Dataset_2015",
      "label": "IL Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "IL_Dataset_2015",
        "name": "IL Dataset",
        "description": "A collection of arithmetic problems, each solvable by performing one operation",
        "domain": "Arithmetic Word Problems",
        "size": 562,
        "year": "2015",
        "creators": "[\"S. Roy\", \"T. Vieira\", \"D. Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "IL_Dataset_2015"
      }
    },
    {
      "id": "ImageCaptioningDataset_2015",
      "label": "Image Captioning Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ImageCaptioningDataset_2015",
        "name": "Image Captioning Dataset",
        "description": "Dataset of image-caption pairs",
        "domain": "Computer Vision",
        "size": 596000,
        "year": "2015",
        "creators": "[\"Vinyals et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ImageCaptioningDataset_2015"
      }
    },
    {
      "id": "ImageNet_2009",
      "label": "ImageNet",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "ImageNet_2009",
        "name": "ImageNet",
        "description": "一个大规模的层次化图像数据库，基于WordNet结构构建。",
        "domain": "计算机视觉",
        "size": 3200000,
        "year": "2009",
        "creators": "[\"Deng, J.\", \"Dong, W.\", \"Socher, R.\", \"Li, L.-J.\", \"Li, K.\", \"Fei-Fei, L.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ImageNet_2009"
      }
    },
    {
      "id": "IQTestSet_2016",
      "label": "IQ Test Set",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "IQTestSet_2016",
        "name": "IQ Test Set",
        "description": "Verbal comprehension questions from published IQ test books",
        "domain": "Intelligence Testing",
        "size": 232,
        "year": "2016",
        "creators": "[\"Various Authors of IQ Test Books\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "IQTestSet_2016"
      }
    },
    {
      "id": "IWSLT2014_German_to_English_2016",
      "label": "IWSLT 2014 German-to-English",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "IWSLT2014_German_to_English_2016",
        "name": "IWSLT 2014 German-to-English",
        "description": "Dataset from the IWSLT 2014 machine translation evaluation campaign",
        "domain": "Machine Translation",
        "size": 0,
        "year": "2016",
        "creators": "[\"Cettolo, M.\", \"Niehues, J.\", \"Stüker, S.\", \"Bentivogli, L.\", \"Federico, M.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "IWSLT2014_German_to_English_2016"
      }
    },
    {
      "id": "IXL_2014",
      "label": "IXL",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "IXL_2014",
        "name": "IXL",
        "description": "Math word problems with more information gaps",
        "domain": "Mathematics",
        "size": 395,
        "year": "2014",
        "creators": "[\"Hosseini et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "IXL_2014"
      }
    },
    {
      "id": "Kushman2014_Dataset_2014",
      "label": "Kushman2014 Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Kushman2014_Dataset_2014",
        "name": "Kushman2014 Dataset",
        "description": "Benchmark dataset for algebra word problems",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2014",
        "creators": "[\"Nate Kushman\", \"Yoav Artzi\", \"Luke Zettlemoyer\", \"Regina Barzilay\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Kushman2014_Dataset_2014"
      }
    },
    {
      "id": "LabelMe_2008",
      "label": "LabelMe",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "LabelMe_2008",
        "name": "LabelMe",
        "description": "一个包含30000张带标注和分割的图像数据集。",
        "domain": "计算机视觉",
        "size": 30000,
        "year": "2008",
        "creators": "[\"Russell, B.\", \"Torralba, A.\", \"Murphy, K.\", \"Freeman, W.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "LabelMe_2008"
      }
    },
    {
      "id": "LotusHill_2007",
      "label": "Lotus Hill",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "LotusHill_2007",
        "name": "Lotus Hill",
        "description": "一个包含50000张带标注和分割的图像数据集。",
        "domain": "计算机视觉",
        "size": 50000,
        "year": "2007",
        "creators": "[\"Yao, B.\", \"Yang, X.\", \"Zhu, S.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "LotusHill_2007"
      }
    },
    {
      "id": "MA1_2014",
      "label": "MA1",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MA1_2014",
        "name": "MA1",
        "description": "Simple math word problems on addition and subtraction for third, fourth, and fifth graders",
        "domain": "Mathematics",
        "size": 395,
        "year": "2014",
        "creators": "[\"Hosseini et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MA1_2014"
      }
    },
    {
      "id": "MA2_2014",
      "label": "MA2",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MA2_2014",
        "name": "MA2",
        "description": "Math word problems with more irrelevant information",
        "domain": "Mathematics",
        "size": 395,
        "year": "2014",
        "creators": "[\"Hosseini et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MA2_2014"
      }
    },
    {
      "id": "Math23K_2014",
      "label": "Math23K",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Math23K_2014",
        "name": "Math23K",
        "description": "Chinese math word problems for elementary school students",
        "domain": "Mathematics",
        "size": 23162,
        "year": "2014",
        "creators": "[\"Wang, Y.\", \"Liu, X.\", \"Shi, S.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "综述",
        "entity_id": "Math23K_2014"
      }
    },
    {
      "id": "Math23K_2015",
      "label": "Math23K",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Math23K_2015",
        "name": "Math23K",
        "description": "Large-scale dataset for arithmetic word problems",
        "domain": "Mathematics",
        "size": 23000,
        "year": "2015",
        "creators": "[\"Kushman et al.\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "Math23K_2015"
      }
    },
    {
      "id": "Math23K_2017",
      "label": "Math23K",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Math23K_2017",
        "name": "Math23K",
        "description": "Large dataset of Chinese algebra word problems",
        "domain": "Natural Language Processing",
        "size": 23164,
        "year": "2017",
        "creators": "[\"Wang, Yan\", \"Liu, Xiaojiang\", \"Shi, Shuming\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Math23K_2017"
      }
    },
    {
      "id": "MathematicalWordProblems_2017",
      "label": "Mathematical Word Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MathematicalWordProblems_2017",
        "name": "Mathematical Word Problems",
        "description": "A dataset of mathematical word problems",
        "domain": "Mathematics",
        "size": 1000,
        "year": "2017",
        "creators": "[\"Wang, K.\", \"Su, Z.\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "MathematicalWordProblems_2017"
      }
    },
    {
      "id": "MAWPS_2016",
      "label": "MAWPS",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MAWPS_2016",
        "name": "MAWPS",
        "description": "Math Word Problem Repository",
        "domain": "Natural Language Processing",
        "size": 2373,
        "year": "2016",
        "creators": "[\"Koncel-Kedziorski, Rik\", \"Roy, Subhro\", \"Amini, Aida\", \"Kushman, Nate\", \"Hajishirzi, Hannaneh\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MAWPS_2016"
      }
    },
    {
      "id": "MAWPS_S_2016",
      "label": "MAWPS-S",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MAWPS_S_2016",
        "name": "MAWPS-S",
        "description": "Dataset for arithmetic word problems with one unknown variable",
        "domain": "Mathematics",
        "size": 2373,
        "year": "2016",
        "creators": "[\"Authors of the paper\"]",
        "entity_type": "Dataset",
        "task_id": "94afd1a4-87cf-461f-bf45-7e59af553b6e",
        "source": "综述",
        "entity_id": "MAWPS_S_2016"
      }
    },
    {
      "id": "MAWPS-S_2016",
      "label": "MAWPS-S",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MAWPS-S_2016",
        "name": "MAWPS-S",
        "description": "Arithmetic word problems with one unknown variable",
        "domain": "Mathematics",
        "size": 2373,
        "year": "2016",
        "creators": "[\"Koncel-Kedziorski\"]",
        "entity_type": "Dataset",
        "task_id": "e6a5c6e0-87b5-49cf-a7c1-5ae80b441cb7",
        "source": "综述",
        "entity_id": "MAWPS-S_2016"
      }
    },
    {
      "id": "MS_COCO_2014",
      "label": "MS COCO",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MS_COCO_2014",
        "name": "MS COCO",
        "description": "Microsoft Common Objects in Context dataset",
        "domain": "Computer Vision",
        "size": 204721,
        "year": "2014",
        "creators": "[\"T.-Y. Lin\", \"M. Maire\", \"S. Belongie\", \"J. Hays\", \"P. Perona\", \"D. Ramanan\", \"P. Dollar\", \"C. L. Zitnick\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MS_COCO_2014"
      }
    },
    {
      "id": "MSRC_2006",
      "label": "MSRC",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MSRC_2006",
        "name": "MSRC",
        "description": "一个包含24个类别的图像分割数据集。",
        "domain": "计算机视觉",
        "size": 591,
        "year": "2006",
        "creators": "[\"Shotton, J.\", \"Winn, J.\", \"Rother, C.\", \"Criminisi, A.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MSRC_2006"
      }
    },
    {
      "id": "MUC-6-TEST_2009",
      "label": "MUC-6-TEST",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MUC-6-TEST_2009",
        "name": "MUC-6-TEST",
        "description": "MUC6 formal evaluation set",
        "domain": "Natural Language Processing",
        "size": 30,
        "year": "2009",
        "creators": "[]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MUC-6-TEST_2009"
      }
    },
    {
      "id": "MUC6-TEST_1995",
      "label": "MUC6-TEST",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "MUC6-TEST_1995",
        "name": "MUC6-TEST",
        "description": "Test corpus from the sixth Message Understanding Conference (MUC-6) evaluation",
        "domain": "Natural Language Processing",
        "size": 30,
        "year": "1995",
        "creators": "[\"Vilain et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MUC6-TEST_1995"
      }
    },
    {
      "id": "NewsArticlesDataset_2013",
      "label": "News Articles Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "NewsArticlesDataset_2013",
        "name": "News Articles Dataset",
        "description": "A large dataset consisting of various news articles",
        "domain": "Natural Language Processing",
        "size": 1000000000,
        "year": "2013",
        "creators": "[\"Google Inc.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "NewsArticlesDataset_2013"
      }
    },
    {
      "id": "NewsCommentary_2014",
      "label": "News Commentary",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "NewsCommentary_2014",
        "name": "News Commentary",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 5500000,
        "year": "2014",
        "creators": "[\"Philipp Koehn\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "NewsCommentary_2014"
      }
    },
    {
      "id": "Newswire_Text_2015",
      "label": "Newswire Text",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Newswire_Text_2015",
        "name": "Newswire Text",
        "description": "600 sentences of newswire text containing quantity mentions",
        "domain": "Natural Language Processing",
        "size": 600,
        "year": "2015",
        "creators": "[\"Subhro Roy\", \"Tim Vieira\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Newswire_Text_2015"
      }
    },
    {
      "id": "NumWord_2015",
      "label": "NumWord",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "NumWord_2015",
        "name": "NumWord",
        "description": "Contains 2,871 number word problems with 1,183 templates",
        "domain": "Mathematics",
        "size": 2871,
        "year": "2015",
        "creators": "[\"Shi et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "NumWord_2015"
      }
    },
    {
      "id": "NY_Regents_Science_Exam_2016",
      "label": "NY Regents Science Exam",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "NY_Regents_Science_Exam_2016",
        "name": "NY Regents Science Exam",
        "description": "Multiple-choice science exam questions for 4th grade students",
        "domain": "Elementary Education",
        "size": 237,
        "year": "2016",
        "creators": "[\"New York State Education Department\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "NY_Regents_Science_Exam_2016"
      }
    },
    {
      "id": "Ontonotes5_2012",
      "label": "Ontonotes-5.0",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Ontonotes5_2012",
        "name": "Ontonotes-5.0",
        "description": "A large annotated corpus for coreference resolution containing 3,145 documents from various sources.",
        "domain": "Natural Language Processing",
        "size": 3145,
        "year": "2012",
        "creators": "[\"Pradhan et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Ontonotes5_2012"
      }
    },
    {
      "id": "PASCAL_2008",
      "label": "PASCAL",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PASCAL_2008",
        "name": "PASCAL",
        "description": "一个用于目标检测和场景分类的基准数据集。",
        "domain": "计算机视觉",
        "size": 2008,
        "year": "2008",
        "creators": "[\"Everingham, M.\", \"Van Gool, L.\", \"Williams, C. K. I.\", \"Winn, J.\", \"Zisserman, A.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PASCAL_2008"
      }
    },
    {
      "id": "PBF_Diagrams_1991",
      "label": "PBF Diagrams",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PBF_Diagrams_1991",
        "name": "PBF Diagrams",
        "description": "Diagrams from Pictorial Books of Flora containing information about plant parts, types, properties, and species.",
        "domain": "Botany",
        "size": 31,
        "year": "1991",
        "creators": "[\"Yasuhiko Watanabe\", \"Makoto Nagao\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PBF_Diagrams_1991"
      }
    },
    {
      "id": "PennTreebank_1999",
      "label": "Penn Treebank",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PennTreebank_1999",
        "name": "Penn Treebank",
        "description": "A widely used dataset for parsing English sentences.",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "1999",
        "creators": "[\"Various contributors\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PennTreebank_1999"
      }
    },
    {
      "id": "PennTreebank_2013",
      "label": "Penn Treebank",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PennTreebank_2013",
        "name": "Penn Treebank",
        "description": "A dataset of syntactically annotated English text",
        "domain": "Natural Language Processing",
        "size": 40000,
        "year": "2013",
        "creators": "[\"Marcus, M.\", \"Marcinkiewicz, M.\", \"Santorini, B.\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "PennTreebank_2013"
      }
    },
    {
      "id": "PennTreebank_WSJ_2013",
      "label": "Penn Treebank WSJ",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PennTreebank_WSJ_2013",
        "name": "Penn Treebank WSJ",
        "description": "Wall Street Journal section of the Penn Treebank",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2013",
        "creators": "[\"Various contributors\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PennTreebank_WSJ_2013"
      }
    },
    {
      "id": "PennTreeBankParsing_1993",
      "label": "Penn Tree Bank Parsing",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PennTreeBankParsing_1993",
        "name": "Penn Tree Bank Parsing",
        "description": "Corpus for syntactic parsing",
        "domain": "Natural Language Processing",
        "size": 40000,
        "year": "1993",
        "creators": "[\"Marcus et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PennTreeBankParsing_1993"
      }
    },
    {
      "id": "Perturb_2018",
      "label": "Perturb",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Perturb_2018",
        "name": "Perturb",
        "description": "New word problems created by perturbing original problems",
        "domain": "Mathematics",
        "size": 661,
        "year": "2018",
        "creators": "[\"Subhro Roy\", \"Dan Roth\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Perturb_2018"
      }
    },
    {
      "id": "PhysicsProblems_1990",
      "label": "Physics Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PhysicsProblems_1990",
        "name": "Physics Problems",
        "description": "Textbook physics problems with accompanying diagrams",
        "domain": "Physics Education",
        "size": 0,
        "year": "1990",
        "creators": "[\"Novak, G. S.\", \"Bulko, W.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PhysicsProblems_1990"
      }
    },
    {
      "id": "PolygonFigures_1996",
      "label": "Polygon Figures",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PolygonFigures_1996",
        "name": "Polygon Figures",
        "description": "Set of randomly-generated polygons used for symmetry detection experiments",
        "domain": "Symmetry Detection",
        "size": 240,
        "year": "1996",
        "creators": "[\"Ronald W. Ferguson\", \"Anna Aminoff\", \"Dedre Gentner\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PolygonFigures_1996"
      }
    },
    {
      "id": "PTB_2016",
      "label": "PTB",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "PTB_2016",
        "name": "PTB",
        "description": "Penn Treebank dataset for word ordering and dependency parsing tasks",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2016",
        "creators": "[\"Zhang, Y.\", \"Clark, S.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "PTB_2016"
      }
    },
    {
      "id": "RTE_Datasets_2006",
      "label": "RTE Datasets",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "RTE_Datasets_2006",
        "name": "RTE Datasets",
        "description": "Textual Entailment datasets from RTE2 to RTE4",
        "domain": "Natural Language Processing",
        "size": 384,
        "year": "2006",
        "creators": "[\"Dagan, I.\", \"Glickman, O.\", \"Magnini, B.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "RTE_Datasets_2006"
      }
    },
    {
      "id": "SAT_Geometry_Questions_2015",
      "label": "SAT Geometry Questions",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "SAT_Geometry_Questions_2015",
        "name": "SAT Geometry Questions",
        "description": "A dataset of SAT plane geometry questions with textual descriptions, diagrams, and multiple-choice answers.",
        "domain": "Educational Testing",
        "size": 186,
        "year": "2015",
        "creators": "[\"College Board\", \"Allen Institute for AI\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SAT_Geometry_Questions_2015"
      }
    },
    {
      "id": "SATGeometryQuestions_2015",
      "label": "SAT Geometry Questions",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "SATGeometryQuestions_2015",
        "name": "SAT Geometry Questions",
        "description": "A dataset of SAT-level geometry problems with text and diagrams",
        "domain": "Geometry",
        "size": 110,
        "year": "2015",
        "creators": "[\"Seo, M.\", \"Hajishirzi, H.\", \"Farhadi, A.\", \"Etzioni, O.\", \"Malcolm, C.\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "SATGeometryQuestions_2015"
      }
    },
    {
      "id": "SingleEQ_2015",
      "label": "SINGLEEQ",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "SingleEQ_2015",
        "name": "SINGLEEQ",
        "description": "Grade-school algebra word problems that map to single equations",
        "domain": "Natural Language Processing",
        "size": 508,
        "year": "2015",
        "creators": "[\"Koncel-Kedziorski, R.\", \"Hajishirzi, H.\", \"Sabharwal, A.\", \"Etzioni, O.\", \"Ang, S. D.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SingleEQ_2015"
      }
    },
    {
      "id": "SingleEQ_2016",
      "label": "SingleEQ",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "SingleEQ_2016",
        "name": "SingleEQ",
        "description": "Dataset containing single-step and multi-step arithmetic problems",
        "domain": "Mathematics",
        "size": 508,
        "year": "2016",
        "creators": "[\"Authors of the paper\"]",
        "entity_type": "Dataset",
        "task_id": "94afd1a4-87cf-461f-bf45-7e59af553b6e",
        "source": "综述",
        "entity_id": "SingleEQ_2016"
      }
    },
    {
      "id": "SNLI_Corpus_2015",
      "label": "SNLI Corpus",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "SNLI_Corpus_2015",
        "name": "SNLI Corpus",
        "description": "570k human-written English sentence pairs manually labeled for entailment, contradiction, and neutral",
        "domain": "Natural Language Processing",
        "size": 570000,
        "year": "2015",
        "creators": "[\"Bowman et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SNLI_Corpus_2015"
      }
    },
    {
      "id": "SolitaireCardGameRules_2014",
      "label": "Solitaire Card Game Rules",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "SolitaireCardGameRules_2014",
        "name": "Solitaire Card Game Rules",
        "description": "Instructions and rules for various versions of the Solitaire card game",
        "domain": "Card Games",
        "size": 0,
        "year": "2014",
        "creators": "[\"Goldwasser, D.\", \"Roth, D.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SolitaireCardGameRules_2014"
      }
    },
    {
      "id": "SuzhouEducationPublishingHouse_2016",
      "label": "Suzhou Education Publishing House",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "SuzhouEducationPublishingHouse_2016",
        "name": "Suzhou Education Publishing House",
        "description": "Arithmetic word problems for training",
        "domain": "Education",
        "size": 0,
        "year": "2016",
        "creators": "[\"Suzhou Education Publishing House\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SuzhouEducationPublishingHouse_2016"
      }
    },
    {
      "id": "TextbookPhysicsProblems_1985",
      "label": "Textbook Physics Problems",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "TextbookPhysicsProblems_1985",
        "name": "Textbook Physics Problems",
        "description": "A dataset of physics problems with text and diagrams",
        "domain": "Physics",
        "size": 100,
        "year": "1985",
        "creators": "[\"Novak, G.\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "TextbookPhysicsProblems_1985"
      }
    },
    {
      "id": "TinyImage_2008",
      "label": "TinyImage",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "TinyImage_2008",
        "name": "TinyImage",
        "description": "一个包含8000万张低分辨率图像的数据集。",
        "domain": "计算机视觉",
        "size": 80000000,
        "year": "2008",
        "creators": "[\"Torralba, A.\", \"Fergus, R.\", \"Freeman, W.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "TinyImage_2008"
      }
    },
    {
      "id": "UN_2014",
      "label": "UN",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "UN_2014",
        "name": "UN",
        "description": "Parallel corpus for statistical machine translation",
        "domain": "Natural Language Processing",
        "size": 421000000,
        "year": "2014",
        "creators": "[\"Philipp Koehn\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "UN_2014"
      }
    },
    {
      "id": "VQA_v2.0_2016",
      "label": "VQA v2.0",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "VQA_v2.0_2016",
        "name": "VQA v2.0",
        "description": "Balanced Visual Question Answering dataset with complementary images",
        "domain": "Computer Vision and Natural Language Processing",
        "size": 1100000,
        "year": "2016",
        "creators": "[\"Yash Goyal\", \"Tejas Khot\", \"Douglas Summers-Stay\", \"Dhruv Batra\", \"Devi Parikh\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "VQA_v2.0_2016"
      }
    },
    {
      "id": "VQADataset_2015",
      "label": "VQA Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "VQADataset_2015",
        "name": "VQA Dataset",
        "description": "A dataset for visual question answering",
        "domain": "Vision and Language",
        "size": 250000,
        "year": "2015",
        "creators": "[\"Antol, S.\", \"Agrawal, A.\", \"Lu, J.\", \"Mitchell, M.\", \"Batra, D.\", \"Zitnick, C.\", \"Parikh, D.\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "VQADataset_2015"
      }
    },
    {
      "id": "WIKI_2009",
      "label": "WIKI",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "WIKI_2009",
        "name": "WIKI",
        "description": "25k articles of English Wikipedia abstracts parsed by the Klein and Manning parser",
        "domain": "Natural Language Processing",
        "size": 25000,
        "year": "2009",
        "creators": "[\"Klein\", \"Manning\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "WIKI_2009"
      }
    },
    {
      "id": "Wikipedia_2010",
      "label": "Wikipedia 2010",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Wikipedia_2010",
        "name": "Wikipedia 2010",
        "description": "A 2010 dump of Wikipedia with 1 billion tokens",
        "domain": "Natural Language Processing",
        "size": 1000000000,
        "year": "2010",
        "creators": "[\"Stanford University\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Wikipedia_2010"
      }
    },
    {
      "id": "Wikipedia_2014",
      "label": "Wikipedia 2014",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Wikipedia_2014",
        "name": "Wikipedia 2014",
        "description": "A 2014 dump of Wikipedia with 1.6 billion tokens",
        "domain": "Natural Language Processing",
        "size": 1600000000,
        "year": "2014",
        "creators": "[\"Stanford University\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Wikipedia_2014"
      }
    },
    {
      "id": "WMT14_English_to_French",
      "label": "WMT'14 English to French",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "WMT14_English_to_French",
        "name": "WMT'14 English to French",
        "description": "A dataset for English to French translation used in the WMT'14 evaluation campaign.",
        "domain": "Natural Language Processing",
        "size": 12000000,
        "year": "2014",
        "creators": "[\"WMT'14\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "WMT14_English_to_French"
      }
    },
    {
      "id": "WMT14EnglishFrench_2014",
      "label": "WMT-14 English-French",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "WMT14EnglishFrench_2014",
        "name": "WMT-14 English-French",
        "description": "A dataset for English-to-French translation",
        "domain": "Machine Translation",
        "size": 3000000,
        "year": "2014",
        "creators": "[\"WMT\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "WMT14EnglishFrench_2014"
      }
    },
    {
      "id": "WMT15_EnglishGermanTranslation_2015",
      "label": "WMT'15 English-German Translation",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "WMT15_EnglishGermanTranslation_2015",
        "name": "WMT'15 English-German Translation",
        "description": "Parallel corpus for English-German translation",
        "domain": "Machine Translation",
        "size": 4500000,
        "year": "2015",
        "creators": "[\"Bojar et al.\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "WMT15_EnglishGermanTranslation_2015"
      }
    },
    {
      "id": "WSJ_Treebank_2010",
      "label": "WSJ Treebank",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "WSJ_Treebank_2010",
        "name": "WSJ Treebank",
        "description": "Wall Street Journal Treebank used for dependency parsing",
        "domain": "Natural Language Processing",
        "size": 0,
        "year": "2010",
        "creators": "[\"Various contributors\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "WSJ_Treebank_2010"
      }
    },
    {
      "id": "Yahoo_Answers_2015",
      "label": "Yahoo Answers",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Yahoo_Answers_2015",
        "name": "Yahoo Answers",
        "description": "A Q&A platform where users can post and answer questions",
        "domain": "Mathematics",
        "size": 1878,
        "year": "2015",
        "creators": "[\"Various Contributors\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Yahoo_Answers_2015"
      }
    },
    {
      "id": "YahooAnswers_2016",
      "label": "Yahoo! Answers",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "YahooAnswers_2016",
        "name": "Yahoo! Answers",
        "description": "Community question-answering web pages",
        "domain": "Mathematics",
        "size": 1000000,
        "year": "2016",
        "creators": "[\"Danqing Huang\", \"Shuming Shi\", \"Chin-Yew Lin\", \"Jian Yin\", \"Wei-Ying Ma\"]",
        "entity_type": "Dataset",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "YahooAnswers_2016"
      }
    },
    {
      "id": "Yelp_Dataset_2017",
      "label": "Yelp Dataset",
      "type": "Dataset",
      "entity_type": "Dataset",
      "data": {
        "dataset_id": "Yelp_Dataset_2017",
        "name": "Yelp Dataset",
        "description": "2.7M Yelp reviews with star ratings",
        "domain": "Natural Language Processing",
        "size": 2700000,
        "year": "2017",
        "creators": "[\"Yelp\"]",
        "entity_type": "Dataset",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Yelp_Dataset_2017"
      }
    },
    {
      "id": "Accuracy_Arithmetic",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Arithmetic",
        "name": "Accuracy",
        "description": "Proportion of correctly solved arithmetic word problems",
        "category": "Arithmetic Word Problem Solving",
        "formula": "Correctly solved problems / Total problems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "综述",
        "entity_id": "Accuracy_Arithmetic"
      }
    },
    {
      "id": "Accuracy_Classification",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Classification",
        "name": "Accuracy",
        "description": "Classification accuracy",
        "category": "Classification Evaluation",
        "formula": "Correct classifications / Total classifications",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_Classification"
      }
    },
    {
      "id": "Accuracy_Dependency_Parsing",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Dependency_Parsing",
        "name": "Accuracy",
        "description": "Accuracy of dependency parsing in matching dependency parse structures to ground truth annotations.",
        "category": "Dependency Parsing",
        "formula": "Number of Correct Structures / Total Number of Structures",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_Dependency_Parsing"
      }
    },
    {
      "id": "Accuracy_Detection",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Detection",
        "name": "Accuracy",
        "description": "Detection accuracy",
        "category": "Detection Evaluation",
        "formula": "Correct detections / Total detections",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_Detection"
      }
    },
    {
      "id": "Accuracy_EquationSet",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_EquationSet",
        "name": "Accuracy",
        "description": "Proportion of correctly solved equation set problems",
        "category": "Equation Set Problem Solving",
        "formula": "Correctly solved problems / Total problems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "综述",
        "entity_id": "Accuracy_EquationSet"
      }
    },
    {
      "id": "Accuracy_Math",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Math",
        "name": "Accuracy",
        "description": "Accuracy of the generated mathematical word problems",
        "category": "Math Problem Generation",
        "formula": "Number of Correct Problems / Total Number of Generated Problems",
        "entity_type": "Metric",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "Accuracy_Math"
      }
    },
    {
      "id": "Accuracy_Math_Word_Problems",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Math_Word_Problems",
        "name": "Accuracy",
        "description": "Proportion of correct answers",
        "category": "Math Word Problem Solving",
        "formula": "Correct Answers / Total Questions",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_Math_Word_Problems"
      }
    },
    {
      "id": "Accuracy_MathProblemSolving",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_MathProblemSolving",
        "name": "Accuracy",
        "description": "Proportion of correctly solved math word problems",
        "category": "Math Word Problem Solving",
        "formula": "Number of Correct Solutions / Total Number of Problems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_MathProblemSolving"
      }
    },
    {
      "id": "Accuracy_Multiple-Choice",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Multiple-Choice",
        "name": "Accuracy",
        "description": "Accuracy for multiple-choice task",
        "category": "Classification Evaluation",
        "formula": "Number of correct answers / Total number of questions",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_Multiple-Choice"
      }
    },
    {
      "id": "Accuracy_Open-Answer",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_Open-Answer",
        "name": "Accuracy",
        "description": "Accuracy for open-answer task",
        "category": "Classification Evaluation",
        "formula": "min(# humans that provided that answer / 3, 1)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_Open-Answer"
      }
    },
    {
      "id": "Accuracy_SymmetryJudgment",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_SymmetryJudgment",
        "name": "Accuracy",
        "description": "Accuracy of symmetry judgment in polygon figures",
        "category": "Symmetry Detection",
        "formula": "Correctly identified symmetric polygons / Total number of polygons",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_SymmetryJudgment"
      }
    },
    {
      "id": "Accuracy_VQA",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_VQA",
        "name": "Accuracy",
        "description": "Proportion of correctly answered questions",
        "category": "Classification",
        "formula": "Correctly classified samples / Total samples",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_VQA"
      }
    },
    {
      "id": "Accuracy_WordAnalogy",
      "label": "Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Accuracy_WordAnalogy",
        "name": "Accuracy",
        "description": "Accuracy on the word analogy task",
        "category": "Word Analogy Evaluation",
        "formula": "Correct answers / Total questions",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Accuracy_WordAnalogy"
      }
    },
    {
      "id": "Answer_Accuracy",
      "label": "Answer Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Answer_Accuracy",
        "name": "Answer Accuracy",
        "description": "Evaluates how often the generated numerical answer is correct",
        "category": "Numerical Answer Evaluation",
        "formula": "Number of Correct Answers / Total Number of Answers",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Answer_Accuracy"
      }
    },
    {
      "id": "AUC_ROC",
      "label": "AUC",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "AUC_ROC",
        "name": "AUC",
        "description": "ROC曲线下面积",
        "category": "分类评估",
        "formula": "ROC曲线下的面积",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "AUC_ROC"
      }
    },
    {
      "id": "B-Cubed_F-Score_Coreference",
      "label": "B-Cubed F-Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "B-Cubed_F-Score_Coreference",
        "name": "B-Cubed F-Score",
        "description": "A measure of the overlap of predicted clusters and true clusters",
        "category": "Coreference Evaluation",
        "formula": "Harmonic mean of precision and recall",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "B-Cubed_F-Score_Coreference"
      }
    },
    {
      "id": "B3_Coreference",
      "label": "b3",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "B3_Coreference",
        "name": "b3",
        "description": "For each mention, form the intersection between the predicted cluster and the true cluster for that mention",
        "category": "Coreference Evaluation",
        "formula": "F1 = 2 * (precision * recall) / (precision + recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "B3_Coreference"
      }
    },
    {
      "id": "B3_Scoring",
      "label": "B3",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "B3_Scoring",
        "name": "B3",
        "description": "Uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the predicted and gold clusters as denominators for precision and recall",
        "category": "Coreference Resolution",
        "formula": "Not specified in the paper",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "B3_Scoring"
      }
    },
    {
      "id": "BCUB_Coreference",
      "label": "BCUB",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "BCUB_Coreference",
        "name": "BCUB",
        "description": "A metric for evaluating coreference resolution systems.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "BCUB_Coreference"
      }
    },
    {
      "id": "BLEU_Translation",
      "label": "BLEU",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "BLEU_Translation",
        "name": "BLEU",
        "description": "Bilingual Evaluation Understudy",
        "category": "Machine Translation Evaluation",
        "formula": "Exponential of the weighted sum of modified n-gram precision and brevity penalty",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "BLEU_Translation"
      }
    },
    {
      "id": "BLEUScore_Translation",
      "label": "BLEU Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "BLEUScore_Translation",
        "name": "BLEU Score",
        "description": "Bilingual Evaluation Understudy Score",
        "category": "Translation Evaluation",
        "formula": "BP * exp(sum_{n=1}^N w_n log p_n)",
        "entity_type": "Metric",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "BLEUScore_Translation"
      }
    },
    {
      "id": "CEAF_Coreference",
      "label": "CEAF",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "CEAF_Coreference",
        "name": "CEAF",
        "description": "Scores the best match between true and predicted clusters using a similarity function",
        "category": "Coreference Evaluation",
        "formula": "Best match using φ3 similarity function",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CEAF_Coreference"
      }
    },
    {
      "id": "CEAF_EntityBased",
      "label": "Entity-based CEAF",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "CEAF_EntityBased",
        "name": "Entity-based CEAF",
        "description": "A metric for evaluating coreference resolution systems.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CEAF_EntityBased"
      }
    },
    {
      "id": "Complete_Correct_Parse",
      "label": "Complete",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Complete_Correct_Parse",
        "name": "Complete",
        "description": "Percentage of sentences in which all tokens were assigned their correct parent",
        "category": "Dependency Parsing Evaluation",
        "formula": "Sentences with all correct token assignments / Total sentences",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Complete_Correct_Parse"
      }
    },
    {
      "id": "Correct_Solution_Rate_Understanding",
      "label": "Correct Solution Rate",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Correct_Solution_Rate_Understanding",
        "name": "Correct Solution Rate",
        "description": "The proportion of correctly solved word problems.",
        "category": "Problem Solving Performance",
        "formula": "Number of Correct Solutions / Total Number of Problems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Correct_Solution_Rate_Understanding"
      }
    },
    {
      "id": "Correctness_Completeness",
      "label": "Correctness and Completeness",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Correctness_Completeness",
        "name": "Correctness and Completeness",
        "description": "Validation of the model's correctness and completeness when supplied as input to a physics problem-solving program",
        "category": "Model Validation",
        "formula": "",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Correctness_Completeness"
      }
    },
    {
      "id": "Correctness_Physics",
      "label": "Correctness",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Correctness_Physics",
        "name": "Correctness",
        "description": "Correctness of the model's solution to physics problems",
        "category": "Physics Problem Solving",
        "formula": "Number of Correct Solutions / Total Number of Problems",
        "entity_type": "Metric",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "Correctness_Physics"
      }
    },
    {
      "id": "CorrectnessOfSolution_ProblemSolving",
      "label": "Correctness of Solution",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "CorrectnessOfSolution_ProblemSolving",
        "name": "Correctness of Solution",
        "description": "Whether the multi-step addition and subtraction word problems are correctly solved",
        "category": "Problem Solving Evaluation",
        "formula": "Number of Correct Solutions / Total Number of Problems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "CorrectnessOfSolution_ProblemSolving"
      }
    },
    {
      "id": "Count_ColinearPoints",
      "label": "Count of Colinear Points",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Count_ColinearPoints",
        "name": "Count of Colinear Points",
        "description": "Number of colinear points detected by the Hough Transformation",
        "category": "Line Detection",
        "formula": "k figure points lie along the line whose normal parameters are (θ, p)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Count_ColinearPoints"
      }
    },
    {
      "id": "DeductiveSteps_Classification",
      "label": "Deductive Steps",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "DeductiveSteps_Classification",
        "name": "Deductive Steps",
        "description": "The number of hyperedges in the problem hypergraph.",
        "category": "Classification Assessment",
        "formula": "Number of hyperedges",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "DeductiveSteps_Classification"
      }
    },
    {
      "id": "DependencyAccuracy_Classification",
      "label": "Dependency Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "DependencyAccuracy_Classification",
        "name": "Dependency Accuracy",
        "description": "Measures the accuracy of dependency relations in a parse.",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correctly parsed dependencies / Total dependencies",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "DependencyAccuracy_Classification"
      }
    },
    {
      "id": "Efficiency_GeometryRuleApplications",
      "label": "Efficiency",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Efficiency_GeometryRuleApplications",
        "name": "Efficiency",
        "description": "Efficiency of Geometry Rule Applications",
        "category": "Search Space Evaluation",
        "formula": "Number of Inferences per Layer",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Efficiency_GeometryRuleApplications"
      }
    },
    {
      "id": "Entropy_AnswerDistribution",
      "label": "Entropy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Entropy_AnswerDistribution",
        "name": "Entropy",
        "description": "Measure of uncertainty in the answer distribution",
        "category": "Distribution Analysis",
        "formula": "-sum(p(x) * log(p(x)))",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Entropy_AnswerDistribution"
      }
    },
    {
      "id": "Equation_Accuracy",
      "label": "Equation Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Equation_Accuracy",
        "name": "Equation Accuracy",
        "description": "Measures how often the system generates the correct equation system",
        "category": "Equation Generation",
        "formula": "Number of Correct Equation Systems / Total Number of Equation Systems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Equation_Accuracy"
      }
    },
    {
      "id": "ErrorRate_Difficulty",
      "label": "Error Rate",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "ErrorRate_Difficulty",
        "name": "Error Rate",
        "description": "Measure of the difficulty level of generated problems",
        "category": "Difficulty Evaluation",
        "formula": "Percentage of incorrect answers",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ErrorRate_Difficulty"
      }
    },
    {
      "id": "ErrorSignalStability_ErrorFlow",
      "label": "Error Signal Stability",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "ErrorSignalStability_ErrorFlow",
        "name": "Error Signal Stability",
        "description": "Stability of error signals over time.",
        "category": "Error Flow Evaluation",
        "formula": "Magnitude of error signals over time",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ErrorSignalStability_ErrorFlow"
      }
    },
    {
      "id": "F1_Evaluation",
      "label": "F1",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Evaluation",
        "name": "F1",
        "description": "The harmonic mean of precision and recall",
        "category": "Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_Evaluation"
      }
    },
    {
      "id": "F1_NEL",
      "label": "F1",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_NEL",
        "name": "F1",
        "description": "Harmonic mean of precision and recall",
        "category": "Named-entity linking evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_NEL"
      }
    },
    {
      "id": "F1_Score",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Classification Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "综述",
        "entity_id": "F1_Score"
      }
    },
    {
      "id": "F1_Score_Aligning_Visual_Elements",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score_Aligning_Visual_Elements",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Alignment Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_Score_Aligning_Visual_Elements"
      }
    },
    {
      "id": "F1_Score_Classification",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score_Classification",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Classification Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "94afd1a4-87cf-461f-bf45-7e59af553b6e",
        "source": "综述",
        "entity_id": "F1_Score_Classification"
      }
    },
    {
      "id": "F1_Score_Identifying_Primitives",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score_Identifying_Primitives",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Classification Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_Score_Identifying_Primitives"
      }
    },
    {
      "id": "F1_Score_NER",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score_NER",
        "name": "F1 Score",
        "description": "F1 score on the named entity recognition task",
        "category": "Named Entity Recognition Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_Score_NER"
      }
    },
    {
      "id": "F1_Score_QA",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score_QA",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall.",
        "category": "Question Answering Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_Score_QA"
      }
    },
    {
      "id": "F1_Score_Quantitative_Reasoning",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score_Quantitative_Reasoning",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Quantitative Reasoning",
        "formula": "2 * (precision * recall) / (precision + recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_Score_Quantitative_Reasoning"
      }
    },
    {
      "id": "F1_Score_Text_Interpretation",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1_Score_Text_Interpretation",
        "name": "F1 Score",
        "description": "F1 score of text interpretation in deriving literals for geometry question texts.",
        "category": "Text Interpretation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1_Score_Text_Interpretation"
      }
    },
    {
      "id": "F1Score_Classification",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1Score_Classification",
        "name": "F1 Score",
        "description": "F1得分",
        "category": "分类评估",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "F1Score_Classification"
      }
    },
    {
      "id": "F1Score_Parsing",
      "label": "F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "F1Score_Parsing",
        "name": "F1 Score",
        "description": "Harmonic mean of precision and recall",
        "category": "Parsing Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "F1Score_Parsing"
      }
    },
    {
      "id": "Labeled_F1_Score_Parsing",
      "label": "Labeled F1 Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Labeled_F1_Score_Parsing",
        "name": "Labeled F1 Score",
        "description": "F1 score for labeled syntactic parsing",
        "category": "Syntactic Parsing Evaluation",
        "formula": "2 * (Precision * Recall) / (Precision + Recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Labeled_F1_Score_Parsing"
      }
    },
    {
      "id": "LabeledAttachmentScore_Parsing",
      "label": "Labeled Attachment Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "LabeledAttachmentScore_Parsing",
        "name": "Labeled Attachment Score",
        "description": "Dependency parsing score with labels",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correct Labeled Arcs / Total Arcs",
        "entity_type": "Metric",
        "task_id": "94afd1a4-87cf-461f-bf45-7e59af553b6e",
        "source": "引文",
        "entity_id": "LabeledAttachmentScore_Parsing"
      }
    },
    {
      "id": "LAS_Parsing",
      "label": "Labeled Attachment Score (LAS)",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "LAS_Parsing",
        "name": "Labeled Attachment Score (LAS)",
        "description": "The proportion of words that are attached to their correct heads and have the correct label",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correctly attached and labeled words / Total words",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "LAS_Parsing"
      }
    },
    {
      "id": "MUC_Coreference",
      "label": "MUC",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "MUC_Coreference",
        "name": "MUC",
        "description": "A metric for evaluating coreference resolution systems.",
        "category": "Coreference Evaluation",
        "formula": "Not explicitly defined in the paper",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MUC_Coreference"
      }
    },
    {
      "id": "MUC_F-Score_Coreference",
      "label": "MUC F-Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "MUC_F-Score_Coreference",
        "name": "MUC F-Score",
        "description": "Official MUC scoring algorithm",
        "category": "Coreference Evaluation",
        "formula": "Harmonic mean of precision and recall, counting precision errors by computing the minimum number of links that must be added and recall errors by the number of links that must be removed",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MUC_F-Score_Coreference"
      }
    },
    {
      "id": "MUC_Scoring",
      "label": "MUC",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "MUC_Scoring",
        "name": "MUC",
        "description": "Measures how many predicted clusters need to be merged to cover the gold clusters",
        "category": "Coreference Resolution",
        "formula": "Not specified in the paper",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "MUC_Scoring"
      }
    },
    {
      "id": "NumberOfGeneratedProblems_Generation",
      "label": "Number of Generated Problems",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "NumberOfGeneratedProblems_Generation",
        "name": "Number of Generated Problems",
        "description": "The number of geometry proof problems generated per figure.",
        "category": "Problem Generation Evaluation",
        "formula": "Total number of problems / Number of figures",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "NumberOfGeneratedProblems_Generation"
      }
    },
    {
      "id": "Pairwise_Coreference",
      "label": "Pairwise",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Pairwise_Coreference",
        "name": "Pairwise",
        "description": "Measures the accuracy of predicting pairs of mentions as coreferent",
        "category": "Coreference evaluation",
        "formula": "Correctly predicted pairs / Total pairs",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Pairwise_Coreference"
      }
    },
    {
      "id": "Pairwise_F1_Coreference",
      "label": "Pairwise F1",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Pairwise_F1_Coreference",
        "name": "Pairwise F1",
        "description": "Computed over mention pairs in the same entity cluster",
        "category": "Coreference Resolution",
        "formula": "2 * (precision * recall) / (precision + recall)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Pairwise_F1_Coreference"
      }
    },
    {
      "id": "Perplexity_LanguageModeling",
      "label": "Perplexity",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Perplexity_LanguageModeling",
        "name": "Perplexity",
        "description": "Measure of how well a probability distribution predicts a sample",
        "category": "Language Modeling Evaluation",
        "formula": "exp(-1/N * sum(log p(x_i))",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Perplexity_LanguageModeling"
      }
    },
    {
      "id": "Precision_Classification",
      "label": "Precision",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Precision_Classification",
        "name": "Precision",
        "description": "精确率",
        "category": "分类评估",
        "formula": "真正例 / （真正例 + 假正例）",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Precision_Classification"
      }
    },
    {
      "id": "Precision_Evaluation",
      "label": "Precision",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Precision_Evaluation",
        "name": "Precision",
        "description": "The proportion of true positive results among the total predicted positives",
        "category": "Evaluation",
        "formula": "True Positives / (True Positives + False Positives)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Precision_Evaluation"
      }
    },
    {
      "id": "Precision_QA",
      "label": "Precision",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Precision_QA",
        "name": "Precision",
        "description": "Percentage of produced queries with correct answers.",
        "category": "Question Answering Evaluation",
        "formula": "Correctly answered queries / Produced queries",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Precision_QA"
      }
    },
    {
      "id": "Precision_Quantitative_Reasoning",
      "label": "Precision",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Precision_Quantitative_Reasoning",
        "name": "Precision",
        "description": "Proportion of true positive predictions",
        "category": "Quantitative Reasoning",
        "formula": "True Positives / (True Positives + False Positives)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Precision_Quantitative_Reasoning"
      }
    },
    {
      "id": "Precision_Text_Interpretation",
      "label": "Precision",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Precision_Text_Interpretation",
        "name": "Precision",
        "description": "Precision of text interpretation in deriving literals for geometry question texts.",
        "category": "Text Interpretation",
        "formula": "True Positives / (True Positives + False Positives)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Precision_Text_Interpretation"
      }
    },
    {
      "id": "ProofLength_Classification",
      "label": "Proof Length",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "ProofLength_Classification",
        "name": "Proof Length",
        "description": "The diameter of the problem hypergraph.",
        "category": "Classification Assessment",
        "formula": "Diameter of the hypergraph",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ProofLength_Classification"
      }
    },
    {
      "id": "ProofWidth_Classification",
      "label": "Proof Width",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "ProofWidth_Classification",
        "name": "Proof Width",
        "description": "The width of the problem hypergraph.",
        "category": "Classification Assessment",
        "formula": "Width of the hypergraph",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "ProofWidth_Classification"
      }
    },
    {
      "id": "QualitativeFactors_SymmetryJudgment",
      "label": "Qualitative Factors",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "QualitativeFactors_SymmetryJudgment",
        "name": "Qualitative Factors",
        "description": "Effect of qualitative visual structure on symmetry judgment",
        "category": "Symmetry Detection",
        "formula": "Significant effect of qualitative factors on accuracy",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "QualitativeFactors_SymmetryJudgment"
      }
    },
    {
      "id": "Recall_Classification",
      "label": "Recall",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Recall_Classification",
        "name": "Recall",
        "description": "召回率",
        "category": "分类评估",
        "formula": "真正例 / （真正例 + 假负例）",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Recall_Classification"
      }
    },
    {
      "id": "Recall_Evaluation",
      "label": "Recall",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Recall_Evaluation",
        "name": "Recall",
        "description": "The proportion of true positive results among the total actual positives",
        "category": "Evaluation",
        "formula": "True Positives / (True Positives + False Negatives)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Recall_Evaluation"
      }
    },
    {
      "id": "Recall_QA",
      "label": "Recall",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Recall_QA",
        "name": "Recall",
        "description": "Percentage of total questions answered correctly.",
        "category": "Question Answering Evaluation",
        "formula": "Correctly answered questions / Total questions",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Recall_QA"
      }
    },
    {
      "id": "Recall_Quantitative_Reasoning",
      "label": "Recall",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Recall_Quantitative_Reasoning",
        "name": "Recall",
        "description": "Proportion of actual positives that are correctly identified",
        "category": "Quantitative Reasoning",
        "formula": "True Positives / (True Positives + False Negatives)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Recall_Quantitative_Reasoning"
      }
    },
    {
      "id": "Recall_Text_Interpretation",
      "label": "Recall",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Recall_Text_Interpretation",
        "name": "Recall",
        "description": "Recall of text interpretation in deriving literals for geometry question texts.",
        "category": "Text Interpretation",
        "formula": "True Positives / (True Positives + False Negatives)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Recall_Text_Interpretation"
      }
    },
    {
      "id": "Relax_Accuracy",
      "label": "Relax Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Relax_Accuracy",
        "name": "Relax Accuracy",
        "description": "Fraction of quantities or quantity pairs correctly predicted",
        "category": "Partial Evaluation",
        "formula": "Correct Predictions / Total Predictions",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Relax_Accuracy"
      }
    },
    {
      "id": "Root_Prediction",
      "label": "Root",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Root_Prediction",
        "name": "Root",
        "description": "Percentage of sentences in which the ROOT attachment is correct",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correct ROOT attachments / Total sentences",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Root_Prediction"
      }
    },
    {
      "id": "RunTimeStatistics_ProblemSolving",
      "label": "Run-Time Statistics",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "RunTimeStatistics_ProblemSolving",
        "name": "Run-Time Statistics",
        "description": "Statistics collected during the execution of the program, including number of production rules fired, number of conversions, number of LTM searches, and maximum number of chunks held over.",
        "category": "Program Performance Evaluation",
        "formula": "Not explicitly defined, but includes counts of various operations",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "RunTimeStatistics_ProblemSolving"
      }
    },
    {
      "id": "SAT_Score_Geometry",
      "label": "SAT Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "SAT_Score_Geometry",
        "name": "SAT Score",
        "description": "Score achieved on SAT geometry questions, penalized for wrong answers.",
        "category": "Test Performance",
        "formula": "Correctly answered questions - 0.25 * Incorrectly answered questions",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SAT_Score_Geometry"
      }
    },
    {
      "id": "Score_GEOS",
      "label": "Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Score_GEOS",
        "name": "Score",
        "description": "Score on SAT geometry questions",
        "category": "Geometry Problem Solving",
        "formula": "Number of Correct Answers / Total Number of Questions",
        "entity_type": "Metric",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "Score_GEOS"
      }
    },
    {
      "id": "SemanticAccuracy_Classification",
      "label": "Semantic Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "SemanticAccuracy_Classification",
        "name": "Semantic Accuracy",
        "description": "Accuracy on semantic analogical reasoning",
        "category": "Classification Evaluation",
        "formula": "Correct Semantic Analogies / Total Semantic Analogies",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SemanticAccuracy_Classification"
      }
    },
    {
      "id": "Solution_Accuracy",
      "label": "Solution Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Solution_Accuracy",
        "name": "Solution Accuracy",
        "description": "Accuracy of the final solution to the math word problem",
        "category": "Math Word Problem Solving Evaluation",
        "formula": "Number of correctly solved problems / Total number of problems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Solution_Accuracy"
      }
    },
    {
      "id": "Solution_Type_Accuracy",
      "label": "Solution Type Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Solution_Type_Accuracy",
        "name": "Solution Type Accuracy",
        "description": "Accuracy of identifying the correct solution type",
        "category": "Classification evaluation",
        "formula": "Correctly identified solution types / Total solution types",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Solution_Type_Accuracy"
      }
    },
    {
      "id": "SolutionAccuracy_Math",
      "label": "Solution Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "SolutionAccuracy_Math",
        "name": "Solution Accuracy",
        "description": "Accuracy of the solution to math word problems",
        "category": "Math Problem Solving",
        "formula": "Number of Correct Solutions / Total Number of Problems",
        "entity_type": "Metric",
        "task_id": "46c68399-2fb2-454a-8e02-858feecbdac0",
        "source": "引文",
        "entity_id": "SolutionAccuracy_Math"
      }
    },
    {
      "id": "SpaceComplexity_Parsing",
      "label": "Space Complexity",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "SpaceComplexity_Parsing",
        "name": "Space Complexity",
        "description": "The memory required to parse a string using the algorithm",
        "category": "Algorithm Performance",
        "formula": "O(n^2)",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SpaceComplexity_Parsing"
      }
    },
    {
      "id": "SpearmanRankCorrelation_WordSimilarity",
      "label": "Spearman Rank Correlation",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "SpearmanRankCorrelation_WordSimilarity",
        "name": "Spearman Rank Correlation",
        "description": "Spearman rank correlation on word similarity tasks",
        "category": "Word Similarity Evaluation",
        "formula": "Correlation between ranked word similarities and human judgments",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SpearmanRankCorrelation_WordSimilarity"
      }
    },
    {
      "id": "StatisticalIndistinguishability_Authenticity",
      "label": "Statistical Indistinguishability",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "StatisticalIndistinguishability_Authenticity",
        "name": "Statistical Indistinguishability",
        "description": "Measure of whether generated problems are statistically indistinguishable from textbook problems",
        "category": "Authenticity Evaluation",
        "formula": "Paired t-test and Chi-square test of independence",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "StatisticalIndistinguishability_Authenticity"
      }
    },
    {
      "id": "Strict_Accuracy",
      "label": "Strict Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Strict_Accuracy",
        "name": "Strict Accuracy",
        "description": "Fraction of problems where all quantities or quantity pairs were correctly classified",
        "category": "Complete Evaluation",
        "formula": "Fully Correct Problems / Total Problems",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Strict_Accuracy"
      }
    },
    {
      "id": "Success_Rate_Semantic_Analysis",
      "label": "Success Rate",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "Success_Rate_Semantic_Analysis",
        "name": "Success Rate",
        "description": "Rate of successful semantic interpretation of diagram elements.",
        "category": "Semantic Analysis Evaluation",
        "formula": "Successful Interpretations / Total Interpretations",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "Success_Rate_Semantic_Analysis"
      }
    },
    {
      "id": "SuccessRate_Classification",
      "label": "Success Rate",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "SuccessRate_Classification",
        "name": "Success Rate",
        "description": "Percentage of successful trials in classification tasks.",
        "category": "Classification Evaluation",
        "formula": "Number of successful trials / Total number of trials",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SuccessRate_Classification"
      }
    },
    {
      "id": "SyntacticAccuracy_Classification",
      "label": "Syntactic Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "SyntacticAccuracy_Classification",
        "name": "Syntactic Accuracy",
        "description": "Accuracy on syntactic analogical reasoning",
        "category": "Classification Evaluation",
        "formula": "Correct Syntactic Analogies / Total Syntactic Analogies",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "SyntacticAccuracy_Classification"
      }
    },
    {
      "id": "TimeComplexity_Parsing",
      "label": "Time Complexity",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "TimeComplexity_Parsing",
        "name": "Time Complexity",
        "description": "The time required to parse a string using the algorithm",
        "category": "Algorithm Performance",
        "formula": "O(n^3) for general context-free grammars, O(n^2) for unambiguous grammars, O(n) for bounded state grammars",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "TimeComplexity_Parsing"
      }
    },
    {
      "id": "TimeTakenToGenerateProblems_Efficiency",
      "label": "Time Taken to Generate Problems",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "TimeTakenToGenerateProblems_Efficiency",
        "name": "Time Taken to Generate Problems",
        "description": "The average time taken to generate problems per figure.",
        "category": "Efficiency Evaluation",
        "formula": "Total time / Number of figures",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "TimeTakenToGenerateProblems_Efficiency"
      }
    },
    {
      "id": "TrainingTime_Performance",
      "label": "Training Time",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "TrainingTime_Performance",
        "name": "Training Time",
        "description": "Time required to train the model successfully.",
        "category": "Performance Evaluation",
        "formula": "Total time taken to achieve stopping criterion",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "TrainingTime_Performance"
      }
    },
    {
      "id": "UAS_Parsing",
      "label": "Unlabeled Attachment Score (UAS)",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "UAS_Parsing",
        "name": "Unlabeled Attachment Score (UAS)",
        "description": "The proportion of words that are attached to their correct heads, ignoring labels",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correctly attached words / Total words",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "UAS_Parsing"
      }
    },
    {
      "id": "UnifiedModel_Accuracy",
      "label": "Unified Model Accuracy",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "UnifiedModel_Accuracy",
        "name": "Unified Model Accuracy",
        "description": "Accuracy of the unified model that combines information from both text and diagram",
        "category": "Model Evaluation",
        "formula": "",
        "entity_type": "Metric",
        "task_id": "dba6b51f-5d11-43f0-9528-026f36379e88",
        "source": "引文",
        "entity_id": "UnifiedModel_Accuracy"
      }
    },
    {
      "id": "UnlabeledAttachmentScore_Parsing",
      "label": "Unlabeled Attachment Score",
      "type": "Metric",
      "entity_type": "Metric",
      "data": {
        "metric_id": "UnlabeledAttachmentScore_Parsing",
        "name": "Unlabeled Attachment Score",
        "description": "Dependency parsing score without labels",
        "category": "Dependency Parsing Evaluation",
        "formula": "Correct Arcs / Total Arcs",
        "entity_type": "Metric",
        "task_id": "94afd1a4-87cf-461f-bf45-7e59af553b6e",
        "source": "引文",
        "entity_id": "UnlabeledAttachmentScore_Parsing"
      }
    },
    {
      "id": "DosSantos2016_AttentivePoolingNetworks",
      "label": "DosSantos2016_AttentivePoolingNetworks",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "DosSantos2016_AttentivePoolingNetworks",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Lee2016_SequentialShortTextClassification",
      "label": "Lee2016_SequentialShortTextClassification",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Lee2016_SequentialShortTextClassification",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Koo2008_SimpleSemiSupervised",
      "label": "Koo2008_SimpleSemiSupervised",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Koo2008_SimpleSemiSupervised",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Chinese_Math_Problems",
      "label": "Chinese_Math_Problems",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Chinese_Math_Problems",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Charniak1968_CARPS",
      "label": "Charniak1968_CARPS",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Charniak1968_CARPS",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Huang2018_NeuralMathWordProblemSolver",
      "label": "Huang2018_NeuralMathWordProblemSolver",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Huang2018_NeuralMathWordProblemSolver",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "SingleEQ",
      "label": "SingleEQ",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "SingleEQ",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "MALT",
      "label": "MALT",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "MALT",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Nivre2006_MaltParser",
      "label": "Nivre2006_MaltParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Nivre2006_MaltParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "MST",
      "label": "MST",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "MST",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Sagae2006b_ParserCombination",
      "label": "Sagae2006b_ParserCombination",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Sagae2006b_ParserCombination",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Duchi2011_AdaptiveSubgradient",
      "label": "Duchi2011_AdaptiveSubgradient",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Duchi2011_AdaptiveSubgradient",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Shen2007_BidirectionalIncrementalConstruction",
      "label": "Shen2007_BidirectionalIncrementalConstruction",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Shen2007_BidirectionalIncrementalConstruction",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Dolphin-S_2016",
      "label": "Dolphin-S_2016",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Dolphin-S_2016",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Koo2008_HigherOrderFeatures",
      "label": "Koo2008_HigherOrderFeatures",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Koo2008_HigherOrderFeatures",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Seo2014_DiagramUnderstanding",
      "label": "Seo2014_DiagramUnderstanding",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Seo2014_DiagramUnderstanding",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Wang2018_TranslatingMathWordProblem",
      "label": "Wang2018_TranslatingMathWordProblem",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Wang2018_TranslatingMathWordProblem",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Zhang2019_DeepNeuralSolver",
      "label": "Zhang2019_DeepNeuralSolver",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Zhang2019_DeepNeuralSolver",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "YahooAnswers",
      "label": "YahooAnswers",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "YahooAnswers",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Nivre2004_ArcStandardSystem",
      "label": "Nivre2004_ArcStandardSystem",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Nivre2004_ArcStandardSystem",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Dolphin-S",
      "label": "Dolphin-S",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Dolphin-S",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Roth2004_LinearProgrammingFormulation",
      "label": "Roth2004_LinearProgrammingFormulation",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Roth2004_LinearProgrammingFormulation",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "MSTParser",
      "label": "MSTParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "MSTParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "LinearT6",
      "label": "LinearT6",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "LinearT6",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "McDonald2005_MSTParser",
      "label": "McDonald2005_MSTParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "McDonald2005_MSTParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "DRAW1K_2015",
      "label": "DRAW1K_2015",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "DRAW1K_2015",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "MaltParser",
      "label": "MaltParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "MaltParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Chinese_Arithmetic_Word_Problems_2015",
      "label": "Chinese_Arithmetic_Word_Problems_2015",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Chinese_Arithmetic_Word_Problems_2015",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Bowman2015_LargeAnnotatedCorpus",
      "label": "Bowman2015_LargeAnnotatedCorpus",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Bowman2015_LargeAnnotatedCorpus",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "StandardPrimarySchoolTestQuestions",
      "label": "StandardPrimarySchoolTestQuestions",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "StandardPrimarySchoolTestQuestions",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Henderson2004_NeuralNetworkParser",
      "label": "Henderson2004_NeuralNetworkParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Henderson2004_NeuralNetworkParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Feng2015_ApplyingDeepLearning",
      "label": "Feng2015_ApplyingDeepLearning",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Feng2015_ApplyingDeepLearning",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Accuracy",
      "label": "Accuracy",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Accuracy",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Kushman2014_AlgebraSolver",
      "label": "Kushman2014_AlgebraSolver",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Kushman2014_AlgebraSolver",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Garg2011_TemporalRestrictedBoltzmanMachine",
      "label": "Garg2011_TemporalRestrictedBoltzmanMachine",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Garg2011_TemporalRestrictedBoltzmanMachine",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Sachan2017_TextbookKnowledge",
      "label": "Sachan2017_TextbookKnowledge",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Sachan2017_TextbookKnowledge",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Munkhdalai2016_NeuralSemanticEncoders",
      "label": "Munkhdalai2016_NeuralSemanticEncoders",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Munkhdalai2016_NeuralSemanticEncoders",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "DosSantos2014_DeepConvolutionalNeuralNetworks",
      "label": "DosSantos2014_DeepConvolutionalNeuralNetworks",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "DosSantos2014_DeepConvolutionalNeuralNetworks",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "AllArith",
      "label": "AllArith",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "AllArith",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Stenetorp2013_RecursiveNeuralNetworks",
      "label": "Stenetorp2013_RecursiveNeuralNetworks",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Stenetorp2013_RecursiveNeuralNetworks",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Yu2009_StructuralSVM",
      "label": "Yu2009_StructuralSVM",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Yu2009_StructuralSVM",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Collins2003_HeadDrivenModel",
      "label": "Collins2003_HeadDrivenModel",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Collins2003_HeadDrivenModel",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Charniak1969_ComputerSolution",
      "label": "Charniak1969_ComputerSolution",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Charniak1969_ComputerSolution",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "MAWPS-S",
      "label": "MAWPS-S",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "MAWPS-S",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Yu2015_SyntaxSemanticsModel",
      "label": "Yu2015_SyntaxSemanticsModel",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Yu2015_SyntaxSemanticsModel",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "VQADataset",
      "label": "VQADataset",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "VQADataset",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Finley2005_SupervisedClustering",
      "label": "Finley2005_SupervisedClustering",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Finley2005_SupervisedClustering",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Hill2016_LearningDistributedRepresentations",
      "label": "Hill2016_LearningDistributedRepresentations",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Hill2016_LearningDistributedRepresentations",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "LinearT2",
      "label": "LinearT2",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "LinearT2",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "KoncelKedziorski2015_ALGES",
      "label": "KoncelKedziorski2015_ALGES",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "KoncelKedziorski2015_ALGES",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Hinton2012_Dropout",
      "label": "Hinton2012_Dropout",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Hinton2012_Dropout",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Huang2009_BilinguallyConstrainedParsing",
      "label": "Huang2009_BilinguallyConstrainedParsing",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Huang2009_BilinguallyConstrainedParsing",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Zhang2011_TransitionBasedParser",
      "label": "Zhang2011_TransitionBasedParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Zhang2011_TransitionBasedParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Carreras2007_ProjectivizationProcedure",
      "label": "Carreras2007_ProjectivizationProcedure",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Carreras2007_ProjectivizationProcedure",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Geometry_Problems",
      "label": "Geometry_Problems",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Geometry_Problems",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Henderson2007_SigmoidBeliefNetworks",
      "label": "Henderson2007_SigmoidBeliefNetworks",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Henderson2007_SigmoidBeliefNetworks",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Munkhdalai2016_NeuralTreeIndexers",
      "label": "Munkhdalai2016_NeuralTreeIndexers",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Munkhdalai2016_NeuralTreeIndexers",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Tan2016_ImprovedRepresentationLearning",
      "label": "Tan2016_ImprovedRepresentationLearning",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Tan2016_ImprovedRepresentationLearning",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Collins2002_DiscriminativeTraining",
      "label": "Collins2002_DiscriminativeTraining",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Collins2002_DiscriminativeTraining",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Huang2013_FeatureSelection",
      "label": "Huang2013_FeatureSelection",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Huang2013_FeatureSelection",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Dolphin18K",
      "label": "Dolphin18K",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Dolphin18K",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "SATGeometryQuestions",
      "label": "SATGeometryQuestions",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "SATGeometryQuestions",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Bowman2016_FastUnifiedModel",
      "label": "Bowman2016_FastUnifiedModel",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Bowman2016_FastUnifiedModel",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Nivre2007_SharedTask",
      "label": "Nivre2007_SharedTask",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Nivre2007_SharedTask",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "MathDQN",
      "label": "MathDQN",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "MathDQN",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Yamada2003_HeadFindingRules",
      "label": "Yamada2003_HeadFindingRules",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Yamada2003_HeadFindingRules",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Collobert2011_DeepLearningParsing",
      "label": "Collobert2011_DeepLearningParsing",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Collobert2011_DeepLearningParsing",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Stanford_CoreNLP",
      "label": "Stanford_CoreNLP",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Stanford_CoreNLP",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "AI2_Translated_to_Arabic",
      "label": "AI2_Translated_to_Arabic",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "AI2_Translated_to_Arabic",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Sagae2006a_BestFirstProbabilisticParser",
      "label": "Sagae2006a_BestFirstProbabilisticParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Sagae2006a_BestFirstProbabilisticParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Zhang2008_BeamSearchParser",
      "label": "Zhang2008_BeamSearchParser",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Zhang2008_BeamSearchParser",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    },
    {
      "id": "Palangi2016_DeepSentenceEmbedding",
      "label": "Palangi2016_DeepSentenceEmbedding",
      "type": "Unknown",
      "entity_type": "Unknown",
      "data": {
        "name": "Palangi2016_DeepSentenceEmbedding",
        "placeholder": true,
        "entity_type": "Unknown"
      }
    }
  ],
  "edges": [
    {
      "id": "edge_0",
      "source": "Roy2015_ExpressionTree",
      "target": "Wang2018_TranslatingMathWordProblem",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "Expression Tree construction extended with Seq2Seq model",
            "problem_addressed": "",
            "evidence": "Seq2SeqET extended the idea of DNS by using expression tree as the output sequence (Wang et al., 2018).",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_1",
      "source": "KoncelKedziorski2015_ALGES",
      "target": "Wang2018_TranslatingMathWordProblem",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Mechanism",
            "detail": "Optimized equation tree construction with Seq2Seq model",
            "problem_addressed": "",
            "evidence": "ALGES adopts a more brutal-force manner to exploit all the possible equation trees, which incurs higher computation cost. Wang et al. (2018) optimized this with Seq2Seq model.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_2",
      "source": "Huang2017_FGExpression",
      "target": "Zhang2019_DeepNeuralSolver",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Components",
            "detail": "Improved template parsing with deep learning",
            "problem_addressed": "",
            "evidence": "FG-Expression parses an equation template into fine-grained units, while DNS uses deep learning to avoid non-trivial feature engineering (Wang et al., 2017).",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_3",
      "source": "Wang2018_MathDQN",
      "target": "Zhang2019_DeepNeuralSolver",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology.Training_Strategy",
            "detail": "Extended MathDQN's reinforcement learning with deep Q-network",
            "problem_addressed": "",
            "evidence": "MathDQN models the tree construction as Markov Decision Process and leverages the strengths of deep Q-network (Wang et al., 2018). DNS further extends this with a seq2seq model.",
            "confidence": 0.89
          }
        ]
      }
    },
    {
      "id": "edge_4",
      "source": "Huang2018_NeuralMathWordProblemSolver",
      "target": "Zhang2019_DeepNeuralSolver",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Mechanism",
            "detail": "Optimized neural architecture with gated recurrent units",
            "problem_addressed": "",
            "evidence": "Neural Math Word Problem Solver uses GRU for efficient training, while DNS uses LSTM and GRU for better performance (Huang et al., 2018).",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_5",
      "source": "Roy2017_UnitDependencyGraph",
      "target": "Zhang2019_DeepNeuralSolver",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Feature_Processing",
            "detail": "Replaced unit dependency graph with deep learning",
            "problem_addressed": "",
            "evidence": "UnitDep requires additional annotation overhead to induce UDGs, while DNS uses deep learning to automatically learn features (Roy et al., 2017).",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_6",
      "source": "Chiang2018_SemanticallyAlignedEquationGeneration",
      "target": "Zhang2019_DeepNeuralSolver",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "Extended semantically-aligned equation generation with deep learning",
            "problem_addressed": "",
            "evidence": "Semantically-Aligned Equation Generation uses LSTM and self-attention, while DNS extends this with a seq2seq model (Chiang et al., 2018).",
            "confidence": 0.87
          }
        ]
      }
    },
    {
      "id": "edge_7",
      "source": "AI2_2014",
      "target": "Math23K_2017",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended AI2 dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Math23K contains Chinese math word problems for elementary school students, extending the scope of AI2 (Wang et al., 2017).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_8",
      "source": "IL_2015",
      "target": "Math23K_2017",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended IL dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Math23K contains Chinese math word problems for elementary school students, extending the scope of IL (Wang et al., 2017).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_9",
      "source": "CC_2015",
      "target": "Math23K_2017",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended CC dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Math23K contains Chinese math word problems for elementary school students, extending the scope of CC (Wang et al., 2017).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_10",
      "source": "Zhang2019_DeepNeuralSolver",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Used Math23K for evaluation",
            "problem_addressed": "",
            "evidence": "DNS was evaluated on the large-scale Math23K dataset (Wang et al., 2017).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_11",
      "source": "Wang2018_TranslatingMathWordProblem",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Used Math23K for evaluation",
            "problem_addressed": "",
            "evidence": "Translating Math Word Problem to Expression Tree was evaluated on the large-scale Math23K dataset (Wang et al., 2018).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_12",
      "source": "Wang2018_MathDQN",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Used Math23K for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the large-scale Math23K dataset (Wang et al., 2018).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_13",
      "source": "Huang2018_NeuralMathWordProblemSolver",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Used Math23K for evaluation",
            "problem_addressed": "",
            "evidence": "Neural Math Word Problem Solver was evaluated on the large-scale Math23K dataset (Huang et al., 2018).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_14",
      "source": "Zhang2019_DeepNeuralSolver",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Use",
            "detail": "Used Accuracy for evaluation",
            "problem_addressed": "",
            "evidence": "DNS reports accuracy on various datasets (Wang et al., 2017).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_15",
      "source": "Wang2018_TranslatingMathWordProblem",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Use",
            "detail": "Used Accuracy for evaluation",
            "problem_addressed": "",
            "evidence": "Translating Math Word Problem to Expression Tree reports accuracy on various datasets (Wang et al., 2018).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_16",
      "source": "Wang2018_MathDQN",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "MathDQN uses Accuracy metric for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated using the Accuracy metric, which measures the proportion of correctly solved problems.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_17",
      "source": "Huang2018_NeuralMathWordProblemSolver",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Use",
            "detail": "Used Accuracy for evaluation",
            "problem_addressed": "",
            "evidence": "Neural Math Word Problem Solver reports accuracy on various datasets (Huang et al., 2018).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_18",
      "source": "Seo2014_GeoSolver",
      "target": "Dolphin18K_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Used Dolphin18K for evaluation",
            "problem_addressed": "",
            "evidence": "GEOS was evaluated on the large-scale Dolphin18K dataset (Seo et al., 2015).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_19",
      "source": "Seo2015_GEOS",
      "target": "Dolphin18K_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "GEOS uses Dolphin18K dataset for evaluation",
            "problem_addressed": "",
            "evidence": "GEOS was evaluated on the Dolphin18K dataset, which contains geometry word problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_20",
      "source": "Huang2017_FGExpression",
      "target": "Dolphin18K_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "FG-Expression uses Dolphin18K dataset for evaluation",
            "problem_addressed": "",
            "evidence": "FG-Expression was evaluated on the Dolphin18K dataset, which contains 18,460 problems and 5,871 templates.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_21",
      "source": "Seo2014_DiagramUnderstanding",
      "target": "Seo2015_GEOS",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "Extended diagram understanding with text-parser",
            "problem_addressed": "",
            "evidence": "Diagram Understanding was extended to GEOS, which combines text and diagram parsing (Seo et al., 2015).",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_22",
      "source": "Seo2015_GEOS",
      "target": "Sachan2017_TextbookKnowledge",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Methodology.Training_Strategy",
            "detail": "Improved reasoning engine with axiomatic knowledge",
            "problem_addressed": "",
            "evidence": "GEOS was improved by harvesting axiomatic knowledge from textbooks (Sachan et al., 2017).",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_23",
      "source": "Seo2015_GEOS",
      "target": "Alvin2017_GeoShader",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "Extended with hypergraph for shaded area reasoning",
            "problem_addressed": "",
            "evidence": "GEOS was extended to GeoShader, which handles shaded area problems using hypergraph (Alvin et al., 2017).",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_24",
      "source": "AI2_2014",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset",
            "detail": "Dolphin18K extends AI2 by providing a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems and 5,871 templates, providing a larger and more diverse dataset compared to AI2.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_25",
      "source": "IL_2015",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended IL dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains a large number of problems and templates, extending the scope of IL (Huang et al., 2016).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_26",
      "source": "CC_2015",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended CC dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains a large number of problems and templates, extending the scope of CC (Huang et al., 2016).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_27",
      "source": "ALG514_2014",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended ALG514 dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains a large number of problems and templates, extending the scope of ALG514 (Huang et al., 2016).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_28",
      "source": "Dolphin1878_2015",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended Dolphin1878 dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains a large number of problems and templates, extending the scope of Dolphin1878 (Huang et al., 2016).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_29",
      "source": "DRAW1K_2016",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Extended DRAW1K dataset with more complex problems",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains a large number of problems and templates, extending the scope of DRAW1K (Huang et al., 2016).",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_30",
      "source": "Roy2015_ExpressionTree",
      "target": "Zhang2017_DeepNeuralSolver",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Architecture.Mechanism",
            "detail": "Expression Tree's rule-based and tree-based approach replaced by Deep Neural Solver's neural sequence-to-sequence model",
            "problem_addressed": "",
            "evidence": "Deep Neural Solver (DNS) is the first deep learning based algorithm that does not rely on hand-crafted features... This is a milestone contribution because all the previous methods (including MathDQN) require human intelligence to help extract features that are effective.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_31",
      "source": "KoncelKedziorski2015_ALGES",
      "target": "Zhang2017_DeepNeuralSolver",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Architecture.Mechanism",
            "detail": "ALGES's Integer Linear Programming and Beam Search replaced by DNS's neural sequence-to-sequence model",
            "problem_addressed": "",
            "evidence": "Deep Neural Solver (DNS) is the first deep learning based algorithm that does not rely on hand-crafted features... This is a milestone contribution because all the previous methods (including MathDQN) require human intelligence to help extract features that are effective.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_32",
      "source": "Huang2017_FGExpression",
      "target": "Zhang2017_DeepNeuralSolver",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Feature_Processing",
            "detail": "FG-Expression's fine-grained feature extraction optimized by DNS's end-to-end neural model",
            "problem_addressed": "",
            "evidence": "This is a milestone contribution because all the previous methods (including MathDQN) require human intelligence to help extract features that are effective.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_33",
      "source": "Wang2018_MathDQN",
      "target": "Zhang2017_DeepNeuralSolver",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology.Training_Strategy",
            "detail": "MathDQN extends DNS by incorporating deep reinforcement learning for arithmetic word problems",
            "problem_addressed": "",
            "evidence": "MathDQN: Solving Arithmetic Word Problems via Deep Reinforcement Learning... It formulates the expression tree construction as a Markov Decision Process and leverages the strengths of deep Q-network (DQN).",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_34",
      "source": "Seo2015_GEOS",
      "target": "Zhang2017_DeepNeuralSolver",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Task.Domain",
            "detail": "GEOS extends DNS by focusing on geometry word problems requiring text and diagram interpretation",
            "problem_addressed": "",
            "evidence": "GEOS: Solving Geometry Problems: Combining Text and Diagram Interpretation... It combines text and diagram interpretation to solve geometry word problems.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_35",
      "source": "Roy2015_ExpressionTree",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Expression Tree uses AI2 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "Expression Tree was evaluated on the AI2 dataset, which contains arithmetic word problems for third, fourth, and fifth graders.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_36",
      "source": "KoncelKedziorski2015_ALGES",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "ALGES uses AI2 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_37",
      "source": "Wang2018_MathDQN",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "MathDQN uses AI2 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the AI2 dataset, which contains arithmetic word problems for third, fourth, and fifth graders.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_38",
      "source": "Wang2018_MathDQN",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on IL dataset",
            "problem_addressed": "",
            "evidence": "Wang2018_MathDQN evaluates its performance on the IL dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_39",
      "source": "Wang2018_MathDQN",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on CC dataset",
            "problem_addressed": "",
            "evidence": "Wang2018_MathDQN evaluates its performance on the CC dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_40",
      "source": "Wang2018_MathDQN",
      "target": "SingleEQ_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MathDQN uses SingleEQ dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_41",
      "source": "Wang2018_MathDQN",
      "target": "AllArith_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MathDQN uses AllArith dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_42",
      "source": "Wang2018_MathDQN",
      "target": "Dolphin-S_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MathDQN uses Dolphin-S dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_43",
      "source": "Wang2018_MathDQN",
      "target": "MAWPS-S_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MathDQN uses MAWPS-S dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_44",
      "source": "Zhang2017_DeepNeuralSolver",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Deep Neural Solver uses Math23K dataset for evaluation",
            "problem_addressed": "",
            "evidence": "Math23K is another large-scale dataset which contains Chinese math word problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_45",
      "source": "Upadhyay2016_MixedSP",
      "target": "ALG514_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MixedSP uses ALG514 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_46",
      "source": "Upadhyay2016_MixedSP",
      "target": "DRAW1K_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MixedSP uses DRAW1K dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_47",
      "source": "Yu2015_SyntaxSemanticsModel",
      "target": "Chinese_Arithmetic_Word_Problems_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Syntax-Semantics Model uses Chinese Arithmetic Word Problems dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The accuracy of arithmetic word problems is evaluated on the datasets that are manually harvested and annotated from online websites.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_48",
      "source": "Zhang2017_DeepNeuralSolver",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Use",
            "detail": "Deep Neural Solver uses Accuracy as an evaluation metric",
            "problem_addressed": "",
            "evidence": "The accuracies of existing approaches for equation set problems degrade sharply to less than 25%.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_49",
      "source": "Roy2015_ExpressionTree",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Use",
            "detail": "Expression Tree uses Accuracy as an evaluation metric",
            "problem_addressed": "",
            "evidence": "The accuracies of existing approaches for equation set problems degrade sharply to less than 25%.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_50",
      "source": "KoncelKedziorski2015_ALGES",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Use",
            "detail": "ALGES uses Accuracy as an evaluation metric",
            "problem_addressed": "",
            "evidence": "The accuracies of existing approaches for equation set problems degrade sharply to less than 25%.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_51",
      "source": "Huang2017_FGExpression",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "FG-Expression uses Accuracy metric for evaluation",
            "problem_addressed": "",
            "evidence": "FG-Expression was evaluated using the Accuracy metric, which measures the proportion of correctly solved problems.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_52",
      "source": "Seo2015_GEOS",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "GEOS uses Accuracy metric for evaluation",
            "problem_addressed": "",
            "evidence": "GEOS was evaluated using the Accuracy metric, which measures the proportion of correctly solved problems.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_53",
      "source": "Upadhyay2016_MixedSP",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Use",
            "detail": "MixedSP uses Accuracy as an evaluation metric",
            "problem_addressed": "",
            "evidence": "The accuracies of existing approaches for equation set problems degrade sharply to less than 25%.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_54",
      "source": "Roy2015_ExpressionTree",
      "target": "Wang2018_TranslationToExpressionTree",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Connections",
            "detail": "Beam Search replaces Bottom-Up Construction",
            "problem_addressed": "",
            "evidence": "ExpressionTree uses beam search for efficiency concerns, while Wang2018_TranslationToExpressionTree employs a seq2seq model with beam search for better performance.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_55",
      "source": "ALGES2015_EquationTree",
      "target": "Wang2018_TranslationToExpressionTree",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "Expression Tree extended to Equation Tree",
            "problem_addressed": "",
            "evidence": "ALGES constructs equation trees, while Wang2018_TranslationToExpressionTree extends this idea to translate math word problems into expression trees.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_56",
      "source": "UnitDep2017_UnitDependencyGraph",
      "target": "Wang2018_TranslationToExpressionTree",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "Uses Unit Dependency Graph for feature extraction",
            "problem_addressed": "",
            "evidence": "UnitDep introduces unit dependency graphs for feature extraction, which Wang2018_TranslationToExpressionTree can utilize.",
            "confidence": 0.82
          }
        ]
      }
    },
    {
      "id": "edge_57",
      "source": "DNS2017_DeepNeuralSolver",
      "target": "Huang2017_NeuralMathWordProblemSolver",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "Adds fine-grained expressions and policy gradient",
            "problem_addressed": "",
            "evidence": "DNS2017_DeepNeuralSolver uses a seq2seq model, while Huang2017_NeuralMathWordProblemSolver extends this with fine-grained expressions and policy gradient for reinforcement learning.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_58",
      "source": "DNS2017_DeepNeuralSolver",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on Math23K dataset",
            "problem_addressed": "",
            "evidence": "DNS2017_DeepNeuralSolver evaluates its performance on the Math23K dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_59",
      "source": "Math23K_2017",
      "target": "Dolphin18K",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Dolphin18K is a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems, expanding on the 23,162 problems in Math23K.",
            "confidence": 0.87
          }
        ]
      }
    },
    {
      "id": "edge_60",
      "source": "Huang2017_NeuralMathWordProblemSolver",
      "target": "Dolphin18K",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on Dolphin18K dataset",
            "problem_addressed": "",
            "evidence": "Huang2017_NeuralMathWordProblemSolver evaluates its performance on the Dolphin18K dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_61",
      "source": "Wang2018_MathDQN",
      "target": "MathDQN",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Architecture.Mechanisms",
            "detail": "Replaces traditional methods with deep reinforcement learning",
            "problem_addressed": "",
            "evidence": "Wang2018_MathDQN uses deep Q-networks and reinforcement learning to solve arithmetic word problems, replacing traditional methods.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_62",
      "source": "CASS2018_ReinforcementLearningSolver",
      "target": "MathDQN",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "Introduces copy and alignment mechanisms",
            "problem_addressed": "",
            "evidence": "CASS2018_ReinforcementLearningSolver extends MathDQN by introducing copy and alignment mechanisms.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_63",
      "source": "Roy2015_ExpressionTree",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on IL dataset",
            "problem_addressed": "",
            "evidence": "Roy2015_ExpressionTree evaluates its performance on the IL dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_64",
      "source": "Roy2015_ExpressionTree",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on CC dataset",
            "problem_addressed": "",
            "evidence": "Roy2015_ExpressionTree evaluates its performance on the CC dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_65",
      "source": "ALGES2015_EquationTree",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on AI2 dataset",
            "problem_addressed": "",
            "evidence": "ALGES2015_EquationTree evaluates its performance on the AI2 dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_66",
      "source": "ALGES2015_EquationTree",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on IL dataset",
            "problem_addressed": "",
            "evidence": "ALGES2015_EquationTree evaluates its performance on the IL dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_67",
      "source": "ALGES2015_EquationTree",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on CC dataset",
            "problem_addressed": "",
            "evidence": "ALGES2015_EquationTree evaluates its performance on the CC dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_68",
      "source": "UnitDep2017_UnitDependencyGraph",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on AI2 dataset",
            "problem_addressed": "",
            "evidence": "UnitDep2017_UnitDependencyGraph evaluates its performance on the AI2 dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_69",
      "source": "UnitDep2017_UnitDependencyGraph",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on IL dataset",
            "problem_addressed": "",
            "evidence": "UnitDep2017_UnitDependencyGraph evaluates its performance on the IL dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_70",
      "source": "UnitDep2017_UnitDependencyGraph",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on CC dataset",
            "problem_addressed": "",
            "evidence": "UnitDep2017_UnitDependencyGraph evaluates its performance on the CC dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_71",
      "source": "CASS2018_ReinforcementLearningSolver",
      "target": "Dolphin18K",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on Dolphin18K dataset",
            "problem_addressed": "",
            "evidence": "CASS2018_ReinforcementLearningSolver evaluates its performance on the Dolphin18K dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_72",
      "source": "Wang2018_TranslationToExpressionTree",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on AI2 dataset",
            "problem_addressed": "",
            "evidence": "Wang2018_TranslationToExpressionTree evaluates its performance on the AI2 dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_73",
      "source": "Wang2018_TranslationToExpressionTree",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on IL dataset",
            "problem_addressed": "",
            "evidence": "Wang2018_TranslationToExpressionTree evaluates its performance on the IL dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_74",
      "source": "Wang2018_TranslationToExpressionTree",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on CC dataset",
            "problem_addressed": "",
            "evidence": "Wang2018_TranslationToExpressionTree evaluates its performance on the CC dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_75",
      "source": "Chiang2018_SemanticallyAlignedEquationGeneration",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on AI2 dataset",
            "problem_addressed": "",
            "evidence": "Chiang2018_SemanticallyAlignedEquationGeneration evaluates its performance on the AI2 dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_76",
      "source": "Chiang2018_SemanticallyAlignedEquationGeneration",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on IL dataset",
            "problem_addressed": "",
            "evidence": "Chiang2018_SemanticallyAlignedEquationGeneration evaluates its performance on the IL dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_77",
      "source": "Chiang2018_SemanticallyAlignedEquationGeneration",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Evaluates on CC dataset",
            "problem_addressed": "",
            "evidence": "Chiang2018_SemanticallyAlignedEquationGeneration evaluates its performance on the CC dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_78",
      "source": "AI2_2014",
      "target": "Dolphin18K",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Dolphin18K extends AI2 with a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems and 5,871 templates, making it a larger and more diversified dataset compared to AI2.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_79",
      "source": "IL_2015",
      "target": "Dolphin18K",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Dolphin18K extends IL with a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems and 5,871 templates, making it a larger and more diversified dataset compared to IL.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_80",
      "source": "CC_2015",
      "target": "Dolphin18K",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Size",
            "detail": "Dolphin18K extends CC with a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems and 5,871 templates, making it a larger and more diversified dataset compared to CC.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_81",
      "source": "Huang2017_NeuralMathWordProblemSolver",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "Evaluates using Accuracy metric",
            "problem_addressed": "",
            "evidence": "Huang2017_NeuralMathWordProblemSolver reports accuracy on the Dolphin18K dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_82",
      "source": "Wang2018_TranslationToExpressionTree",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "Evaluates using Accuracy metric",
            "problem_addressed": "",
            "evidence": "Wang2018_TranslationToExpressionTree reports accuracy on the AI2, IL, and CC datasets.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_83",
      "source": "Chiang2018_SemanticallyAlignedEquationGeneration",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "Evaluates using Accuracy metric",
            "problem_addressed": "",
            "evidence": "Chiang2018_SemanticallyAlignedEquationGeneration reports accuracy on the AI2, IL, and CC datasets.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_84",
      "source": "CASS2018_ReinforcementLearningSolver",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "Evaluates using Accuracy metric",
            "problem_addressed": "",
            "evidence": "CASS2018_ReinforcementLearningSolver reports accuracy on the Dolphin18K dataset.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_85",
      "source": "Chang2013_L3M",
      "target": "Raghunathan2010_MultiPassSieve",
      "label": "Improve, Optimize",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "L3M model introduces latent variables and constraints to improve coreference resolution",
            "problem_addressed": "",
            "evidence": "We show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches as well as some structured prediction models proposed in the literature.",
            "confidence": 0.88
          },
          {
            "type": "Optimize",
            "structure": "Architecture.Connections",
            "detail": "L3M incorporates knowledge-based constraints into the coreference resolution process, enhancing the precision of the Multi-Pass Sieve approach",
            "problem_addressed": "",
            "evidence": "We show that L3M admits efficient inference and can be augmented with knowledge-based constraints; we also present a fast stochastic gradient based learning.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_86",
      "source": "Chen2014_NeuralParser",
      "target": "Roy2015_ARIS",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Task.Dataset",
            "detail": "Neural Dependency Parser is used for dependency parsing in the ARIS system",
            "problem_addressed": "",
            "evidence": "Our system, ARIS, analyzes each of the sentences in the problem statement to identify the relevant variables and their values.",
            "confidence": 0.75
          }
        ]
      }
    },
    {
      "id": "edge_87",
      "source": "Roy2015_ARIS",
      "target": "StandardPrimarySchoolTestQuestions",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Task.Dataset",
            "detail": "ARIS is evaluated on a corpus of standard primary school test questions",
            "problem_addressed": "",
            "evidence": "We report the first learning results on this task without reliance on pre-defined templates and make our data publicly available.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_88",
      "source": "Huang2016_LargeScaleDataset",
      "target": "YahooAnswers",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Task.Dataset",
            "detail": "Ranking SVM Model is evaluated on Yahoo! Answers dataset",
            "problem_addressed": "",
            "evidence": "Problems in the dataset are semi-automatically obtained from community question-answering (CQA) web pages.",
            "confidence": 0.89
          }
        ]
      }
    },
    {
      "id": "edge_89",
      "source": "Roy2018_DeclarativeKnowledge",
      "target": "Roy2015_ARIS",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Declarative Knowledge Mapping extends ARIS by incorporating declarative rules",
            "problem_addressed": "",
            "evidence": "Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression.",
            "confidence": 0.87
          }
        ]
      }
    },
    {
      "id": "edge_90",
      "source": "Wang2017_DeepNeuralSolver",
      "target": "Roy2015_ARIS",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Architecture.Mechanism",
            "detail": "Deep Neural Solver replaces ARIS with a neural sequence-to-sequence architecture",
            "problem_addressed": "",
            "evidence": "In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model.",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_91",
      "source": "Zhou2015_QuadraticProgramming",
      "target": "Kushman2014_AlgebraSolver",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Quadratic Programming improves upon Kushman et al.'s algebra solver by using a log-linear model",
            "problem_addressed": "",
            "evidence": "Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014).",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_92",
      "source": "Roy2015_GeneralArithmetic",
      "target": "Roy2018_DeclarativeKnowledge",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "General Arithmetic Solver is extended by incorporating declarative knowledge",
            "problem_addressed": "",
            "evidence": "This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations.",
            "confidence": 0.86
          }
        ]
      }
    },
    {
      "id": "edge_93",
      "source": "Seo2015_GEOS",
      "target": "SATGeometryQuestions",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Task.Dataset",
            "detail": "GEOS is evaluated on SAT geometry questions",
            "problem_addressed": "",
            "evidence": "In our experiments, GEOS achieves a 49% score on official SAT questions, and a score of 61% on practice questions.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_94",
      "source": "Wang2017_MathDQN",
      "target": "Roy2015_ARIS",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Mechanism",
            "detail": "MathDQN optimizes ARIS by using deep reinforcement learning",
            "problem_addressed": "",
            "evidence": "In this paper, we make the first attempt of applying deep reinforcement learning to solve arithmetic word problems.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_95",
      "source": "Roy2018_UnitDependencyGraph",
      "target": "Roy2015_ARIS",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Unit Dependency Graph extends ARIS by introducing a new component for handling units",
            "problem_addressed": "",
            "evidence": "This paper proposes a principled way to capture and reason about units and shows how it can benefit an arithmetic word problem solver.",
            "confidence": 0.89
          }
        ]
      }
    },
    {
      "id": "edge_96",
      "source": "Wang2019_SemanticallyAligned",
      "target": "Wang2017_DeepNeuralSolver",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Semantically-Aligned Equation Generation improves upon Deep Neural Solver by bridging symbolic and semantic worlds",
            "problem_addressed": "",
            "evidence": "Our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10% accuracy.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_97",
      "source": "Wang2019_TemplateBased",
      "target": "Wang2017_DeepNeuralSolver",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Template-Based Solver extends Deep Neural Solver by using a tree-structure template",
            "problem_addressed": "",
            "evidence": "To reduce the number of templates and improve the accuracy of template prediction, we proposed equation normalization and operator encapsulation.",
            "confidence": 0.87
          }
        ]
      }
    },
    {
      "id": "edge_98",
      "source": "Wang2017_TranslationModel",
      "target": "Wang2017_DeepNeuralSolver",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Mechanism",
            "detail": "Translation Model optimizes Deep Neural Solver by normalizing equations",
            "problem_addressed": "",
            "evidence": "By considering the uniqueness of expression tree, we propose an equation normalization method to normalize the duplicated equations.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_99",
      "source": "Antol2015_VQA",
      "target": "VQADataset",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Task.Dataset",
            "detail": "VQA is evaluated on VQA Dataset",
            "problem_addressed": "",
            "evidence": "We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides.",
            "confidence": 0.94
          }
        ]
      }
    },
    {
      "id": "edge_100",
      "source": "Roy2015_ExpressionTree",
      "target": "MathDQN",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Mechanism",
            "detail": "ExpressionTree's beam search replaced by MathDQN's reinforcement learning",
            "problem_addressed": "",
            "evidence": "MathDQN models the tree construction as Markov Decision Process and leverages the strengths of deep Q-network (DQN). By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_101",
      "source": "Huang2017_FineGrainedExpressions",
      "target": "MathDQN",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology.Training_Strategy",
            "detail": "Fine-Grained Expressions' log-linear model extended by MathDQN's reinforcement learning",
            "problem_addressed": "",
            "evidence": "MathDQN uses reinforcement learning to improve the accuracy of solving arithmetic word problems, which can be seen as an extension of the log-linear model used in Fine-Grained Expressions.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_102",
      "source": "Wang2017_DeepNeuralSolver",
      "target": "MathDQN",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Connections",
            "detail": "Deep Neural Solver's seq2seq model optimized by MathDQN's deep Q-network",
            "problem_addressed": "",
            "evidence": "MathDQN iteratively picks the best operator for two selected quantities, which can be viewed as beam search with k=1 when exploiting candidate expression trees. Its deep Q-network acts as the operator classifier and guides the model to select the most promising operator for tree construction.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_103",
      "source": "Zhang2015_ExpressionTree",
      "target": "MathDQN",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Connections",
            "detail": "ExpressionTree's beam search replaced by MathDQN's reinforcement learning mechanism",
            "problem_addressed": "",
            "evidence": "MathDQN iteratively picks the best operator for two selected quantities, which can be viewed as beam search with k=1. Its deep Q-network acts as the operator classifier and guides the model to select the most promising operator for tree construction.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_104",
      "source": "Huang2017_FGExpression",
      "target": "Zhang2015_ExpressionTree",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Feature_Processing",
            "detail": "FG-Expression introduces fine-grained units and template fragments to enhance feature extraction",
            "problem_addressed": "",
            "evidence": "FG-Expression parses an equation template into fine-grained units, called template fragment. Each template is represented in a tree structure and each fragment represents a sub-tree rooted at an internal node.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_105",
      "source": "Wang2018_MathDQN",
      "target": "Zhang2015_ExpressionTree",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Architecture.Mechanism",
            "detail": "MathDQN replaces ExpressionTree's binary classifier with a deep Q-network",
            "problem_addressed": "",
            "evidence": "MathDQN models the tree construction as Markov Decision Process and leverages the strengths of deep Q-network (DQN). By using a two-layer feed-forward neural network as the deep Q-network to approximate the Q-value function, the framework learns model parameters from the reward feedback of the environment.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_106",
      "source": "Roy2015_ALGES",
      "target": "Zhang2015_ExpressionTree",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "ALGES extends ExpressionTree by incorporating integer linear programming for syntactic validity",
            "problem_addressed": "",
            "evidence": "ALGES does not discard irrelevant quantities but enumerates all syntactically valid trees. Integer Linear Programming (ILP) is applied to enforce syntactic validity, type consistency, and domain-specific simplicity considerations.",
            "confidence": 0.89
          }
        ]
      }
    },
    {
      "id": "edge_107",
      "source": "Shi2015_DOL",
      "target": "Roy2015_ALGES",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "DOL extends ALGES by introducing a meaning representation language and CFG parser",
            "problem_addressed": "",
            "evidence": "DOL designs a meaning representation language called DOL and uses a context-free grammar (CFG) parser to transform textual sentences into DOL trees.",
            "confidence": 0.87
          }
        ]
      }
    },
    {
      "id": "edge_108",
      "source": "Upadhyay2016_MixedSP",
      "target": "Roy2015_ALGES",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Methodology.Training_Strategy",
            "detail": "MixedSP optimizes ALGES by using joint training with explicit and implicit supervision",
            "problem_addressed": "",
            "evidence": "MixedSP leverages a large number of algebra word problems with noisy and implicit supervision signals to improve a strong solver trained by fully annotated data.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_109",
      "source": "Seo2014_GeoSolver",
      "target": "Seo2015_GEOS",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "GeoSolver's diagram parser extended in GEOS to handle text and diagram interpretation",
            "problem_addressed": "",
            "evidence": "GEOS combines text and diagram interpretation to solve geometry problems, extending the capabilities of GeoSolver.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_110",
      "source": "Wang2017_DeepNeuralSolver",
      "target": "Zhang2015_ExpressionTree",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Architecture.Components",
            "detail": "Deep Neural Solver replaces ExpressionTree's components with a GRU encoder and seq2seq model",
            "problem_addressed": "",
            "evidence": "Deep Neural Solver uses a GRU encoder and seq2seq model to translate math word problems into equation templates, reducing the need for hand-crafted features.",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_111",
      "source": "Yu2015_ChineseEquationSet",
      "target": "Zhang2015_ExpressionTree",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "Chinese Equation Set Problem Solver extends ExpressionTree for Chinese language",
            "problem_addressed": "",
            "evidence": "The Syntax-Semantics (S2) model extracts quantity relations from Chinese problems, allowing for better handling of Chinese language nuances.",
            "confidence": 0.86
          }
        ]
      }
    },
    {
      "id": "edge_112",
      "source": "Siyam2017_ArabicArithmetic",
      "target": "Zhang2015_ExpressionTree",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "Arabic Arithmetic Word Problem Solver extends ExpressionTree for Arabic language",
            "problem_addressed": "",
            "evidence": "The proposed techniques rely on verb categorization and syntactic parser customization for the Arabic language.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_113",
      "source": "Accuracy_Classification",
      "target": "F1_Score_Classification",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Metric.Category",
            "detail": "F1 Score extends Accuracy by considering both precision and recall",
            "problem_addressed": "",
            "evidence": "F1 Score is the harmonic mean of precision and recall, providing a more balanced evaluation metric.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_114",
      "source": "Precision_Classification",
      "target": "F1_Score_Classification",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Metric.Category",
            "detail": "F1 Score extends Precision by considering recall",
            "problem_addressed": "",
            "evidence": "F1 Score is the harmonic mean of precision and recall, providing a more balanced evaluation metric.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_115",
      "source": "Recall_Classification",
      "target": "F1_Score_Classification",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Metric.Category",
            "detail": "F1 Score extends Recall by considering precision",
            "problem_addressed": "",
            "evidence": "F1 Score is the harmonic mean of precision and recall, providing a more balanced evaluation metric.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_116",
      "source": "Zhang2015_ExpressionTree",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "ExpressionTree uses AI2 for evaluation",
            "problem_addressed": "",
            "evidence": "ExpressionTree was evaluated on the AI2 dataset, which contains 395 arithmetic word problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_117",
      "source": "Zhang2015_ExpressionTree",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "ExpressionTree uses IL for evaluation",
            "problem_addressed": "",
            "evidence": "ExpressionTree was evaluated on the IL dataset, which contains 562 single-step word problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_118",
      "source": "Zhang2015_ExpressionTree",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "ExpressionTree uses CC for evaluation",
            "problem_addressed": "",
            "evidence": "ExpressionTree was evaluated on the CC dataset, which contains 600 multi-step math problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_119",
      "source": "MathDQN",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MathDQN uses AI2 for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the AI2 dataset, which contains 395 arithmetic word problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_120",
      "source": "MathDQN",
      "target": "IL_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MathDQN uses IL for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the IL dataset, which contains 562 single-step word problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_121",
      "source": "MathDQN",
      "target": "CC_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "MathDQN uses CC for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the CC dataset, which contains 600 multi-step math problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_122",
      "source": "Huang2017_FGExpression",
      "target": "Dolphin18K",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "FG-Expression uses Dolphin18K for evaluation",
            "problem_addressed": "",
            "evidence": "FG-Expression was evaluated on the Dolphin18K dataset, which contains 18,460 problems and 5,871 templates.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_123",
      "source": "Wang2017_DeepNeuralSolver",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Deep Neural Solver uses Math23K for evaluation",
            "problem_addressed": "",
            "evidence": "Deep Neural Solver was evaluated on the Math23K dataset, which contains 23,162 Chinese math word problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_124",
      "source": "Seo2015_GEOS",
      "target": "Geometry_Problems",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "GEOS uses Geometry Problems for evaluation",
            "problem_addressed": "",
            "evidence": "GEOS was evaluated on a dataset of geometry problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_125",
      "source": "Chiang2018_SemanticallyAligned",
      "target": "Math23K_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Semantically-Aligned Equation Generation uses Math23K for evaluation",
            "problem_addressed": "",
            "evidence": "Semantically-Aligned Equation Generation was evaluated on the Math23K dataset, which contains 23,162 Chinese math word problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_126",
      "source": "Yu2015_ChineseEquationSet",
      "target": "Chinese_Math_Problems",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Chinese Equation Set Problem Solver uses Chinese Math Problems for evaluation",
            "problem_addressed": "",
            "evidence": "The Chinese Equation Set Problem Solver was evaluated on a dataset of Chinese math problems.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_127",
      "source": "Siyam2017_ArabicArithmetic",
      "target": "AI2_Translated_to_Arabic",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Use",
            "detail": "Arabic Arithmetic Word Problem Solver uses AI2 Translated to Arabic for evaluation",
            "problem_addressed": "",
            "evidence": "The Arabic Arithmetic Word Problem Solver was evaluated on a dataset translated from AI2 to Arabic.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_128",
      "source": "Chen2014_NeuralParser",
      "target": "Raghunathan2010_MultiPassSieve",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "Neural Dependency Parser extends the Multi-Pass Sieve by using neural networks for dependency parsing",
            "problem_addressed": "",
            "evidence": "Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_129",
      "source": "Roy2015_ARIS",
      "target": "Roy2018_DeclarativeMapping",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "ARIS uses verb categorization for arithmetic word problems, which is extended by Declarative Mapping to include declarative rules for broader problem solving",
            "problem_addressed": "",
            "evidence": "This paper presents a novel approach to learning to solve simple arithmetic word problems. Our system, ARIS, analyzes each of the sentences in the problem statement to identify the relevant variables and their values.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_130",
      "source": "Huang2016_LargeScaleDataset",
      "target": "Huang2017_FineGrainedExpressions",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset.Metrics",
            "detail": "The large-scale dataset used in Huang2016 is extended to include fine-grained expressions for solving math word problems",
            "problem_addressed": "",
            "evidence": "We build a large-scale dataset which is more than 9 times the size of previous ones, and contains many more problem types.",
            "confidence": 0.89
          }
        ]
      }
    },
    {
      "id": "edge_131",
      "source": "Wang2017_DeepNeuralSolver",
      "target": "Wang2017_TemplateBasedSolver",
      "label": "Replace",
      "relation_type": "Replace",
      "data": {
        "relations": [
          {
            "type": "Replace",
            "structure": "Architecture.Mechanism",
            "detail": "Deep Neural Solver replaces the template-based approach with a neural sequence-to-sequence model",
            "problem_addressed": "",
            "evidence": "In contrast to previous statistical learning approaches, we directly translate math word problems to equation templates using a recurrent neural network (RNN) model, without sophisticated feature engineering.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_132",
      "source": "Zhou2015_QuadraticProgramming",
      "target": "Roy2015_GeneralArithmetic",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Mechanisms",
            "detail": "Quadratic Programming optimizes the decision surface for solving algebra word problems, which is used in General Arithmetic Solver",
            "problem_addressed": "",
            "evidence": "To obtain a robust decision surface, we train a log-linear model to make the margin between the correct assignments and the false ones as large as possible.",
            "confidence": 0.87
          }
        ]
      }
    },
    {
      "id": "edge_133",
      "source": "Roy2015_GeneralArithmetic",
      "target": "Roy2018_DeclarativeMapping",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "General Arithmetic Solver is extended by Declarative Mapping to incorporate declarative rules for better semantic alignment",
            "problem_addressed": "",
            "evidence": "We develop declarative rules which govern the translation of natural language description of these concepts to math expressions.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_134",
      "source": "Seo2015_GEOS",
      "target": "Seo2015_GEOS",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Architecture.Components",
            "detail": "GEOS uses both text understanding and diagram interpretation components",
            "problem_addressed": "",
            "evidence": "This paper introduces GEOS, the first automated system to solve unaltered SAT geometry questions by combining text understanding and diagram interpretation.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_135",
      "source": "Wang2017_MathDQN",
      "target": "Huang2018_ReinforcementLearning",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "MathDQN extends the use of reinforcement learning for solving arithmetic word problems",
            "problem_addressed": "",
            "evidence": "We propose incorporating copy and alignment mechanism to the sequence-to-sequence model (namely CASS) to address these shortcomings.",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_136",
      "source": "Wang2017_SemanticallyAligned",
      "target": "Wang2017_TemplateBasedSolver",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Connections",
            "detail": "Semantically-Aligned Equation Generation optimizes the template-based solver by aligning symbolic and semantic worlds",
            "problem_addressed": "",
            "evidence": "Our model significantly outperforms state-of-the-art single model and the best non-retrieval-based model over about 10% accuracy.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_137",
      "source": "Kwiatkowski2013_SemanticParsers",
      "target": "Seo2015_GEOS",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "Semantic Parsers with On-the-fly Ontology Matching extends the capabilities of GEOS for geometry problem solving",
            "problem_addressed": "",
            "evidence": "In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_138",
      "source": "Roy2015_ExpressionTree",
      "target": "Koncel-Kedziorski2015_ALGES",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "ALGES extends ExpressionTree by adopting a more comprehensive search space and using Integer Linear Programming for syntactic validity and type consistency checks",
            "problem_addressed": "",
            "evidence": "ALGES does not discard irrelevant quantities but enumerates all syntactically valid trees, and applies Integer Linear Programming to enforce constraints such as syntactic validity and type consistency.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_139",
      "source": "Koncel-Kedziorski2015_ALGES",
      "target": "Huang2017_FGExpression",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Components",
            "detail": "FG-Expression improves upon ALGES by parsing equation templates into fine-grained units and using a max-margin objective for training",
            "problem_addressed": "",
            "evidence": "FG-Expression parses an equation template into fine-grained units, called template fragments, and uses a max-margin objective to train the log-linear model.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_140",
      "source": "Zhang2017_DeepNeuralSolver",
      "target": "Wang2018_MathDQN",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Methodology.Training_Strategy",
            "detail": "MathDQN optimizes Deep Neural Solver by incorporating reinforcement learning and deep Q-networks",
            "problem_addressed": "",
            "evidence": "MathDQN models the tree construction as a Markov Decision Process and leverages the strengths of deep Q-network (DQN) to improve performance on multi-step problems.",
            "confidence": 0.94
          }
        ]
      }
    },
    {
      "id": "edge_141",
      "source": "Seo2014_GALINGER",
      "target": "Seo2015_GEOS",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "GEOS extends GALINGER by combining text and diagram interpretation for solving geometry word problems",
            "problem_addressed": "",
            "evidence": "GEOS combines text and diagram interpretation to address geometry word problems, while GALINGER focuses on primitive detection.",
            "confidence": 0.96
          }
        ]
      }
    },
    {
      "id": "edge_142",
      "source": "Zhang2017_DeepNeuralSolver",
      "target": "Math23K_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Deep Neural Solver uses Math23K dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The Deep Neural Solver was evaluated on the Math23K dataset, which contains Chinese math word problems for elementary school students.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_143",
      "source": "Roy2015_ExpressionTree",
      "target": "IL_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Expression Tree uses IL dataset for evaluation",
            "problem_addressed": "",
            "evidence": "Expression Tree was evaluated on the IL dataset, which contains single-step word problems with one operator.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_144",
      "source": "Roy2015_ExpressionTree",
      "target": "CC_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Expression Tree uses CC dataset for evaluation",
            "problem_addressed": "",
            "evidence": "Expression Tree was evaluated on the CC dataset, which contains multi-step math problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_145",
      "source": "Koncel-Kedziorski2015_ALGES",
      "target": "AI2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "ALGES uses AI2 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "ALGES was evaluated on the AI2 dataset, which contains arithmetic word problems for third, fourth, and fifth graders.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_146",
      "source": "Koncel-Kedziorski2015_ALGES",
      "target": "IL_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "ALGES uses IL dataset for evaluation",
            "problem_addressed": "",
            "evidence": "ALGES was evaluated on the IL dataset, which contains single-step word problems with one operator.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_147",
      "source": "Koncel-Kedziorski2015_ALGES",
      "target": "CC_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "ALGES uses CC dataset for evaluation",
            "problem_addressed": "",
            "evidence": "ALGES was evaluated on the CC dataset, which contains multi-step math problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_148",
      "source": "Wang2018_MathDQN",
      "target": "IL_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "MathDQN uses IL dataset for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the IL dataset, which contains single-step word problems with one operator.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_149",
      "source": "Wang2018_MathDQN",
      "target": "CC_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "MathDQN uses CC dataset for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the CC dataset, which contains multi-step math problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_150",
      "source": "Wang2018_MathDQN",
      "target": "SingleEQ",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "MathDQN uses SingleEQ dataset for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the SingleEQ dataset, which contains both single-step and multi-step arithmetic problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_151",
      "source": "Wang2018_MathDQN",
      "target": "AllArith",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "MathDQN uses AllArith dataset for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the AllArith dataset, which contains arithmetic word problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_152",
      "source": "Wang2018_MathDQN",
      "target": "Dolphin-S",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "MathDQN uses Dolphin-S dataset for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the Dolphin-S dataset, which contains arithmetic word problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_153",
      "source": "Wang2018_MathDQN",
      "target": "MAWPS-S",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "MathDQN uses MAWPS-S dataset for evaluation",
            "problem_addressed": "",
            "evidence": "MathDQN was evaluated on the MAWPS-S dataset, which contains arithmetic word problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_154",
      "source": "Seo2014_GALINGER",
      "target": "Dolphin18K_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "G-ALINGER uses Dolphin18K dataset for evaluation",
            "problem_addressed": "",
            "evidence": "G-ALINGER was evaluated on the Dolphin18K dataset, which contains geometry word problems.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_155",
      "source": "Seo2014_GALINGER",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "G-ALINGER uses Accuracy metric for evaluation",
            "problem_addressed": "",
            "evidence": "G-ALINGER was evaluated using the Accuracy metric, which measures the proportion of correctly solved problems.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_156",
      "source": "IL_2014",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset",
            "detail": "Dolphin18K extends IL by providing a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems and 5,871 templates, providing a larger and more diverse dataset compared to IL.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_157",
      "source": "CC_2014",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset",
            "detail": "Dolphin18K extends CC by providing a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems and 5,871 templates, providing a larger and more diverse dataset compared to CC.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_158",
      "source": "Math23K_2014",
      "target": "Dolphin18K_2016",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Dataset",
            "detail": "Dolphin18K extends Math23K by providing a larger and more diverse dataset",
            "problem_addressed": "",
            "evidence": "Dolphin18K contains 18,460 problems and 5,871 templates, providing a larger and more diverse dataset compared to Math23K.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_159",
      "source": "Accuracy_Classification",
      "target": "Accuracy_Arithmetic",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Metric",
            "detail": "Accuracy_Arithmetic extends Accuracy_Classification for arithmetic word problems",
            "problem_addressed": "",
            "evidence": "Accuracy_Arithmetic measures the proportion of correctly solved arithmetic word problems, extending the general Accuracy metric.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_160",
      "source": "Accuracy_Classification",
      "target": "Accuracy_EquationSet",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Metric",
            "detail": "Accuracy_EquationSet extends Accuracy_Classification for equation set problems",
            "problem_addressed": "",
            "evidence": "Accuracy_EquationSet measures the proportion of correctly solved equation set problems, extending the general Accuracy metric.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_161",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "L3M extended to CL3M by incorporating domain knowledge-based constraints",
            "problem_addressed": "",
            "evidence": "We augment L3M with domain-specific constraints, resulting in CL3M, which can leverage external knowledge to improve performance.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_162",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "L3M extended to PL3M by introducing a probabilistic model for mention-entity links",
            "problem_addressed": "",
            "evidence": "We generalize L3M to PL3M, which considers multiple left-links and captures the notion of a mention-to-cluster link.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_163",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "label": "Improve, Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "CL3M extended to CPL3M by incorporating probabilistic modeling",
            "problem_addressed": "",
            "evidence": "We extend CL3M to CPL3M by incorporating the probabilistic mechanisms of PL3M.",
            "confidence": 0.95
          },
          {
            "type": "Improve",
            "structure": "Architecture.Component",
            "detail": "Integrated probabilistic model into constrained latent left linking model",
            "problem_addressed": "",
            "evidence": "The CL3M integrates the probabilistic model of PL3M while maintaining domain knowledge-based constraints.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_164",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "ACE2004",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "L3M evaluated on ACE 2004 dataset",
            "problem_addressed": "",
            "evidence": "Experiments on ACE and Ontonotes show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_165",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "Ontonotes5_2012",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "L3M evaluated on Ontonotes-5.0 dataset",
            "problem_addressed": "",
            "evidence": "Experiments on ACE and Ontonotes show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_166",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "ACE2004",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "CL3M evaluated on ACE 2004 dataset",
            "problem_addressed": "",
            "evidence": "Experiments on ACE and Ontonotes show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_167",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "Ontonotes5_2012",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "CL3M evaluated on Ontonotes-5.0 dataset",
            "problem_addressed": "",
            "evidence": "Experiments on ACE and Ontonotes show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_168",
      "source": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "target": "ACE2004",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "PL3M evaluated on ACE 2004 dataset",
            "problem_addressed": "",
            "evidence": "Experiments on ACE and Ontonotes show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_169",
      "source": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "target": "Ontonotes5_2012",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "PL3M evaluated on Ontonotes-5.0 dataset",
            "problem_addressed": "",
            "evidence": "Experiments on ACE and Ontonotes show that L3M and its constrained version, CL3M, are more accurate than several state-of-the-art approaches.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_170",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "MUC_Coreference",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "L3M evaluated using MUC metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_171",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "BCUB_Coreference",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "L3M evaluated using BCUB metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_172",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "CEAF_EntityBased",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "L3M evaluated using Entity-based CEAF metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_173",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "MUC_Coreference",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "CL3M evaluated using MUC metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_174",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "BCUB_Coreference",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "CL3M evaluated using BCUB metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_175",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "CEAF_EntityBased",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "CL3M evaluated using Entity-based CEAF metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_176",
      "source": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "target": "MUC_Coreference",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "PL3M evaluated using MUC metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_177",
      "source": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "target": "BCUB_Coreference",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "PL3M evaluated using BCUB metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_178",
      "source": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "target": "CEAF_EntityBased",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "PL3M evaluated using Entity-based CEAF metric",
            "problem_addressed": "",
            "evidence": "We compare the systems using three popular metrics for coreference — MUC, BCUB, and Entity-based CEAF.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_179",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "Chang2013_LatentLeftLinkingModel",
      "label": "Improve, Extend",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Components",
            "detail": "CL3M improves upon L3M by incorporating constraints",
            "problem_addressed": "",
            "evidence": "CL3M, with just five constraints, compares favorably with other, more complicated, state-of-the-art algorithms on a variety of evaluation metrics.",
            "confidence": 0.95
          },
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "CL3M extends L3M with domain knowledge-based constraints",
            "problem_addressed": "",
            "evidence": "CL3M augments L3M with domain-specific constraints.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_180",
      "source": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "target": "Chang2013_LatentLeftLinkingModel",
      "label": "Improve, Extend",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanisms",
            "detail": "PL3M improves upon L3M by considering multiple left-links",
            "problem_addressed": "",
            "evidence": "PL3M, when tuning the value of γ, is a strictly more general model than L3M.",
            "confidence": 0.95
          },
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "PL3M extends L3M with probabilistic modeling",
            "problem_addressed": "",
            "evidence": "We present a probabilistic generalization of L3M that can take into account entity-mention links by considering multiple possible coreference links.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_181",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "Raghunathan2010_MultiPassSieve",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Methodology.Training_Strategy",
            "detail": "CL3M improves upon Multi-Pass Sieve by using a principled learning approach",
            "problem_addressed": "",
            "evidence": "Unlike the unsupervised learning approach of Multi-Pass Sieve, CL3M uses a principled max-margin approach.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_182",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "Bengtson2008_PairwiseCoreferenceModel",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Connections",
            "detail": "L3M extends Best-Left-Link approach with latent structural SVM",
            "problem_addressed": "",
            "evidence": "L3M admits efficient inference, linking each mention to a previously occurring mention to its left, much like the existing best-left-link inference models.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_183",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "Finley2005_SupervisedClustering",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Methodology.Training_Strategy",
            "detail": "L3M improves upon supervised clustering by using a max-margin approach",
            "problem_addressed": "",
            "evidence": "L3M uses a max-margin approach to learn w, achieving better performance compared to supervised clustering.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_184",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "Roth2004_LinearProgrammingFormulation",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "CL3M extends linear programming formulation with domain-specific constraints",
            "problem_addressed": "",
            "evidence": "CL3M augments L3M with knowledge-based constraints following Roth and Yih (2004).",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_185",
      "source": "Chang2013_LatentLeftLinkingModel",
      "target": "Yu2009_StructuralSVM",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "L3M extends latent structural SVM with stochastic gradient descent",
            "problem_addressed": "",
            "evidence": "We present a novel latent structural SVM approach, optimized using a fast stochastic gradient-based technique.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_186",
      "source": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "target": "Yu2009_StructuralSVM",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanisms",
            "detail": "PL3M extends latent structural SVM with probabilistic modeling",
            "problem_addressed": "",
            "evidence": "We present a probabilistic generalization of L3M that can take into account entity-mention links by considering multiple possible coreference links.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_187",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Goldberg2010_EasyFirstNonDirectionalParser",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Architecture.Mechanism",
            "detail": "Neural Network Parser optimizes parsing by using dense features instead of sparse indicator features",
            "problem_addressed": "",
            "evidence": "Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly. In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_188",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "English_Penn_Treebank_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "The neural network parser is evaluated on the English Penn Treebank dataset.",
            "problem_addressed": "",
            "evidence": "We conduct our experiments on the English Penn Treebank(PTB) and the Chinese Penn Treebank (CTB) datasets.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_189",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Chinese_Penn_Treebank_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "The neural network parser is evaluated on the Chinese Penn Treebank dataset.",
            "problem_addressed": "",
            "evidence": "We conduct our experiments on the English Penn Treebank(PTB) and the Chinese Penn Treebank (CTB) datasets.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_190",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "UAS_Parsing",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "The neural network parser uses the Unlabeled Attachment Score (UAS) as an evaluation metric.",
            "problem_addressed": "",
            "evidence": "On all datasets, we report unlabeled attachment scores(UAS) and labeled attachment scores (LAS) and punctuation is excluded in all evaluation metrics.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_191",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "LAS_Parsing",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "The neural network parser uses the Labeled Attachment Score (LAS) as an evaluation metric.",
            "problem_addressed": "",
            "evidence": "On all datasets, we report unlabeled attachment scores(UAS) and labeled attachment scores (LAS) and punctuation is excluded in all evaluation metrics.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_192",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "MaltParser",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser outperforms MaltParser in terms of both accuracy and speed.",
            "problem_addressed": "",
            "evidence": "Our parser even surpasses MaltParser using liblinear, which is known to be highly optimized, while our parser achieves much better accuracy.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_193",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "MSTParser",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser outperforms MSTParser in terms of speed.",
            "problem_addressed": "",
            "evidence": "Despite the fact that the graph-based MST-Parser achieves a similar result to ours on PTB (CoNLL dependencies), our parser is nearly 100 times faster.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_194",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Zhang2011_TransitionBasedParser",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser optimizes the transition-based dependency parsing approach by using dense features.",
            "problem_addressed": "",
            "evidence": "In this work, we address all of these problems by using dense features in place of the sparse indicator features.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_195",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Collobert2011_DeepLearningParsing",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser extends the use of dense representations for parsing.",
            "problem_addressed": "",
            "evidence": "This is inspired by the recent success of distributed word representations in many NLP tasks, e.g., POS tagging(Collobert et al., 2011).",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_196",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Henderson2004_NeuralNetworkParser",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser extends the use of neural networks for dependency parsing.",
            "problem_addressed": "",
            "evidence": "(Henderson, 2004) was the first to attempt to use neural networks in a broad-coverage Penn Treebank parser, using a simple synchrony network to predict parse decisions in a constituency parser.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_197",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Koo2008_HigherOrderFeatures",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm.FeatureProcessing",
            "detail": "The neural network parser improves upon higher-order feature usage by learning compact dense vector representations.",
            "problem_addressed": "",
            "evidence": "While in aggregate both lexicalized features and higher-order interaction term features are very important in improving the performance of these systems, nevertheless, there is insufficient data to correctly weight most such features.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_198",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Huang2013_FeatureSelection",
      "label": "Optimize",
      "relation_type": "Optimize",
      "data": {
        "relations": [
          {
            "type": "Optimize",
            "structure": "Algorithm.FeatureProcessing",
            "detail": "The neural network parser optimizes feature selection by using dense representations.",
            "problem_addressed": "",
            "evidence": "The use of many feature templates cause a less studied problem: in modern dependency parsers, most of the runtime is consumed not by the core parsing algorithm but in the feature extraction step.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_199",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "deMarneffe2006_TypedDependencyExtractor",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Algorithm.Component",
            "detail": "The neural network parser uses typed dependency representations.",
            "problem_addressed": "",
            "evidence": "We adopt two different dependency representations: CoNLL Syntactic Dependencies(CD) and Stanford Basic Dependencies(SD).",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_200",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Mikolov2013_SkipGram",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Algorithm.Component",
            "detail": "The neural network parser uses pre-trained word embeddings from Skip-gram.",
            "problem_addressed": "",
            "evidence": "We use the pre-trained word embeddings from(Collobert et al., 2011) for English and our trained 50-dimensional word2vec embeddings(Mikolov et al., 2013) on Wikipedia and Gigaword corpus for Chinese.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_201",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Hinton2012_Dropout",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Algorithm.Methodology",
            "detail": "The neural network parser uses dropout for regularization.",
            "problem_addressed": "",
            "evidence": "We also apply a dropout (Hinton et al., 2012) with 0.5 rate.",
            "confidence": 0.94
          }
        ]
      }
    },
    {
      "id": "edge_202",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Duchi2011_AdaptiveSubgradient",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Algorithm.Methodology",
            "detail": "The neural network parser uses AdaGrad for optimization.",
            "problem_addressed": "",
            "evidence": "We use mini-batched AdaGrad(Duchi et al., 2011) for optimization.",
            "confidence": 0.94
          }
        ]
      }
    },
    {
      "id": "edge_203",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Koo2008_SimpleSemiSupervised",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm.TrainingStrategy",
            "detail": "The neural network parser improves upon semi-supervised learning methods.",
            "problem_addressed": "",
            "evidence": "Simple semi-supervised dependency parsing(Terry Koo et al., 2008) has been successful in improving parsing performance.",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_204",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Socher2013_CompositionalVectorGrammar",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser extends the use of compositional vector grammars.",
            "problem_addressed": "",
            "evidence": "There have been a number of recent uses of deep learning for constituency parsing(Collobert, 2011; Socher et al., 2013).",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_205",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Nivre2004_ArcStandardSystem",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Algorithm.Component",
            "detail": "The neural network parser uses the arc-standard system for transition-based parsing.",
            "problem_addressed": "",
            "evidence": "In the arc-standard system, a configuration c=(s, b, A) consists of a stack s, a buffer b, and a set of dependency arcs A.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_206",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Henderson2007_SigmoidBeliefNetworks",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser improves upon previous neural network architectures for parsing.",
            "problem_addressed": "",
            "evidence": "These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical.",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_207",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Stenetorp2013_RecursiveNeuralNetworks",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser improves upon recursive neural networks for transition-based dependency parsing.",
            "problem_addressed": "",
            "evidence": "Most recently,(Stenetorp, 2013) attempted to build recursive neural networks for transition-based dependency parsing, however the empirical performance of his model is still unsatisfactory.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_208",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Collins2003_HeadDrivenModel",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm.Architecture",
            "detail": "The neural network parser improves upon head-driven statistical models.",
            "problem_addressed": "",
            "evidence": "Almost all current dependency parsers classify based on millions of sparse indicator features. Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly.",
            "confidence": 0.91
          }
        ]
      }
    },
    {
      "id": "edge_209",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "Haghighi2009_SimpleCoreferenceResolution",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Constrained Latent Left Linking Model improves upon Simple Coreference Resolution by incorporating domain knowledge-based constraints.",
            "problem_addressed": "",
            "evidence": "Our constrained latent left linking model incorporates domain knowledge-based constraints, which are not present in the simple coreference resolution model (Haghighi and Klein, 2009).",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_210",
      "source": "Sleator1993_LinkParser",
      "target": "Chen2014_NeuralNetworkParser",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Connections",
            "detail": "Neural network parser improves upon Link Parser by using dense representations and neural network mechanisms.",
            "problem_addressed": "",
            "evidence": "Our parser uses dense representations and neural network mechanisms, improving scalability and efficiency over constraint-based parsing methods like Link Parser.",
            "confidence": 0.87
          }
        ]
      }
    },
    {
      "id": "edge_211",
      "source": "Chen2014_NeuralNetworkParser",
      "target": "Garg2011_TemporalRestrictedBoltzmanMachine",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Neural network parser improves upon temporal restricted Boltzmann machines by using dense representations and a novel cube activation function.",
            "problem_addressed": "",
            "evidence": "Our parser builds on the transition-based dependency parser approach using a Temporal Restricted Boltzman Machine (Garg and Henderson, 2011) but uses dense representations and a novel cube activation function.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_212",
      "source": "Raghunathan2010_MultiPassSieve",
      "target": "Haghighi2009_SimpleCoreferenceResolution",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Multi-pass sieve architecture improves upon the single-pass model by applying tiers of coreference models from highest to lowest precision.",
            "problem_addressed": "",
            "evidence": "This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_213",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Lin2017_StructuredSelfAttention",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Structured Self-attentive Sentence Embedding extends to include multi-hop attention mechanism",
            "problem_addressed": "",
            "evidence": "We propose a self-attention mechanism for these sequential models to replace the max pooling or averaging step. Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_214",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Liang2016_TagBasedSolver",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Components",
            "detail": "Structured Self-attentive Sentence Embedding improves upon Tag-based Solver by using a matrix embedding",
            "problem_addressed": "",
            "evidence": "We propose a self-attention mechanism for these sequential models to replace the max pooling or averaging step. Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_215",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Hochreiter1997_LongShortTermMemory",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Structured Self-attentive Sentence Embedding extends the use of LSTM by adding a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_216",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Age_Dataset_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model is evaluated on the Age dataset.",
            "problem_addressed": "",
            "evidence": "We evaluate our model on 3 different datasets: the Age dataset, the Yelp dataset, and the Stanford Natural Language Inference (SNLI) Corpus.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_217",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Yelp_Dataset_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model is evaluated on the Yelp dataset.",
            "problem_addressed": "",
            "evidence": "We evaluate our model on 3 different datasets: the Age dataset, the Yelp dataset, and the Stanford Natural Language Inference (SNLI) Corpus.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_218",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "SNLI_Corpus_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model is evaluated on the SNLI Corpus.",
            "problem_addressed": "",
            "evidence": "We evaluate our model on 3 different datasets: the Age dataset, the Yelp dataset, and the Stanford Natural Language Inference (SNLI) Corpus.",
            "confidence": 0.98
          }
        ]
      }
    },
    {
      "id": "edge_219",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model uses classification accuracy as an evaluation metric.",
            "problem_addressed": "",
            "evidence": "Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks. We use classification accuracy as a measurement.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_220",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Chen2014_NeuralNetworkParser",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon Neural Network Parser by incorporating self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_221",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "DosSantos2016_AttentivePoolingNetworks",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding extends attentive pooling networks by using a matrix embedding and a penalization term.",
            "problem_addressed": "",
            "evidence": "We propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding.",
            "confidence": 0.93
          }
        ]
      }
    },
    {
      "id": "edge_222",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Bowman2015_LargeAnnotatedCorpus",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model uses the SNLI Corpus for textual entailment task.",
            "problem_addressed": "",
            "evidence": "We use the biggest dataset in textual entailment, the SNLI corpus (Bowman et al., 2015) for our evaluation on this task.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_223",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "DosSantos2014_DeepConvolutionalNeuralNetworks",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon deep convolutional neural networks by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_224",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Feng2015_ApplyingDeepLearning",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon applying deep learning to answer selection by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_225",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Palangi2016_DeepSentenceEmbedding",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon deep sentence embedding using LSTM by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_226",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Munkhdalai2016_NeuralSemanticEncoders",
      "label": "Improve, Use",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon neural semantic encoders by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          },
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model uses the SNLI Corpus for textual entailment task.",
            "problem_addressed": "",
            "evidence": "We use the biggest dataset in textual entailment, the SNLI corpus (Bowman et al., 2015) for our evaluation on this task.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_227",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Bowman2016_FastUnifiedModel",
      "label": "Improve, Use",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon fast unified model for parsing and sentence understanding by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          },
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model uses the SNLI Corpus for textual entailment task.",
            "problem_addressed": "",
            "evidence": "We use the biggest dataset in textual entailment, the SNLI corpus (Bowman et al., 2015) for our evaluation on this task.",
            "confidence": 0.97
          }
        ]
      }
    },
    {
      "id": "edge_228",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Munkhdalai2016_NeuralTreeIndexers",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon neural tree indexers by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_229",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Tan2016_ImprovedRepresentationLearning",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon improved representation learning for question answer matching by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_230",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "label": "Improve, Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model uses the same datasets as Constrained Latent Left Linking Model.",
            "problem_addressed": "",
            "evidence": "Both models use the ACE 2004 and Ontonotes-5.0 datasets.",
            "confidence": 0.9
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon constrained latent left linking model by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_231",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Mikolov2013_SkipGram",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "The Structured Self-attentive Sentence Embedding model uses word embeddings initialized with Skip-gram.",
            "problem_addressed": "",
            "evidence": "We use 100-dimensional word2vec as initialization for word embeddings, and tune the embedding during training across all of our experiments.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_232",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Pennington2014_GloVe",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "The Structured Self-attentive Sentence Embedding model uses GloVe for initializing word embeddings.",
            "problem_addressed": "",
            "evidence": "We use 300-dimensional GloVe word embedding to initialize word embeddings.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_233",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Hill2016_LearningDistributedRepresentations",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon learning distributed representations of sentences by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_234",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Lee2016_SequentialShortTextClassification",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon sequential short-text classification by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          }
        ]
      }
    },
    {
      "id": "edge_235",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "label": "Improve, Use",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon probabilistic latent left linking model by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          },
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model uses the same datasets as Probabilistic Latent Left Linking Model.",
            "problem_addressed": "",
            "evidence": "Both models use the ACE 2004 and Ontonotes-5.0 datasets.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_236",
      "source": "Lin2017_StructuredSelfAttentiveSentenceEmbedding",
      "target": "Chang2013_LatentLeftLinkingModel",
      "label": "Improve, Use",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Structured Self-attentive Sentence Embedding improves upon latent left linking model by using a self-attention mechanism.",
            "problem_addressed": "",
            "evidence": "Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations.",
            "confidence": 0.92
          },
          {
            "type": "Use",
            "structure": "Dataset.Task",
            "detail": "The Structured Self-attentive Sentence Embedding model uses the same datasets as Latent Left Linking Model.",
            "problem_addressed": "",
            "evidence": "Both models use the ACE 2004 and Ontonotes-5.0 datasets.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_237",
      "source": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "target": "Hajishirzi2013_NECO",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Component",
            "detail": "Adds domain knowledge-based constraints and probabilistic model to the constrained latent left linking model",
            "problem_addressed": "",
            "evidence": "The Constrained Latent Left Linking Model (CL3M) extends the Latent Left Linking Model (L3M) by incorporating domain knowledge-based constraints and a probabilistic model, which improves coreference resolution performance.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_238",
      "source": "Liang2016_TagBasedSolver",
      "target": "Hosseini2014_ARIS",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Tag-based approach improves upon ARIS by handling more problem types and being less sensitive to irrelevant information.",
            "problem_addressed": "",
            "evidence": "Many rule-based approaches only handle addition and subtraction math operations, but we can solve much more problems types, such as Multiplication, Division, Comparison, Algebra, etc. Also, it can handle much more problem types other than addition and subtraction.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_239",
      "source": "Liang2016_TagBasedSolver",
      "target": "Kushman2014_KAZB",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Tag-based approach improves upon KAZB by incorporating understanding and reasoning, leading to better performance on complex problems.",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information, which can be used to identify the desired operand and filter out irrelevant quantities.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_240",
      "source": "Liang2016_TagBasedSolver",
      "target": "MA1_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Liang et al.'s solver evaluated on MA1 dataset",
            "problem_addressed": "",
            "evidence": "The experiments are performed on the datasets MA1, MA2 and IXL provided by Hosseini et al. (2014)",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_241",
      "source": "Liang2016_TagBasedSolver",
      "target": "MA2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Liang et al.'s solver evaluated on MA2 dataset",
            "problem_addressed": "",
            "evidence": "The experiments are performed on the datasets MA1, MA2 and IXL provided by Hosseini et al. (2014)",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_242",
      "source": "Liang2016_TagBasedSolver",
      "target": "IXL_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "Liang et al.'s solver evaluated on IXL dataset",
            "problem_addressed": "",
            "evidence": "The experiments are performed on the datasets MA1, MA2 and IXL provided by Hosseini et al. (2014)",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_243",
      "source": "Liang2016_TagBasedSolver",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "Liang et al.'s solver evaluated using classification accuracy",
            "problem_addressed": "",
            "evidence": "The performance of our system is compared with ARIS and KAZB in overall performance.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_244",
      "source": "Liang2016_TagBasedSolver",
      "target": "Solution_Type_Accuracy",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "Liang et al.'s solver evaluated using solution type accuracy",
            "problem_addressed": "",
            "evidence": "The performance of our system is compared with ARIS and KAZB in overall performance.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_245",
      "source": "Liang2016_TagBasedSolver",
      "target": "Stanford_CoreNLP",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Tool",
            "detail": "Liang et al.'s solver uses Stanford CoreNLP for language analysis",
            "problem_addressed": "",
            "evidence": "The Stanford CoreNLP suite is adopted as our LA, which enables a list of annotators to generate the necessary linguistic information.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_246",
      "source": "Liang2016_TagBasedSolver",
      "target": "Charniak2000_MaximumEntropyParser",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Liang et al.'s solver extends Maximum Entropy Parser for solution type classification",
            "problem_addressed": "",
            "evidence": "A SVM classifier with linear kernel functions is used, and it adopted various feature-sets.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses maximum entropy parser for linguistic analysis",
            "problem_addressed": "",
            "evidence": "The Stanford CoreNLP suite is adopted as our LA, which enables a list of annotators to generate the necessary linguistic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_247",
      "source": "Liang2016_TagBasedSolver",
      "target": "Chang2013_ConstrainedLatentLeftLinkingModel",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses constrained latent left linking model for co-reference resolution",
            "problem_addressed": "",
            "evidence": "Dependency relation and co-reference resolution will provide such information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_248",
      "source": "Liang2016_TagBasedSolver",
      "target": "Chang2013_ProbabilisticLatentLeftLinkingModel",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses probabilistic latent left linking model for co-reference resolution",
            "problem_addressed": "",
            "evidence": "Dependency relation and co-reference resolution will provide such information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_249",
      "source": "Liang2016_TagBasedSolver",
      "target": "Chang2013_LatentLeftLinkingModel",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses latent left linking model for co-reference resolution",
            "problem_addressed": "",
            "evidence": "Dependency relation and co-reference resolution will provide such information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_250",
      "source": "Liang2016_TagBasedSolver",
      "target": "Hosseini2014_VerbCategorization",
      "label": "Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends verb categorization for better problem understanding",
            "problem_addressed": "",
            "evidence": "The STC selects a math operation based on the global information across various input sentences.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_251",
      "source": "Liang2016_TagBasedSolver",
      "target": "Roy2015_QuantityExtraction",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends quantity extraction for better operand identification",
            "problem_addressed": "",
            "evidence": "The proposed tag provides the flexibility for annotating a given math quantity with associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses quantity extraction for operand identification",
            "problem_addressed": "",
            "evidence": "The proposed tag provides the flexibility for annotating a given math quantity with associated syntactic and semantic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_252",
      "source": "Liang2016_TagBasedSolver",
      "target": "Roy2015_QuantityEntailment",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends quantity entailment for better reasoning",
            "problem_addressed": "",
            "evidence": "It analyzes the text and transforms both body and question parts into their tag-based logic forms, and then performs inference on them.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses quantity entailment for reasoning",
            "problem_addressed": "",
            "evidence": "It analyzes the text and transforms both body and question parts into their tag-based logic forms, and then performs inference on them.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_253",
      "source": "Liang2016_TagBasedSolver",
      "target": "Roy2015_MathWordProblemSolver",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends the cascade of classifiers for better solution generation",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses sparse averaged perceptron for classification",
            "problem_addressed": "",
            "evidence": "A SVM classifier with linear kernel functions is used, and it adopted various feature-sets.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_254",
      "source": "Liang2016_TagBasedSolver",
      "target": "Roy2017_UNITDEP",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends unit dependency graph for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses unit dependency graph for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_255",
      "source": "Liang2016_TagBasedSolver",
      "target": "Roy2015_LCA++",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends LCA++ for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses LCA++ for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_256",
      "source": "Liang2016_TagBasedSolver",
      "target": "Roy2016_ExpressionTreeBasedSolver",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends expression tree-based solver for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses expression tree-based solver for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_257",
      "source": "Liang2016_TagBasedSolver",
      "target": "Roy2018_KNOWLEDGE",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends KNOWLEDGE for better declarative rule selection",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses KNOWLEDGE for better declarative rule selection",
            "problem_addressed": "",
            "evidence": "The proposed tag provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_258",
      "source": "Liang2016_TagBasedSolver",
      "target": "Charniak1968_CARPS",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends CARPS for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses CARPS for linguistic analysis",
            "problem_addressed": "",
            "evidence": "The Stanford CoreNLP suite is adopted as our LA, which enables a list of annotators to generate the necessary linguistic information.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Liang et al.'s solver improves upon CARPS by incorporating tag-based annotation",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_259",
      "source": "Liang2016_TagBasedSolver",
      "target": "Charniak1969_ComputerSolution",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Methodology",
            "detail": "Liang et al.'s solver extends computer solution for better problem solving",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Component",
            "detail": "Liang et al.'s solver uses computer solution for linguistic analysis",
            "problem_addressed": "",
            "evidence": "The Stanford CoreNLP suite is adopted as our LA, which enables a list of annotators to generate the necessary linguistic information.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "Liang et al.'s solver improves upon computer solution by incorporating tag-based annotation",
            "problem_addressed": "",
            "evidence": "The proposed tag-based approach provides the flexibility for annotating an extracted math quantity with its associated syntactic and semantic information.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_260",
      "source": "Roy2015_LCA++",
      "target": "Roy2015_QuantityExtraction",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "LCA++ extends QuantityExtraction with additional components",
            "problem_addressed": "",
            "evidence": "The architecture of LCA++ includes components like Irrelevance Classifier and LCA Operation Classifier.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Architecture.Component",
            "detail": "Used quantity extraction techniques",
            "problem_addressed": "",
            "evidence": "LCA++ uses the quantity extraction techniques of Roy et al.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_261",
      "source": "Roy2015_LCA++",
      "target": "Roy2015_QuantityEntailment",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Integrated quantity entailment techniques",
            "problem_addressed": "",
            "evidence": "LCA++ extends the quantity entailment techniques of Roy et al.",
            "confidence": 0.9
          },
          {
            "type": "Use",
            "structure": "Architecture.Component",
            "detail": "Used quantity entailment techniques",
            "problem_addressed": "",
            "evidence": "LCA++ uses the quantity entailment techniques of Roy et al.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_262",
      "source": "Roy2015_LCA++",
      "target": "Roy2015_MathWordProblemSolver",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Integrated math word problem solving techniques",
            "problem_addressed": "",
            "evidence": "LCA++ extends the math word problem solving techniques of Roy et al.",
            "confidence": 0.9
          },
          {
            "type": "Use",
            "structure": "Architecture.Component",
            "detail": "Used math word problem solving techniques",
            "problem_addressed": "",
            "evidence": "LCA++ uses the math word problem solving techniques of Roy et al.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_263",
      "source": "Roy2015_LCA++",
      "target": "Roy2017_UNITDEP",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Integrated unit dependency graph techniques",
            "problem_addressed": "",
            "evidence": "LCA++ extends the unit dependency graph techniques of Roy et al.",
            "confidence": 0.9
          },
          {
            "type": "Use",
            "structure": "Architecture.Component",
            "detail": "Used unit dependency graph techniques",
            "problem_addressed": "",
            "evidence": "LCA++ uses the unit dependency graph techniques of Roy et al.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_264",
      "source": "Roy2015_LCA++",
      "target": "Roy2015_LCA++",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Architecture.Component",
            "detail": "Used LCA operation prediction techniques",
            "problem_addressed": "",
            "evidence": "LCA++ uses LCA operation prediction techniques.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_265",
      "source": "Roy2015_LCA++",
      "target": "Roy2016_ExpressionTreeBasedSolver",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Integrated expression tree-based solving techniques",
            "problem_addressed": "",
            "evidence": "LCA++ extends the expression tree-based solving techniques of Roy et al.",
            "confidence": 0.9
          },
          {
            "type": "Use",
            "structure": "Architecture.Component",
            "detail": "Used expression tree-based solving techniques",
            "problem_addressed": "",
            "evidence": "LCA++ uses the expression tree-based solving techniques of Roy et al.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_266",
      "source": "Roy2015_LCA++",
      "target": "Roy2018_KNOWLEDGE",
      "label": "Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Component",
            "detail": "Integrated declarative knowledge mapping techniques",
            "problem_addressed": "",
            "evidence": "LCA++ extends the declarative knowledge mapping techniques of Roy et al.",
            "confidence": 0.9
          },
          {
            "type": "Use",
            "structure": "Architecture.Component",
            "detail": "Used declarative knowledge mapping techniques",
            "problem_addressed": "",
            "evidence": "LCA++ uses the declarative knowledge mapping techniques of Roy et al.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_267",
      "source": "Hosseini2014_ARIS",
      "target": "MA1_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "ARIS uses MA1 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include 395 problems and 1,483 sentences in total. MA1 covers simple MWPs on addition and subtraction for third, fourth, and fifth graders.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_268",
      "source": "Hosseini2014_ARIS",
      "target": "MA2_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "ARIS uses MA2 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "Problems in MA2 include more irrelevant information compared to the other two datasets.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_269",
      "source": "Hosseini2014_ARIS",
      "target": "IXL_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "ARIS uses IXL dataset for evaluation",
            "problem_addressed": "",
            "evidence": "IXL includes more information gaps.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_270",
      "source": "Hosseini2014_ARIS",
      "target": "Accuracy_Classification",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "ARIS uses Accuracy as a classification metric",
            "problem_addressed": "",
            "evidence": "The performance of our system is compared with ARIS which is a rule-based system that changes the entity attribute according to the schema.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_271",
      "source": "Kushman2014_KAZB",
      "target": "LinearT2",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "KAZB uses LinearT2 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include LinearT2 and LinearT6.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_272",
      "source": "Kushman2014_KAZB",
      "target": "LinearT6",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "KAZB uses LinearT6 dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include LinearT2 and LinearT6.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_273",
      "source": "Kushman2014_KAZB",
      "target": "Accuracy",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "KAZB uses Accuracy as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include Accuracy.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_274",
      "source": "Roy2015_QuantityExtraction",
      "target": "RTE_Datasets_2006",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "QuantityExtraction uses RTE datasets for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include RTE Datasets and Newswire Text.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_275",
      "source": "Roy2015_QuantityExtraction",
      "target": "Newswire_Text_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "QuantityExtraction uses Newswire Text dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include RTE Datasets and Newswire Text.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_276",
      "source": "Roy2015_QuantityExtraction",
      "target": "F1_Score_Quantitative_Reasoning",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "QuantityExtraction uses F1 Score as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include F1 Score, Precision, and Recall.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_277",
      "source": "Roy2015_QuantityExtraction",
      "target": "Precision_Quantitative_Reasoning",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "QuantityExtraction uses Precision as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include F1 Score, Precision, and Recall.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_278",
      "source": "Roy2015_QuantityExtraction",
      "target": "Recall_Quantitative_Reasoning",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "QuantityExtraction uses Recall as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include F1 Score, Precision, and Recall.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_279",
      "source": "Roy2015_QuantityEntailment",
      "target": "RTE_Datasets_2006",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "QuantityEntailment uses RTE datasets for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include RTE Datasets and Newswire Text.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_280",
      "source": "Roy2015_QuantityEntailment",
      "target": "Newswire_Text_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "QuantityEntailment uses Newswire Text dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include RTE Datasets and Newswire Text.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_281",
      "source": "Roy2015_QuantityEntailment",
      "target": "F1_Score_Quantitative_Reasoning",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "QuantityEntailment uses F1 Score as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include F1 Score, Precision, and Recall.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_282",
      "source": "Roy2015_QuantityEntailment",
      "target": "Precision_Quantitative_Reasoning",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "QuantityEntailment uses Precision as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include F1 Score, Precision, and Recall.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_283",
      "source": "Roy2015_QuantityEntailment",
      "target": "Recall_Quantitative_Reasoning",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "QuantityEntailment uses Recall as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include F1 Score, Precision, and Recall.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_284",
      "source": "Roy2015_MathWordProblemSolver",
      "target": "Elementary_Math_Word_Problems_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "MathWordProblemSolver uses Elementary Math Word Problems dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include Elementary Math Word Problems.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_285",
      "source": "Roy2015_MathWordProblemSolver",
      "target": "Accuracy_MathProblemSolving",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "MathWordProblemSolver uses Accuracy as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include Accuracy.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_286",
      "source": "Roy2017_UNITDEP",
      "target": "AllArith_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "UNITDEP uses AllArith dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, and AllArithTmpl.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_287",
      "source": "Roy2017_UNITDEP",
      "target": "AllArithLex_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "UNITDEP uses AllArithLex dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, and AllArithTmpl.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_288",
      "source": "Roy2017_UNITDEP",
      "target": "AllArithTmpl_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "UNITDEP uses AllArithTmpl dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, and AllArithTmpl.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_289",
      "source": "Roy2017_UNITDEP",
      "target": "Accuracy",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "UNITDEP uses Accuracy as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include Accuracy.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_290",
      "source": "Roy2015_LCA++",
      "target": "AllArith_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "LCA++ uses AllArith dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, and AllArithTmpl.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_291",
      "source": "Roy2015_LCA++",
      "target": "AllArithLex_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "LCA++ uses AllArithLex dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, and AllArithTmpl.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_292",
      "source": "Roy2015_LCA++",
      "target": "AllArithTmpl_2017",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "LCA++ uses AllArithTmpl dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, and AllArithTmpl.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_293",
      "source": "Roy2015_LCA++",
      "target": "Accuracy",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "LCA++ uses Accuracy as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include Accuracy.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_294",
      "source": "Roy2016_ExpressionTreeBasedSolver",
      "target": "AI2_Dataset_2014",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "ExpressionTreeBasedSolver uses AI2 Dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AI2 Dataset, IL Dataset, and Commoncore Dataset.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_295",
      "source": "Roy2016_ExpressionTreeBasedSolver",
      "target": "IL_Dataset_2015",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "ExpressionTreeBasedSolver uses IL Dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AI2 Dataset, IL Dataset, and Commoncore Dataset.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_296",
      "source": "Roy2016_ExpressionTreeBasedSolver",
      "target": "Commoncore_Dataset_2016",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "ExpressionTreeBasedSolver uses Commoncore Dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AI2 Dataset, IL Dataset, and Commoncore Dataset.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_297",
      "source": "Roy2016_ExpressionTreeBasedSolver",
      "target": "Accuracy",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "ExpressionTreeBasedSolver uses Accuracy as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include Accuracy.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_298",
      "source": "Roy2018_KNOWLEDGE",
      "target": "AllArith_2018",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "KNOWLEDGE uses AllArith dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, AllArithTmpl, Perturb, and Aggregate.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_299",
      "source": "Roy2018_KNOWLEDGE",
      "target": "AllArithLex_2018",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "KNOWLEDGE uses AllArithLex dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, AllArithTmpl, Perturb, and Aggregate.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_300",
      "source": "Roy2018_KNOWLEDGE",
      "target": "AllArithTmpl_2018",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "KNOWLEDGE uses AllArithTmpl dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, AllArithTmpl, Perturb, and Aggregate.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_301",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Perturb_2018",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "KNOWLEDGE uses Perturb dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, AllArithTmpl, Perturb, and Aggregate.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_302",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Aggregate_2018",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Datasets",
            "detail": "KNOWLEDGE uses Aggregate dataset for evaluation",
            "problem_addressed": "",
            "evidence": "The datasets include AllArith, AllArithLex, AllArithTmpl, Perturb, and Aggregate.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_303",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Accuracy",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metrics",
            "detail": "KNOWLEDGE uses Accuracy as a metric",
            "problem_addressed": "",
            "evidence": "The metrics include Accuracy.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_304",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Roy2015_QuantityExtraction",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "KNOWLEDGE extends QuantityExtraction by incorporating declarative rules",
            "problem_addressed": "",
            "evidence": "The architecture components include Concept Selection and Declarative Rule Selection.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "KNOWLEDGE uses QuantityExtraction for feature processing",
            "problem_addressed": "",
            "evidence": "Feature processing includes Dependency Parsing, Coreference Resolution, Verb Classification, and Rate Component Detection.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanisms",
            "detail": "KNOWLEDGE improves QuantityExtraction by adding declarative rule selection",
            "problem_addressed": "",
            "evidence": "The mechanisms include Transfer, Dimensional Analysis, Part-Whole Relation, and Explicit Math.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_305",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Roy2015_QuantityEntailment",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "KNOWLEDGE extends QuantityEntailment by incorporating declarative rules",
            "problem_addressed": "",
            "evidence": "The architecture components include Concept Selection and Declarative Rule Selection.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "KNOWLEDGE uses QuantityEntailment for feature processing",
            "problem_addressed": "",
            "evidence": "Feature processing includes Dependency Parsing, Coreference Resolution, Verb Classification, and Rate Component Detection.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanisms",
            "detail": "KNOWLEDGE improves QuantityEntailment by adding declarative rule selection",
            "problem_addressed": "",
            "evidence": "The mechanisms include Transfer, Dimensional Analysis, Part-Whole Relation, and Explicit Math.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_306",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Roy2015_MathWordProblemSolver",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "KNOWLEDGE extends MathWordProblemSolver by incorporating declarative rules",
            "problem_addressed": "",
            "evidence": "The architecture components include Concept Selection and Declarative Rule Selection.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "KNOWLEDGE uses MathWordProblemSolver for feature processing",
            "problem_addressed": "",
            "evidence": "Feature processing includes Dependency Parsing, Coreference Resolution, Verb Classification, and Rate Component Detection.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanisms",
            "detail": "KNOWLEDGE improves MathWordProblemSolver by adding declarative rule selection",
            "problem_addressed": "",
            "evidence": "The mechanisms include Transfer, Dimensional Analysis, Part-Whole Relation, and Explicit Math.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_307",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Roy2017_UNITDEP",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "KNOWLEDGE extends UNITDEP by incorporating declarative rules",
            "problem_addressed": "",
            "evidence": "The architecture components include Concept Selection and Declarative Rule Selection.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "KNOWLEDGE uses UNITDEP for feature processing",
            "problem_addressed": "",
            "evidence": "Feature processing includes Dependency Parsing, Coreference Resolution, Verb Classification, and Rate Component Detection.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanisms",
            "detail": "KNOWLEDGE improves UNITDEP by adding declarative rule selection",
            "problem_addressed": "",
            "evidence": "The mechanisms include Transfer, Dimensional Analysis, Part-Whole Relation, and Explicit Math.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_308",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Roy2015_LCA++",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "KNOWLEDGE extends LCA++ by incorporating declarative rules",
            "problem_addressed": "",
            "evidence": "The architecture components include Concept Selection and Declarative Rule Selection.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "KNOWLEDGE uses LCA++ for feature processing",
            "problem_addressed": "",
            "evidence": "Feature processing includes Dependency Parsing, Coreference Resolution, Verb Classification, and Rate Component Detection.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanisms",
            "detail": "KNOWLEDGE improves LCA++ by adding declarative rule selection",
            "problem_addressed": "",
            "evidence": "The mechanisms include Transfer, Dimensional Analysis, Part-Whole Relation, and Explicit Math.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_309",
      "source": "Roy2018_KNOWLEDGE",
      "target": "Roy2016_ExpressionTreeBasedSolver",
      "label": "Improve, Extend, Use",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Architecture.Components",
            "detail": "KNOWLEDGE extends ExpressionTreeBasedSolver by incorporating declarative rules",
            "problem_addressed": "",
            "evidence": "The architecture components include Concept Selection and Declarative Rule Selection.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "KNOWLEDGE uses ExpressionTreeBasedSolver for feature processing",
            "problem_addressed": "",
            "evidence": "Feature processing includes Dependency Parsing, Coreference Resolution, Verb Classification, and Rate Component Detection.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Architecture.Mechanisms",
            "detail": "KNOWLEDGE improves ExpressionTreeBasedSolver by adding declarative rule selection",
            "problem_addressed": "",
            "evidence": "The mechanisms include Transfer, Dimensional Analysis, Part-Whole Relation, and Explicit Math.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_310",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Chen2014_NeuralNetworkParser",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Architecture.Mechanism",
            "detail": "The neural network parser improves upon the easy-first non-directional dependency parsing algorithm by incorporating neural embeddings and transition-based parsing.",
            "problem_addressed": "",
            "evidence": "Our parser uses neural embeddings and a transition-based approach, which significantly outperforms traditional parsing algorithms such as the easy-first non-directional parser (Goldberg and Elhadad, 2010).",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_311",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "WSJ_Treebank_2010",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "The easy-first non-directional dependency parsing algorithm was evaluated on the WSJ Treebank.",
            "problem_addressed": "",
            "evidence": "We evaluate the parser using the WSJ Treebank. The trees were converted to dependency structures with the Penn2Malt conversion program.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_312",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "CoNLL_2007_English_dataset_2010",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Dataset",
            "detail": "The easy-first non-directional dependency parsing algorithm was evaluated on the CoNLL 2007 English dataset.",
            "problem_addressed": "",
            "evidence": "We evaluated the parsers also on the English dataset from the CoNLL 2007 shared task.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_313",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Accuracy",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "The easy-first non-directional dependency parsing algorithm uses accuracy as an evaluation metric.",
            "problem_addressed": "",
            "evidence": "We evaluate the parsers using three common measures: (unlabeled) Accuracy.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_314",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Root_Prediction",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "The easy-first non-directional dependency parsing algorithm uses root prediction as an evaluation metric.",
            "problem_addressed": "",
            "evidence": "Root: The percentage of sentences in which the ROOT attachment is correct.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_315",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Complete_Correct_Parse",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Metric",
            "detail": "The easy-first non-directional dependency parsing algorithm uses complete correct parse as an evaluation metric.",
            "problem_addressed": "",
            "evidence": "Complete: the percentage of sentences in which all tokens were assigned their correct parent.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_316",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "MALT",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon the MALT parser by addressing locality issues.",
            "problem_addressed": "",
            "evidence": "While all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_317",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "MST",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon the MST parser by reducing computational complexity.",
            "problem_addressed": "",
            "evidence": "In terms of feature extraction and score calculation operations, our algorithm has the same cost as traditional shift-reduce(MALT) parsers, and is an order of magnitude more efficient than graph-based (MST) parsers.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_318",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Shen2007_BidirectionalIncrementalConstruction",
      "label": "Improve, Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm extends the bidirectional incremental construction approach.",
            "problem_addressed": "",
            "evidence": "We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm.",
            "confidence": 0.85
          },
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon the bidirectional incremental construction approach by optimizing the order of parsing actions.",
            "problem_addressed": "",
            "evidence": "The parser learns both the attachment preferences and the order in which they should be performed.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_319",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Nivre2004_ArcStandardSystem",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon the Arc-Standard system by allowing more flexible parsing order.",
            "problem_addressed": "",
            "evidence": "In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbors to connect at each parsing step.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_320",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Huang2009_BilinguallyConstrainedParsing",
      "label": "Improve, Extend, Use",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon bilingually constrained parsing by incorporating richer structural features.",
            "problem_addressed": "",
            "evidence": "We extended that feature set to include the structure on both sides of the proposed attachment point.",
            "confidence": 0.85
          },
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "The easy-first non-directional dependency parsing algorithm uses feature templates similar to those in bilingually constrained parsing.",
            "problem_addressed": "",
            "evidence": "We extended that feature set to include the structure on both sides of the proposed attachment point.",
            "confidence": 0.9
          },
          {
            "type": "Extend",
            "structure": "Feature_Processing",
            "detail": "The easy-first non-directional dependency parsing algorithm extends bilingually constrained parsing by incorporating richer structural features.",
            "problem_addressed": "",
            "evidence": "We extended that feature set to include the structure on both sides of the proposed attachment point.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_321",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Zhang2008_BeamSearchParser",
      "label": "Improve",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon beam search parsers by reducing computational complexity.",
            "problem_addressed": "",
            "evidence": "Beam-search decoding for left-to-right parsers(Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beam-size. The reported results in(Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k= 6.",
            "confidence": 0.85
          }
        ]
      }
    },
    {
      "id": "edge_322",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Collins2002_DiscriminativeTraining",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Methodology",
            "detail": "The easy-first non-directional dependency parsing algorithm uses a structured perceptron for training.",
            "problem_addressed": "",
            "evidence": "We use a linear model score(x)=w~ · φ(x), where φ(x) is a feature representation andw~ is a weight vector. We write φact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron(Collins, 2002).",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_323",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Nivre2006_MaltParser",
      "label": "Improve, Compare",
      "relation_type": "Compare",
      "data": {
        "relations": [
          {
            "type": "Compare",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm is compared to the MALT parser.",
            "problem_addressed": "",
            "evidence": "We evaluate our parser against the transition-based MALT parser and the graph-based MST parser.",
            "confidence": 0.9
          },
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon the MALT parser by addressing locality issues.",
            "problem_addressed": "",
            "evidence": "While all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_324",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "McDonald2005_MSTParser",
      "label": "Improve, Compare",
      "relation_type": "Compare",
      "data": {
        "relations": [
          {
            "type": "Compare",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm is compared to the MST parser.",
            "problem_addressed": "",
            "evidence": "We evaluate our parser against the transition-based MALT parser and the graph-based MST parser.",
            "confidence": 0.9
          },
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon the MST parser by reducing computational complexity.",
            "problem_addressed": "",
            "evidence": "In terms of feature extraction and score calculation operations, our algorithm has the same cost as traditional shift-reduce(MALT) parsers, and is an order of magnitude more efficient than graph-based (MST) parsers.",
            "confidence": 0.88
          }
        ]
      }
    },
    {
      "id": "edge_325",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "deMarneffe2006_TypedDependencyExtractor",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "The easy-first non-directional dependency parsing algorithm uses typed dependency extraction for feature processing.",
            "problem_addressed": "",
            "evidence": "We extended that feature set to include the structure on both sides of the proposed attachment point.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_326",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Sagae2006a_BestFirstProbabilisticParser",
      "label": "Improve, Compare",
      "relation_type": "Improve",
      "data": {
        "relations": [
          {
            "type": "Improve",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm improves upon the best-first probabilistic parser by incorporating richer structural features.",
            "problem_addressed": "",
            "evidence": "Our non-directional easy-first parser significantly outperforms the left-to-right greedy MALT parser in terms of accuracy and root prediction, and significantly outperforms both parsers in terms of exact match.",
            "confidence": 0.85
          },
          {
            "type": "Compare",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm is compared to the best-first probabilistic parser.",
            "problem_addressed": "",
            "evidence": "While all models suffer from the move to the smaller dataset and the more challenging annotation scheme, the overall story remains the same: the non-directional parser is better than MALT but not as good as MST in terms of parent-accuracy and root prediction, and is better than both MALT and MST in terms of producing complete correct parses.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_327",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Sagae2006b_ParserCombination",
      "label": "Compare, Extend",
      "relation_type": "Extend",
      "data": {
        "relations": [
          {
            "type": "Extend",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm extends the idea of parser combination by producing different structures.",
            "problem_addressed": "",
            "evidence": "The parses produced by the non-directional parser are different than the parses produced by the graph-based and left-to-right parsers.",
            "confidence": 0.85
          },
          {
            "type": "Compare",
            "structure": "Algorithm",
            "detail": "The easy-first non-directional dependency parsing algorithm is compared to parser combination methods.",
            "problem_addressed": "",
            "evidence": "A non-oracle blending of MALT+MST+NONDIR using Sagae and Lavie’s(2006) simplest combination method assigning each component the same weight, yield an accuracy of 90.8 on the CoNLL 2007 English dataset, making it the highest scoring system among the participants.",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_328",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Nivre2007_SharedTask",
      "label": "Evaluate",
      "relation_type": "Evaluate",
      "data": {
        "relations": [
          {
            "type": "Evaluate",
            "structure": "Dataset",
            "detail": "The easy-first non-directional dependency parsing algorithm was evaluated in the CoNLL 2007 shared task.",
            "problem_addressed": "",
            "evidence": "We evaluated the parsers also on the English dataset from the CoNLL 2007 shared task.",
            "confidence": 0.95
          }
        ]
      }
    },
    {
      "id": "edge_329",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Yamada2003_HeadFindingRules",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "The easy-first non-directional dependency parsing algorithm uses head-finding rules for feature processing.",
            "problem_addressed": "",
            "evidence": "The trees were converted to dependency structures with the Penn2Malt conversion program, using the head-finding rules from(Yamada and Matsumoto, 2003).",
            "confidence": 0.9
          }
        ]
      }
    },
    {
      "id": "edge_330",
      "source": "Goldberg2010_EasyFirstNonDirectionalParser",
      "target": "Carreras2007_ProjectivizationProcedure",
      "label": "Use",
      "relation_type": "Use",
      "data": {
        "relations": [
          {
            "type": "Use",
            "structure": "Feature_Processing",
            "detail": "The easy-first non-directional dependency parsing algorithm uses projectivization procedures for handling non-projective structures.",
            "problem_addressed": "",
            "evidence": "For the non-directional parser, we projectivize the training set prior to training using the procedure described in(Carreras, 2007).",
            "confidence": 0.9
          }
        ]
      }
    }
  ],
  "task_id": null
}