#!/usr/bin/env python3
"""
AutoSurveyé›†æˆåŠŸèƒ½ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
"""

import sys
import os
import asyncio
import tempfile
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.modules.autosurvey_integration import (
    TaskData, EntityData, RelationData,
    DataFormatConverter, AlgorithmLineageAnalyzer,
    AutoSurveyConnector, AutoSurveyConfig, SurveyGenerationRequest
)
from app.modules.survey_generation_engine import SurveyContentGenerator
from app.modules.survey_storage_manager import SurveyStorageManager
from app.modules.lineage_description_generator import AlgorithmLineageDescriptionGenerator
from app.modules.survey_formatter import SurveyFormatter, FormattingOptions

class MockDatabaseManager:
    """æ¨¡æ‹Ÿæ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.db_utils = MockDBUtils()

class MockDBUtils:
    """æ¨¡æ‹Ÿæ•°æ®åº“å·¥å…·"""
    
    def fetch_all(self, sql, params=None):
        return []
    
    def fetch_one(self, sql, params=None):
        return None

def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    print("ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®...")
    
    # åˆ›å»ºæµ‹è¯•å®ä½“
    entities = [
        EntityData(
            entity_id="bert_2018",
            entity_type="Algorithm",
            name="BERT",
            title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            year=2018,
            authors=["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"],
            description="åŒå‘ç¼–ç å™¨è¡¨ç¤ºçš„Transformeræ¨¡å‹",
            source="Google AI"
        ),
        EntityData(
            entity_id="gpt_2018",
            entity_type="Algorithm", 
            name="GPT",
            title="Improving Language Understanding by Generative Pre-Training",
            year=2018,
            authors=["Alec Radford", "Karthik Narasimhan"],
            description="ç”Ÿæˆå¼é¢„è®­ç»ƒTransformeræ¨¡å‹",
            source="OpenAI"
        ),
        EntityData(
            entity_id="transformer_2017",
            entity_type="Algorithm",
            name="Transformer",
            title="Attention Is All You Need",
            year=2017,
            authors=["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
            description="åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹",
            source="Google Brain"
        ),
        EntityData(
            entity_id="glue_2018",
            entity_type="Dataset",
            name="GLUE",
            description="é€šç”¨è¯­è¨€ç†è§£è¯„ä¼°åŸºå‡†",
            year=2018,
            authors=["Alex Wang", "Amanpreet Singh"],
            source="NYU"
        )
    ]
    
    # åˆ›å»ºæµ‹è¯•å…³ç³»
    relations = [
        RelationData(
            from_entity="transformer_2017",
            to_entity="bert_2018",
            relation_type="åŸºç¡€æ¶æ„",
            from_entity_type="Algorithm",
            to_entity_type="Algorithm",
            detail="BERTåŸºäºTransformeræ¶æ„",
            evidence="BERTè®ºæ–‡æ˜ç¡®æåˆ°ä½¿ç”¨Transformerç¼–ç å™¨",
            confidence=0.95
        ),
        RelationData(
            from_entity="transformer_2017",
            to_entity="gpt_2018",
            relation_type="åŸºç¡€æ¶æ„",
            from_entity_type="Algorithm",
            to_entity_type="Algorithm",
            detail="GPTåŸºäºTransformerè§£ç å™¨",
            evidence="GPTè®ºæ–‡ä½¿ç”¨Transformerè§£ç å™¨æ¶æ„",
            confidence=0.95
        ),
        RelationData(
            from_entity="bert_2018",
            to_entity="gpt_2018",
            relation_type="å¯¹æ¯”ç ”ç©¶",
            from_entity_type="Algorithm",
            to_entity_type="Algorithm",
            detail="BERTå’ŒGPTä»£è¡¨ä¸åŒçš„é¢„è®­ç»ƒæ–¹å‘",
            evidence="ä¸¤è€…åœ¨é¢„è®­ç»ƒç›®æ ‡å’Œæ¶æ„ä¸Šæœ‰æ˜¾è‘—å·®å¼‚",
            confidence=0.8
        )
    ]
    
    # åˆ›å»ºä»»åŠ¡æ•°æ®
    task_data = TaskData(
        task_id="nlp_pretrain_test",
        task_name="è‡ªç„¶è¯­è¨€å¤„ç†é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•",
        entities=entities,
        relations=relations,
        created_at=datetime.now()
    )
    
    print(f"âœ… åˆ›å»ºäº† {len(entities)} ä¸ªå®ä½“å’Œ {len(relations)} ä¸ªå…³ç³»")
    return task_data

def test_data_format_conversion(task_data):
    """æµ‹è¯•æ•°æ®æ ¼å¼è½¬æ¢"""
    print("\nğŸ”„ æµ‹è¯•æ•°æ®æ ¼å¼è½¬æ¢...")
    
    converter = DataFormatConverter()
    topic = "è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å‘å±•"
    params = {
        "model": "gpt-4o-2024-05-13",
        "section_num": 7,
        "subsection_len": 700
    }
    
    autosurvey_input = converter.convert_to_autosurvey_format(task_data, topic, params)
    
    print(f"âœ… è½¬æ¢å®Œæˆï¼ŒåŒ…å« {len(autosurvey_input['entities'])} ä¸ªå®ä½“")
    print(f"âœ… æ•°æ®è´¨é‡è¯„åˆ†: {autosurvey_input['metadata']['data_quality']['overall']:.2f}")
    
    return autosurvey_input

def test_algorithm_lineage_analysis(task_data):
    """æµ‹è¯•ç®—æ³•è„‰ç»œåˆ†æ"""
    print("\nğŸŒ³ æµ‹è¯•ç®—æ³•è„‰ç»œåˆ†æ...")
    
    mock_db = MockDatabaseManager()
    analyzer = AlgorithmLineageAnalyzer(mock_db)
    
    lineage_analysis = analyzer.analyze_algorithm_lineage(task_data)
    
    print(f"âœ… è¯†åˆ«å‡º {len(lineage_analysis['key_nodes'])} ä¸ªå…³é”®èŠ‚ç‚¹")
    
    # æ˜¾ç¤ºå…³é”®èŠ‚ç‚¹
    for i, node in enumerate(lineage_analysis['key_nodes'][:3], 1):
        print(f"   {i}. {node['name']} ({node.get('year', 'æœªçŸ¥å¹´ä»½')}) - é‡è¦æ€§: {node.get('importance_score', 0):.2f}")
    
    # æ˜¾ç¤ºå‘å±•è·¯å¾„
    dev_paths = lineage_analysis.get('development_paths', {})
    if isinstance(dev_paths, dict) and dev_paths.get('main_paths'):
        print(f"âœ… å‘ç° {len(dev_paths['main_paths'])} æ¡ä¸»è¦å‘å±•è·¯å¾„")
    
    return lineage_analysis

async def test_autosurvey_connector(autosurvey_input, task_data):
    """æµ‹è¯•AutoSurveyè¿æ¥å™¨"""
    print("\nğŸ¤– æµ‹è¯•AutoSurveyè¿æ¥å™¨...")
    
    config = AutoSurveyConfig()
    
    request = SurveyGenerationRequest(
        topic="è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å‘å±•",
        task_data=task_data,
        parameters={}
    )
    
    async with AutoSurveyConnector(config) as connector:
        survey_result = await connector.generate_survey(request)
        
        print(f"âœ… ç”Ÿæˆç»¼è¿°æˆåŠŸï¼ŒID: {survey_result.survey_id}")
        print(f"âœ… å†…å®¹é•¿åº¦: {len(survey_result.content)} å­—ç¬¦")
        print(f"âœ… å‚è€ƒæ–‡çŒ®: {len(survey_result.references)} ç¯‡")
        
        return survey_result

def test_survey_content_generation(autosurvey_result, lineage_analysis):
    """æµ‹è¯•ç»¼è¿°å†…å®¹ç”Ÿæˆ"""
    print("\nğŸ“ æµ‹è¯•ç»¼è¿°å†…å®¹ç”Ÿæˆ...")
    
    generator = SurveyContentGenerator()
    
    # æ¨¡æ‹ŸAutoSurveyç»“æœ
    mock_autosurvey_result = {
        "content": autosurvey_result.content,
        "references": autosurvey_result.references,
        "outline": autosurvey_result.outline
    }
    
    enhanced_content = generator.generate_survey_content(
        topic="è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å‘å±•",
        autosurvey_result=mock_autosurvey_result,
        algorithm_lineage=lineage_analysis,
        template_name="academic_standard"
    )
    
    print(f"âœ… å¢å¼ºå†…å®¹ç”Ÿæˆå®Œæˆ")
    print(f"âœ… ç« èŠ‚æ•°: {len(enhanced_content['sections'])}")
    print(f"âœ… å­—æ•°: {enhanced_content['metadata']['word_count']}")
    
    return enhanced_content

def test_lineage_description_generation(lineage_analysis):
    """æµ‹è¯•è„‰ç»œæè¿°ç”Ÿæˆ"""
    print("\nğŸ“– æµ‹è¯•è„‰ç»œæè¿°ç”Ÿæˆ...")
    
    generator = AlgorithmLineageDescriptionGenerator()
    
    description = generator.generate_lineage_description(
        lineage_analysis, 
        "è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å‘å±•"
    )
    
    print(f"âœ… è„‰ç»œæè¿°ç”Ÿæˆå®Œæˆ")
    print(f"âœ… å…³é”®æ´å¯Ÿ: {len(description.key_insights)} ä¸ª")
    
    # æ˜¾ç¤ºéƒ¨åˆ†æ´å¯Ÿ
    for i, insight in enumerate(description.key_insights[:2], 1):
        print(f"   {i}. {insight}")
    
    return description

def test_survey_formatting(content):
    """æµ‹è¯•ç»¼è¿°æ ¼å¼åŒ–"""
    print("\nğŸ¨ æµ‹è¯•ç»¼è¿°æ ¼å¼åŒ–...")
    
    formatter = SurveyFormatter()
    
    # æµ‹è¯•å¤šç§æ ¼å¼
    formats = ["markdown", "html"]
    results = {}
    
    for format_type in formats:
        options = FormattingOptions(
            format_type=format_type,
            style="academic",
            include_toc=True,
            include_references=True
        )
        
        result = formatter.format_survey(content, {}, options)
        results[format_type] = result
        
        print(f"âœ… {format_type.upper()} æ ¼å¼åŒ–å®Œæˆ")
    
    return results

def test_survey_storage(content, metadata):
    """æµ‹è¯•ç»¼è¿°å­˜å‚¨"""
    print("\nğŸ’¾ æµ‹è¯•ç»¼è¿°å­˜å‚¨...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        storage_manager = SurveyStorageManager(temp_dir)
        
        # å­˜å‚¨ç»¼è¿°
        record = storage_manager.store_survey(
            survey_id="test_survey_001",
            topic="è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹å‘å±•",
            content=content,
            task_ids=["nlp_pretrain_test"],
            metadata=metadata,
            formats=["markdown", "html"]
        )
        
        print(f"âœ… ç»¼è¿°å­˜å‚¨å®Œæˆï¼ŒID: {record.survey_id}")
        print(f"âœ… å¯ç”¨æ ¼å¼: {list(record.file_paths.keys())}")
        
        # æµ‹è¯•æ£€ç´¢
        retrieved = storage_manager.get_survey(record.survey_id)
        assert retrieved is not None
        print(f"âœ… ç»¼è¿°æ£€ç´¢æˆåŠŸ")
        
        # æµ‹è¯•åˆ—è¡¨
        surveys = storage_manager.list_surveys()
        print(f"âœ… ç»¼è¿°åˆ—è¡¨: {len(surveys)} ä¸ª")
        
        # æµ‹è¯•æœç´¢
        search_results = storage_manager.search_surveys("é¢„è®­ç»ƒ")
        print(f"âœ… æœç´¢ç»“æœ: {len(search_results)} ä¸ª")
        
        return record

async def run_integration_test():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹AutoSurveyé›†æˆåŠŸèƒ½ç«¯åˆ°ç«¯æµ‹è¯•\n")
    
    try:
        # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
        task_data = create_test_data()
        
        # 2. æµ‹è¯•æ•°æ®æ ¼å¼è½¬æ¢
        autosurvey_input = test_data_format_conversion(task_data)
        
        # 3. æµ‹è¯•ç®—æ³•è„‰ç»œåˆ†æ
        lineage_analysis = test_algorithm_lineage_analysis(task_data)
        
        # 4. æµ‹è¯•AutoSurveyè¿æ¥å™¨
        autosurvey_result = await test_autosurvey_connector(autosurvey_input, task_data)
        
        # 5. æµ‹è¯•ç»¼è¿°å†…å®¹ç”Ÿæˆ
        enhanced_content = test_survey_content_generation(autosurvey_result, lineage_analysis)
        
        # 6. æµ‹è¯•è„‰ç»œæè¿°ç”Ÿæˆ
        lineage_description = test_lineage_description_generation(lineage_analysis)
        
        # 7. æµ‹è¯•ç»¼è¿°æ ¼å¼åŒ–
        formatted_results = test_survey_formatting(enhanced_content["content"])
        
        # 8. æµ‹è¯•ç»¼è¿°å­˜å‚¨
        storage_record = test_survey_storage(
            enhanced_content["content"], 
            enhanced_content["metadata"]
        )
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AutoSurveyé›†æˆåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        
        # è¾“å‡ºæµ‹è¯•æ‘˜è¦
        print("\nğŸ“‹ æµ‹è¯•æ‘˜è¦:")
        print(f"   â€¢ å¤„ç†å®ä½“: {len(task_data.entities)} ä¸ª")
        print(f"   â€¢ å¤„ç†å…³ç³»: {len(task_data.relations)} ä¸ª")
        print(f"   â€¢ å…³é”®èŠ‚ç‚¹: {len(lineage_analysis['key_nodes'])} ä¸ª")
        print(f"   â€¢ ç»¼è¿°å­—æ•°: {enhanced_content['metadata']['word_count']} å­—")
        print(f"   â€¢ æ”¯æŒæ ¼å¼: {len(formatted_results)} ç§")
        print(f"   â€¢ å­˜å‚¨è·¯å¾„: {storage_record.file_paths}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(run_integration_test())
    sys.exit(0 if success else 1)
