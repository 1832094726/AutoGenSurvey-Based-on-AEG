[
  {
    "page_num": 1,
    "content": [
      {
        "text": "9\n1\n0\n2\n",
        "font-size": 10,
        "token": 8
      },
      {
        "text": "r\np\nA\n9\n2\n",
        "font-size": 7,
        "token": 10
      },
      {
        "text": "]\nL\nC\n.\ns\nc\n[\n",
        "font-size": 13,
        "token": 11
      },
      {
        "text": "2\nv\n0\n9\n2\n7\n0\n.\n8\n0\n8\n1\n:\nv\ni\nX\nr\na\n",
        "font-size": 5,
        "token": 34
      },
      {
        "text": "The Gap of Semantic Parsing: A Survey on Automatic\nMath Word Problem Solvers\n",
        "font-size": 22,
        "token": 17
      },
      {
        "text": "Dongxiang Zhang, Lei Wang, Luming Zhang, Bing Tian Dai and Heng Tao Shen\n",
        "font-size": 10,
        "token": 22
      },
      {
        "text": "1\n",
        "font-size": 6,
        "token": 2
      },
      {
        "text": "Abstract—Solving mathematical word problems (MWPs) automatically is challenging, primarily due to the semantic gap between human-readable\nwords and machine-understandable logics. Despite the long history dated back to the 1960s, MWPs have regained intensive attention in the past few\nyears with the advancement of Artiﬁcial Intelligence (AI). Solving MWPs successfully is considered as a milestone towards general AI. Many systems\nhave claimed promising results in self-crafted and small-scale datasets. However, when applied on large and diverse datasets, none of the proposed\nmethods in the literature achieves high precision, revealing that current MWP solvers still have much room for improvement. This motivated us to\npresent a comprehensive survey to deliver a clear and complete picture of automatic math problem solvers. In this survey, we emphasize on algebraic\nword problems, summarize their extracted features and proposed techniques to bridge the semantic gap, and compare their performance in the\npublicly accessible datasets. We also cover automatic solvers for other types of math problems such as geometric problems that require the\nunderstanding of diagrams. Finally, we identify several emerging research directions for the readers with interests in MWPs.\n Index Terms—math word problem, semantic parser, reasoning, survey, natural language processing, machine learning\n",
        "font-size": 7,
        "token": 264
      },
      {
        "text": "✦\n",
        "font-size": 11,
        "token": 2
      },
      {
        "text": "1 INTRODUCTION\n",
        "font-size": 8,
        "token": 4
      },
      {
        "text": "Designing an automatic solver for mathematical word problems\n(MWPs) has a long history dated back to the 1960s [1], [2],\n[3], and continues to attract intensive research attention. In the\npast three years, more than 40 publications on this topic have\nintelligence. The\nemerged in the premier venues of artiﬁcial\nproblem is particularly challenging because there remains a wide\nsemantic gap to parse the human-readable words into machine-\nunderstandable logics so as to facilitate quantitative reasoning.\nHence, MWPs solvers are broadly considered as good test beds\nto evaluate the intelligence level of agents in terms of natural\nlanguage understanding [4], [5] and the successful solving of\nMWPs would constitute a milestone towards general AI.\n",
        "font-size": 9,
        "token": 163
      },
      {
        "text": "We categorize the evolution of MWP solvers into three major\nstages according to the technologies behind them, as depicted\nin Figure 1. In the ﬁrst pioneering stage, roughly from the year\n1960 to 2010, systems such as STUDENT [1], DEDUCOM [6],\nWORDPRO [7] and ROBUST [8], manually craft rules and\nschemas for pattern matchings. Thereupon, these solvers heavily\nrely on human interventions and can only resolve a limited number\nof scenarios that are deﬁned in advance. Those early efforts\nfor automatic understanding of natural language mathematical\nproblems have been thoroughly reviewed in [9]. We exclude\nthem from the scope of this survey paper and focus on the\nrecent technology developments that have not been covered in\nthe previous survey [9].\n",
        "font-size": 9,
        "token": 177
      },
      {
        "text": "In the second stage, MWP solvers made use of semantic\nparsing [10], [11], with the objective of mapping the sentences\n",
        "font-size": 9,
        "token": 32
      },
      {
        "text": "• D. Zhang and L. Zhang are with the College of Computer Science\nand Technology, Zhejiang University, China. Emails: {zhangdongxi-\nang37,zglumg}@gmail.com;\nL. Wang and H. T. Shen are with the Center for Future Media and\nSchool of Computer Science and Engineering, University of Electronic\nScience and Technology of China. Emails: demolei@outlook.com;\nshenhengtao@hotmail.com\n •\n • B. T. Dai is with School of Information Systems, Singapore Management\n",
        "font-size": 8,
        "token": 116
      },
      {
        "text": "University. Email:btdai@smu.edu.sg\n• Corresponding Author: Heng Tao Shen.\n",
        "font-size": 8,
        "token": 22
      },
      {
        "text": "Semantic \nParsing\n",
        "font-size": 8,
        "token": 4
      },
      {
        "text": "Feature \nEngineering\n",
        "font-size": 8,
        "token": 4
      },
      {
        "text": "Rule-based \nMatching\n",
        "font-size": 8,
        "token": 5
      },
      {
        "text": "Statistical \nLearning\n Deep Learning\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "Reinforcement \nLearning\n 1960-2010\n Fig. 1. Technology evolving trend in solving MWPs.\n",
        "font-size": 7,
        "token": 30
      },
      {
        "text": "from problem statements into structured logic representations so\nas to facilitate quantitative reasoning. It has regained considerable\ninterests from the academic community, and a booming number\nof methods have been proposed in the past years. These methods\nleveraged various strategies of feature engineering and statistical\nlearning for performance boosting. The authors of these methods\nalso claimed promising results in their public or manually\nharvested datasets. In this paper, one of our tasks is to present a\ncomprehensive review on the proposed methods in this stage. The\nmethods will be initially organized according to the sub-tasks of\nMWPs which they were designed to solve, such as arithmetic word\nproblem (in Section 2), equation set word problem (in Section 3)\nand geometric word problem (in Section 5). We then examine\nthe proposed techniques in each sub-task with a clear technical\norganization and accountable experimental evaluations.\n",
        "font-size": 9,
        "token": 183
      },
      {
        "text": "MWP solvers in the third stage were originated from an\nempirical work [12]. Its experimental results on a large-scale\nand diversiﬁed dataset showed that the status of MWP solvers\nthe\nwas not as optimistic as they claimed to be.\naccuracies of many approaches dropped sharply and there is a\ngreat room for improvement in this research area. To design\nmore accurate and robust solutions, the subsequent publications\nare forked into two directions. One is to continue reﬁning\nthe technology of semantic parsing. For instance, Huang et al.\n",
        "font-size": 9,
        "token": 118
      },
      {
        "table": "|None|Semantic Parsing Feature Engineering Statistical Learning|None|\n|Rule-based Matching|None|None|\n|None|None|Deep Learning Reinforcement Learning|",
        "token": 32
      }
    ]
  },
  {
    "page_num": 2,
    "content": [
      {
        "text": "proposed a new type of semantic representation to conduct ﬁne-\ngrained inference [13]. The other direction attempts to exploit the\nadvantages of deep learning models, with the availability of large-\nscale training datasets. This is an emerging research direction\nfor MWP solvers and we observed multiple instances, including\nDeep Neural Solver [14], Seq2SeqET [15], StackDecoder [16],\nMathDQN [17], CASS [18], T-RNN [19]. These models represent\na new technology trend in the research topic of MWP solvers and\nwill be paid special attention in the survey.\n",
        "font-size": 9,
        "token": 135
      },
      {
        "text": "To sum up, we present a comprehensive survey to review\nthe MWP solvers proposed in recent years. Researchers in the\ncommunity can beneﬁt from this survey in the following ways:\n",
        "font-size": 9,
        "token": 39
      },
      {
        "text": "1) We provide a wide coverage on the math word problems,\nincluding arithmetic word problem, equation set problem,\ngeometry word problem and miscellaneous sub-tasks related\nto automatic math solvers. The practitioners can easily\nidentify all relevant approaches for performance evaluations.\nWe observed that the unawareness of relevant competitors\noccurs occasionally in the past literature. We are positive\nthat\nthe availability of our survey can help avoid such\nunawareness.\n",
        "font-size": 9,
        "token": 89
      },
      {
        "text": "2) The solvers designed for arithmetic word problems (AWP)\nwith only one unknown variable and equation set problems\n(ESP) with multiple unknown variables are often not differ-\nentiated by previous works. In fact, the methods proposed for\nESP are more general and can be used to solve AWP. In this\nsurvey, we clearly identify the difference and organize them\nin separate sections.\n",
        "font-size": 9,
        "token": 81
      },
      {
        "text": "3) Feature engineering plays a vital role to bridge the gap\nof semantic parsing. Almost all MWP solvers state their\nstrategies of crafting effective features, resulting in a very\ndiversiﬁed group of features, and there is a lack of clear\norganization among these features. In this survey, we will be\nthe ﬁrst to summarize all these proposed features in Table 5.\n4) As for the fairness on performance evaluations, ideally, there\nshould be a benchmark dataset well accepted and widely\nadopted by the MWP research community, just like Ima-\ngeNet [20] for visual object recognition and VQA [21], [22]\nfor visual question answering. Unfortunately, we observed\nthat many approaches tend to compile their own datasets\nto verify their superiorities, resulting in missing of relevant\ncompetitors as mentioned above. Tables 2 and 3 integrate\nthe results of the existing methods on all public datasets.\nAfter collecting the accuracies that have been reported in\nthe past literature, we observed many empty cells in the\ntable. Each empty cell refers to a missing experiment on a\nparticular algorithm and dataset. In this survey, we make our\nbest efforts to ﬁll the missing results by conducting a number\nof additional experiments, that allow us to provide a more\ncomprehensive comparison and explicit analysis.\n",
        "font-size": 9,
        "token": 282
      },
      {
        "text": "The remainder of the paper is organized as follows. We\nﬁrst review the arithmetic word problem solvers in Section 2,\nfollowed by equation set problem solvers in Section 3. Since\nfeature engineering deserves special attention, we summarize\nthe extracted features as well as the associated pre-processing\ntechniques in Section 4. The geometric word problem solvers are\nreviewed in Section 5. We also cover miscellaneous automatic\nsolvers related to math problems in Section 6. We conclude the\npaper and point out several future directions in MWPs that are\nworth examination in the ﬁnal section.\n",
        "font-size": 9,
        "token": 126
      },
      {
        "text": "2\n",
        "font-size": 6,
        "token": 2
      },
      {
        "text": "2 ARITHMETIC WORD PROBLEM SOLVER\n is the text description for\n",
        "font-size": 9,
        "token": 17
      },
      {
        "text": "The arithmetic word problems are targeted for elementary\nthe\nschool students. The input\nmath problem, represented in the form of a sequence of k\nwords hw0, w1,\t, wki. There are n quantities q1, q2,\t, qn\nmentioned in the text and an unknown variable x whose value is to\nbe resolved. Our goal is to extract the relevant quantities and map\nthis problem into an arithmetic expression E whose evaluation\nvalue provides the solution to the problem. There are only four\ntypes of fundamental operators O = {+, −, ×, ÷} involved in the\nexpression E.\n",
        "font-size": 9,
        "token": 130
      },
      {
        "text": "An example of arithmetic word problem is illustrated in\nFigure 2. The relevant quantities to be extracted from the text\ninclude 17, 7 and 80. The number of hours spent on the bike is\nthe unknown variable x. To solve the problem, we need to identify\nthe correct operators between the quantities and their operation\norder such that we can obtain the ﬁnal equation 17 + 7x = 80 or\nexpression x = (80 − 17) ÷ 7 and return 9 as the solution to this\nproblem.\n",
        "font-size": 9,
        "token": 122
      },
      {
        "text": "Fig. 2. An example of arithmetic word problem.\n",
        "font-size": 7,
        "token": 12
      },
      {
        "text": "In this section, we consider feature extraction as a black\nbox and focus on the high-level algorithms and models. The\ndetails of feature extraction will be comprehensively present in\nSection 4. We classify existing algebra word problem solvers into\nthree categories: rule-based, statistic-based, tree-based and deep\nlearning (DL)-based methods.\n 2.1 Rule-based Methods\n",
        "font-size": 9,
        "token": 79
      },
      {
        "text": "The early approaches to math word problems are rule-based\nsystems based on hand engineering. Published in 1985, WORD-\nPRO [7] solves one-step arithmetic problems. It predeﬁnes four\nincluding change-in, change-out, combine\ntypes of schemas,\nand compare. The problem text is transformed into a set of\npropositions and the answer is derived with simple reasoning\nbased on the propositions. Another system ROBUST, developed\nby Bakman [8], could understand free-format multi-step arith-\nmetic word problems. It further expands the change schema of\nWORDPRO [7] into six distinct categories. The problem text is\nsplit into sentences and each sentence is mapped to a proposition.\nYun et al. also proposed to use schema for multi-step math\nproblem solving [23]. However, the implementation details are\nnot explicitly revealed. Since these systems have been out of date,\nwe only provide such a brief overview to cover the representative\nones. Readers can refer to [9] for a comprehensive survey of\n",
        "font-size": 9,
        "token": 216
      },
      {
        "table": "||\n||\n||\n||\n||\n||\n||",
        "token": 7
      }
    ]
  },
  {
    "page_num": 3,
    "content": [
      {
        "text": "early rule-driven systems for automatic understanding of natural\nlanguage math problems.\n 2.2 Statistic-based Methods\n",
        "font-size": 9,
        "token": 23
      },
      {
        "text": "The statistic-based methods leverage traditional machine learning\nmodels to identify the entities, quantities and operators from the\nproblem text and yield the numeric answer with simple logic\ninference procedure. The scheme of quantity entailment proposed\nin [24] can be used to solve arithmetic problems with only one\noperator. It involves three types of classiﬁers to detect different\nproperties of the word problem. The quantity pair classiﬁer is\ntrained to determine which pair of quantities would be used\nto derive the answer. The operator classiﬁer picks the operator\nop ∈ {+, −, ×, ÷} with the highest probability. The order classiﬁer\nis relevant only for problems involving subtraction or division\nbecause the order of operands matters for these two types of\noperators. With the inferred expression, it is straightforward to\ncalculate the numeric answer for the simple math problem.\n",
        "font-size": 9,
        "token": 183
      },
      {
        "text": "To solve math problems with multi-step arithmetic expression,\nthe statistic-based methods require more advanced logic templates.\nThis usually incurs additional overhead to annotate the text\nproblems and associate them with the introduced template. As\nan early attempt, ARIS [25] deﬁnes a logic template named\nstate that consists of a set of entities, their containers, attributes,\nquantities and relations. For example, “Liz has 9 black kittens”\ninitializes the number of kitten (referring to an entity) with\nblack color (referring to an attribute) and belonging to Liz\n(referring to a container). The solution splits the problem text\ninto fragments and tracks the update of the states by verb\ncategorization. More speciﬁcally, the verbs are classiﬁed into\nseven categories: observation, positive, negative, positive transfer,\nnegative transfer, construct and destroy. To train such a classiﬁer,\nwe need to annotate each split fragment in the training dataset\nwith the associated verb category. Another drawback of ARIS\nis that it only supports addition and subtraction.\n[26] follows a\nsimilar processing logic to ARIS. It predeﬁnes a corpus of logic\nrepresentation named schema, inspired by [8]. The sentences in the\ntext problem are examined sequentially until the sentence matches\na schema, triggering an update operation to modify the number\nassociated with the entities.\n",
        "font-size": 9,
        "token": 291
      },
      {
        "text": "Mitra et al. proposed a new logic template named formula [27].\nThree types of formulas are deﬁned, including part whole, change\nand comparison, to solve problems with addition and subtraction\noperators. For example, the text problem “Dan grew 42 turnips\nand 38 cantelopes. Jessica grew 47 turnips. How many turnips\ndid they grow in total?” is annotated with the part-whole template:\nhwhole : x, parts : {42, 47}i. To solve a math problem, the ﬁrst\nstep connects the assertions to the formulas. In the second step,\nthe most probable formula is identiﬁed using the log-linear model\nwith learned parameters and converted into an algebraic equation.\nAnother type of annotation is introduced in [28], [29] to\nfacilitate solving a math problem. A group of logic forms are\npredeﬁned and the problem text is converted into the logic form\nrepresentation by certain mapping rules. For instance, the sentence\n“Fred picks 36 limes” will be transformed into verb(v1, pick)\n& nsubj(v1, Fred) & dobj(v1, n1) & head(n1, lime) &\nnummod(n1, 36). Finally, logic inference is performed on the\nderived logic statements to obtain the answer.\n",
        "font-size": 9,
        "token": 291
      },
      {
        "text": "To sum up, these statistical-based methods have two draw-\nbacks that limit their usability. First, it requires additional an-\nnotation overhead that prevents them from handling large-scale\n",
        "font-size": 9,
        "token": 35
      },
      {
        "text": "3\n",
        "font-size": 6,
        "token": 2
      },
      {
        "text": "datasets. Second, these methods are essentially based on a set\nof pre-deﬁned templates, which are brittle and rigid. It will take\ngreat efforts to extend the templates to support other operators\nlike multiplication and division. It is also not robust to diversiﬁed\ndatasets. In the following, we will\nintroduce the tree-based\nsolutions, which are widely adopted and become the main-\nstreaming solutions to arithmetic word problems.\n 2.3 Tree-Based Methods\n",
        "font-size": 9,
        "token": 100
      },
      {
        "text": "The arithmetic expression can be naturally represented as a binary\ntree structure such that the operators with higher priority are\nplaced in the lower level and the root of the tree contains\nthe operator with the lowest priority. The idea of tree-based\napproaches [30], [31], [32], [17] is to transform the derivation\nof the arithmetic expression to constructing an equivalent tree\nstructure step by step in a bottom-up manner. One of the\nadvantages is that there is no need for additional annotations such\nas equation template, tags or logic forms. Figure 3 shows two tree\nexamples derived from the math word problem in Figure 2. One\nis called expression tree that is used in [30], [32], [17] and the\nother is called equation tree [31]. These two types of trees are\nessentially equivalent and result in the same solution, except that\nequation tree contains a node for the unknown variable x.\n",
        "font-size": 9,
        "token": 203
      },
      {
        "text": "÷ \n -\n 7\n 80\n 17\n =\n +\n 80\n 17\n ×\n 7\n x\n Expression Tree\n Equation Tree\n Fig. 3. Examples of expression tree and equation tree for Figure 2.\n",
        "font-size": 7,
        "token": 54
      },
      {
        "text": "The overall algorithmic framework among the tree-based\napproaches consists of two processing stages. In the ﬁrst stage,\nthe quantities are extracted from the text and form the bottom\nlevel of the tree. The candidate trees that are syntactically valid,\nbut with different structures and internal nodes, are enumerated.\nIn the second stage, a scoring function is deﬁned to pick the best\nmatching candidate tree, which will be used to derive the ﬁnal\nsolution. A common strategy among these algorithms is to build\na local classiﬁer to determine the likelihood of an operator being\nselected as the internal node. Such local likelihood is taken into\naccount in the global scoring function to determine the likelihood\nof the entire tree.\n",
        "font-size": 9,
        "token": 151
      },
      {
        "text": "Roy et al. [30] proposed the ﬁrst algorithmic approach that\nleverages the concept of expression tree to solve arithmetic\nword problems. Its ﬁrst strategy to reduce the search space is\ntraining a binary classiﬁer to determine whether an extracted\nquantity is relevant or not. Only the relevant ones are used for\ntree construction and placed in the bottom level. The irrelevant\nquantities are discarded. The tree construction procedure is\nmapped to a collection of simple prediction problems, each\ndetermining the lowest common ancestor operation between a\npair of quantities mentioned in the problem. The global scoring\nfunction for an enumerated tree takes into account two terms. The\n",
        "font-size": 9,
        "token": 141
      }
    ]
  },
  {
    "page_num": 4,
    "content": [
      {
        "text": "ﬁrst one, denoted by φ(q), is the likelihood of quantity q being\nirrelevant, i.e., q is not used in creating the expression tree. In the\nideal case, all the irrelevant quantities are correctly predicted with\nhigh conﬁdence, resulting in a large value for the sum of φ(q).\nThe other term, denoted by φ(op), is the likelihood of selecting\nop as the operator for an internal tree node. With these two factors,\nScore(E) is formally deﬁned as\n",
        "font-size": 9,
        "token": 108
      },
      {
        "text": "Score(E) = w1 X\nq∈I (E)\n",
        "font-size": 9,
        "token": 14
      },
      {
        "text": "φ(q) + X\nop∈N\n φ(op)\n (1)\n",
        "font-size": 9,
        "token": 16
      },
      {
        "text": "where I (E) is the group of irrelevant quantities that are not\nincluded in expression E, and N refers to the set of internal\ntree nodes. To further reduce the tree enumeration space, beam\nsearch is applied in [30]. To generate the next state T ′ from\nthe current partial tree, the algorithm avoids choosing all the\npossible pairs of terms and determining their operator. Instead,\nonly top-k candidates with the highest partial scores are retained.\nExperimental results with k = 200 show that the strategy achieves\na good balance between accuracy and running time. The service is\nalso published as a web tool [33] and it can respond promptly to a\nmath word problem.\n",
        "font-size": 9,
        "token": 145
      },
      {
        "text": "ALGES [31] differs from [30] in two major ways. First, it\nadopts a more brutal-force manner to exploit all the possible\nequation trees. More speciﬁcally, ALGES does not discard\nirrevalent quantities, but enumerates all the syntactically valid\ntrees. Integer Linear Programming (ILP) is applied as it can help\nenforce the constraints such as syntactic validity, type consistence\nand domain speciﬁc simplicity considerations. Consequently, its\ncomputation cost is dozens of times higher than that in [30],\naccording to an efﬁciency evaluation in [17]. Second, its scoring\nfunction is different from Equation 1. There is no need for the\nterm φ(q) because ALGES does not build a classiﬁer to check\nthe quantity relevance. Besides the monotonic aggregation of the\nlikelihood from local operator classiﬁers, the scoring function\nincorporates a new term φ(P) = θT fp to assign a coherence score\nfor the tree instance. Here, fP is the global feature extracted from\na problem text P, and θ refers to the parameter vector.\n",
        "font-size": 7,
        "token": 251
      },
      {
        "text": "The goal of [34] is also to build an equation tree by parsing the\nproblem text. It makes two assumptions that can simplify the tree\nconstruction, but also limit its applicability. First, the ﬁnal output\nequation form is restricted to have at most two variables. Second,\neach quantity mentioned in the sentence can be used at most once\nin the ﬁnal equation. The tree construction procedure consists\nof a pipeline of predictors that\nidentify irrelevant quantities,\nrecognize grounded variables, and generate the ﬁnal equation tree.\nWith customized feature selection and SVM based classiﬁer, the\nrelevant quantities and variables are extracted and used as the leaf\nnodes of the equation tree. The tree is built in a bottom-up manner.\nIt is worth noting that to reduce the search space and simplify the\ntree construction, only adjacent nodes are combined to generate\ntheir parent node.\n",
        "font-size": 9,
        "token": 184
      },
      {
        "text": "UnitDep [32] can be viewed as an extension work of [30] by\nthe same authors. An important concept, named Unit Dependency\nGraph (UDG), is proposed to enhance the scoring function. The\nvertices in UDG consist of the extracted quantities. If the quantity\ncorrespond to a rate (e.g., 8 dollars per hour), the vertex is marked\nas RATE. There are six types of edge relations to be considered,\nsuch as whether two quantities are associated with the same unit.\nBuilding the UDG requires additional annotation overhead as we\nneed to train two classiﬁers for the nodes and edges. The node\nclassiﬁer determines whether a node is associated with a rate. The\n",
        "font-size": 9,
        "token": 151
      },
      {
        "text": "4\n",
        "font-size": 6,
        "token": 2
      },
      {
        "text": "edge classiﬁer predicts the type of relationship between any pair of\nquantity nodes. Given a valid unit dependency graph G generated\nby the classiﬁers, its likelihood is deﬁned as\n φ(G) =\n",
        "font-size": 9,
        "token": 46
      },
      {
        "text": "X\nv∈G∧LABEL(v)=rate\n",
        "font-size": 7,
        "token": 11
      },
      {
        "text": "P(v) + λ X\ne∈G\n P(e)\n (2)\n",
        "font-size": 9,
        "token": 17
      },
      {
        "text": "In other words, we sum up the prediction probability for the\nRATE nodes and all the edges. The new scoring function for an\nexpression tree extends Equation 1 by incorporating φ(G). Rules\nare deﬁned to enforce the rate consistence between an expression\ntree T and a candidate graph G. For example, if vi is the only node\nin the tree that is labeled RATE and it appears in the question,\nthere should not exist a path from the leaf node to the root which\nonly contains operators of addition and subtraction. Finally, the\ncandidate graph G with the highest likelihood and rate-consistent\nwith T is used to calculate the total score of T .\n",
        "font-size": 7,
        "token": 137
      },
      {
        "text": "2.4 DL-based Methods\n [42],\n interest\n",
        "font-size": 9,
        "token": 13
      },
      {
        "text": "In recent years, deep learning (DL) has witnessed great success\nin a wide spectrum of “smart” applications, such as visual\nquestion answering [35], [36], video captioning [37], [38], [39],\n[40], personal\n[43] and smart\ninference [41],\ntransportation [44]. The main advantage is that with sufﬁcient\ntraining data, DL is able to learn an effective\namount of\nfeature representation in a data-driven manner without human\nintervention. It is not surprising to notice that several efforts have\nbeen attempted to apply DL for math word problem solving.\nDeep Neural Solver (DNS) [14] is a pioneering work designed for\nequation set problems, which will be introduced in more details\nin the next section. Following DNS, there have emerged multiple\nDL-based solvers for arithmetic word problems. Seq2SeqET [15]\nextended the idea of DNS by using expression tree as the output\nsequence. In other words, it applied seq2seq model to convert\nthe problem text into an expression tree, which can be viewed as\na template. Given the output of an expression tree or template,\nwe can easily infer the numeric answer. To reduce the template\nspace, equation normalization was proposed in Seq2SeqET so\nthat duplicate representation of expression trees can be uniﬁed.\nStackDecoder [16] is also based on seq2seq model. Its encoder\nextracts semantic meanings of quantities in the question text and\nthe decoder is equipped with a stack to facilitate tracking the\nsemantic meanings of operands. T-RNN [19] can be viewed as\nan improvement of Seq2SeqET, in terms of quantity encoding,\ntemplate representation and tree construction. First, an effective\nembedding network with Bi-LSTM and self attention is used\nthe detailed operators in\nto vectorize the quantities. Second,\nthe templates are encapsulated to further reduce the number of\ntemplate space. For example, n1 + n2, n1 − n2, n1 × n2, and\nn1 ÷ n2 are mapped to the same template n1hopin2. Third, a\nrecursive neural network is applied to infer the unknown variables\nin the expression tree in a recursive manner.\n",
        "font-size": 9,
        "token": 479
      },
      {
        "text": "Wang et al. made the ﬁrst attempt of applying deep re-\ninforcement learning to solve arithmetic word problems [17].\nThe motivation is that deep Q-network has witnessed success\nin solving various problems with big search space such as\nplaying text-based games [45], information extraction [46], text\ngeneration [47] and object detection in images [48]. To ﬁt\nthe math problem scenario, they formulate the expression tree\nconstruction as a Markov Decision Process and propose the\nMathDQN that is customized from the general deep reinforcement\n",
        "font-size": 9,
        "token": 119
      }
    ]
  },
  {
    "page_num": 5,
    "content": [
      {
        "text": "learning framework. Technically, they tailor the deﬁnitions of\nstates, actions, and reward functions which are key components in\nthe reinforcement learning framework. By using a two-layer feed-\nforward neural network as the deep Q-network to approximate\nthe Q-value function, the framework learns model parameters\nfrom the reward feedback of the environment. Compared to the\naforementioned approaches, MathDQN iteratively picks the best\noperator for two selected quantities. This procedure can be viewed\nas beam search with k = 1 when exploiting candidate expression\ntrees. Its deep Q-network acts as the operator classiﬁer and\nguides the model to select the most promising operator for tree\nconstruction.\n 2.5 Dataset Repository and Performance Analysis\n",
        "font-size": 9,
        "token": 154
      },
      {
        "text": "The accuracy of arithmetic word problems is evaluated on the\ndatasets that are manually harvested and annotated from online\nwebsites. These datasets are small-scale and contain hundreds\nof math problems. In this subsection, we make a summary on\nthe datasets that have been used in the aforementioned papers.\nMoreover, we organize the performance results on these datasets\ninto one uniﬁed table. We also make our best efforts to conduct\nadditional experiments. The new results are highlighted in blue\ncolor. In this way, readers can easily identify the best performers\nin each dataset.\n 2.5.1 Datasets\n",
        "font-size": 9,
        "token": 123
      },
      {
        "text": "There have been a number of datasets collected for the arithmetic\nword problems. We present their descriptions in the following and\nsummarize the statistics of the datasets in Table 1.\n",
        "font-size": 9,
        "token": 37
      },
      {
        "text": "1) AI2 [25]. There are 395 single-step or multi-step arithmetic\nword problems for the third, fourth, and ﬁfth graders. It\ninvolves problems that can be solved with only addition\nand subtraction. The dataset is harvested from two websites:\nmath-aids.com and ixl.com and comprises three subsets:\nMA1 (from math-aids.com),\nIXL (from ixl.com) and\nMA2 (from math-aids.com). Among them, IXL and MA2\nare more challenging than MA1 because IXL contains\nmore information gaps and MA2 includes more irrelevant\ninformation in its math problems.\n",
        "font-size": 9,
        "token": 139
      },
      {
        "text": "2) IL [30]. The problems are collected from websites\nk5learning.com and dadsworksheets.com. The problems\nthat require background knowledge (e.g., “apple is fruit”\nand “a week comprises 7 days”) are pruned. To improve the\ndiversity, the problems are clustered by textual similarity.\nFor each cluster, at most 5 problems are retained. Finally, the\ndataset contains 562 single-step word problems with only\none operator, including addition, subtraction, multiplication,\nand division.\n",
        "font-size": 9,
        "token": 111
      },
      {
        "text": "3) CC [30]. The dataset is designed for mult-step math prob-\nlems. It contains 600 multi-step problems without irrelevant\nquantities, harvested from commoncoresheets.com. The\ndataset involves various combinations of four basic operators,\nincluding (a) addition followed by subtraction; (b) subtraction\nfollowed by addition; (c) addition and multiplication; (d)\naddition and division; (e) subtraction and multiplication; and\n(f) subtraction and division. It is worth noting that this dataset\ndoes not incorporate irrelevant quantities in the problem text.\nHence, there is no need to apply the quantity relevance\nclassiﬁer for the algorithms containing this component.\n",
        "font-size": 9,
        "token": 143
      },
      {
        "text": "5\n",
        "font-size": 6,
        "token": 2
      },
      {
        "text": "TABLE 1\nStatistics of arithmetic word problem datasets.\n",
        "font-size": 7,
        "token": 11
      },
      {
        "text": "Dataset\nMA1\nIXL\nMA2\nAI2\nIL\nCC\nSingleEQ\nAllArith\nMAWPS-S\nDolphin-S\nMath23K\n",
        "font-size": 8,
        "token": 40
      },
      {
        "text": "# problems\n134\n140\n121\n395\n562\n600\n508\n831\n2,373\n7,070\n23,162\n",
        "font-size": 8,
        "token": 54
      },
      {
        "text": "# single-op\n112\n119\n96\n327\n562\n0\n390\n634\n1,311\n115\n3,131\n",
        "font-size": 8,
        "token": 49
      },
      {
        "text": "# multi-op\n22\n21\n25\n68\n0\n600\n118\n197\n1,062\n6,955\n20,031\n",
        "font-size": 8,
        "token": 49
      },
      {
        "text": "operators O\n{+, −}\n{+, −}\n{+, −}\n{+, −}\n{+, −, ×, ÷}\n{+, −, ×, ÷}\n{+, −, ×, ÷}\n{+, −, ×, ÷}\n{+, −, ×, ÷}\n{+, −, ×, ÷}\n{+, −, ×, ÷}\n",
        "font-size": 8,
        "token": 82
      },
      {
        "text": "4) SingleEQ [31]. The dataset contains both single-step and\nmulti-step arithmetic problems and is a mixture of prob-\nlems from a number of sources, including math-aids.com,\nk5learning.com, ixl.com and a subset of the data from\nAI2. Each problem involves operators of multiplication,\ndivision, subtraction, and addition over non-negative rational\nnumbers.\n",
        "font-size": 9,
        "token": 79
      },
      {
        "text": "5) AllArith [32]. The dataset is a mixture of the data from\nAI2, IL, CC and SingleEQ. All mentions of quantities are\nnormalized into digit representation. To capture how well the\nautomatic solvers can distinguish between different problem\ntypes, near-duplicate problems (with over 80% match of\nunigrams and bigrams) are removed. Finally, there remain\n831 math problems.\n",
        "font-size": 9,
        "token": 91
      },
      {
        "text": "6) MAWPS [49] is another testbed for arithmetic word prob-\nlems with one unknown variable in the question. Its objective\nis to compile a dataset of varying complexity from different\nwebsites. Operationally,\nit combines the published word\nproblem datasets used in [25], [50], [31], [30]. There are\n2, 373 questions in the harvested dataset.\n",
        "font-size": 9,
        "token": 88
      },
      {
        "text": "7) Dolphin-S. This is a subset of Dolphin18K [12] which\noriginally contains 18, 460 problems and 5, 871 templates\nwith one or multiple equations. The problems whose template\nis associated with only one problem are extracted as the\ndataset of Dolphin-S. It contains 115 problems with single\noperator and 6, 955 problems with multiple operators.\n",
        "font-size": 9,
        "token": 93
      },
      {
        "text": "8) Math23K [14]. The dataset contains Chinese math word\nproblems for elementary school students and is crawled\nfrom multiple online education websites. Initially, 60, 000\nproblems with only one unknown variable are collected. The\nequation templates are extracted in a rule-based manner. To\nensure high precision, a large number of problems that do\nnot ﬁt the rules are discarded. Finally, 23, 161 math problems\nwith 2, 187 templates remain.\n",
        "font-size": 9,
        "token": 112
      },
      {
        "text": "2.5.2 Performance Analysis\nGiven the aforementioned datasets, we merge the experimental\nresults reported from previous works into one table. Such a uniﬁed\norganization can facilitate readers in identifying the methods with\nsuperior performance in each dataset. As shown in Table 2, the\nrows refer to the corpus of datasets and the columns are the\nstatistic-based and tree-based methods. The cells are ﬁlled with the\naccuracies of these algorithms when solving math word problems\nin different datasets. We conduct additional experiments to cover\nall the cells by the tree-based solutions. These new experiment\nresults are highlighted in blue color. Those with missing value\n",
        "font-size": 9,
        "token": 137
      }
    ]
  },
  {
    "page_num": 6,
    "content": [
      {
        "text": "TABLE 2\nAccuracy of statistic-based and tree-based methods in solving arithmetic problems.\n 6\n",
        "font-size": 6,
        "token": 20
      },
      {
        "text": "# problems\noperators O\n",
        "font-size": 7,
        "token": 6
      },
      {
        "text": "ARIS [25]\nSchema [26]\nFormula [27]\nLogicForm [28], [29]\nALGES [31]\nExpressionTree [30]\nUNITDEP [32]\nMathDQN [17]\nSeq2SeqET [15]\nT-RNN [19]\nStackDecoder [16]\n",
        "font-size": 7,
        "token": 73
      },
      {
        "text": "2014\n2015\n2016\n2016\n2015\n2015\n2017\n2018\n2018\n2019\n2019\n",
        "font-size": 7,
        "token": 55
      },
      {
        "text": "AI2\n395\n{+, −}\n77.7\n88.64\n86.07\n84.8\n52.4\n72\n56.2\n78.5\n-\n-\n-\n",
        "font-size": 7,
        "token": 54
      },
      {
        "text": "IL\n562\n{+, −, ×, ÷}\n-\n-\n-\n80.1\n72.9\n73.9\n71.0\n73.3\n-\n-\n-\n",
        "font-size": 7,
        "token": 46
      },
      {
        "text": "CC\n600\n{+, −, ×, ÷}\n-\n-\n-\n53.5\n65\n45.2\n53.5\n75.5\n-\n-\n-\n",
        "font-size": 7,
        "token": 44
      },
      {
        "text": "SingleEQ\n508\n{+, −, ×, ÷}\n48\n-\n-\n-\n72\n66.38\n72.25\n52.96\n-\n-\n-\n",
        "font-size": 7,
        "token": 46
      },
      {
        "text": "AllArith\n831\n{+, −, ×, ÷}\n-\n-\n-\n-\n60.4\n79.4\n81.7\n72.68\n-\n-\n-\n",
        "font-size": 7,
        "token": 45
      },
      {
        "text": "Dolphin-S\n7,070\n{+, −, ×, ÷}\n-\n-\n-\n-\n-\n26.11\n28.78\n30.06\n-\n39.1\n-\n",
        "font-size": 7,
        "token": 50
      },
      {
        "text": "MAWPS-S\n2,373\n{+, −, ×, ÷}\n-\n-\n-\n-\n-\n-\n-\n60.25\n-\n66.8\n-\n",
        "font-size": 7,
        "token": 40
      },
      {
        "text": "Math23K\n23,162\n{+, −, ×, ÷}\n-\n-\n-\n-\n-\n-\n-\n-\n66.7\n66.9\n65.8\n Statistic-based\n Tree-based\n DL-based\n",
        "font-size": 7,
        "token": 54
      },
      {
        "text": "are indicated by “-” and it means that there was no experiment\nconducted for the algorithm in the particular dataset. The main\nreason is that they require particular efforts on logic templates\nand annotations, which are very non-trivial and cumbersome for\nexperiment reproduction. There is no algorithm comparison for\nthe dataset Math23K because the problem text is in Chinese and\nthe feature extraction technologies proposed in the statistic-based\nand tree-based approaches are not applicable. From the results in\nTable 2, we derive the following observations worth noting and\nprovide reasonings to explain the results.\n",
        "font-size": 9,
        "token": 121
      },
      {
        "text": "First, the statistic-based methods with advanced logic rep-\nresentation, such as Schema [26], Formula [27] and Logic-\nForm [28], [29], achieve dominating performance in the AI2\ndataset. Their superiority is primarily owned to the additional\nefforts on annotating the text problem with more advanced logic\nrepresentation. These annotations allow them to conduct ﬁne-\ngrained reasoning. In contrast, ARIS [25] does not work as\ngood because it focuses on “change” schema of quantities and\ndoes not fully exploit other schemas like “compare” [26]. Since\nthere are only hundreds of math problems in the datasets, it is\nfeasible to make an exhaustive scan on the math problems and\nmanually deﬁne the templates to ﬁt these datasets. For instance,\nall quantities and the main-goal are ﬁrst identiﬁed by rules in\nLogicForm [28], [29] and explicitly associated with their role-\ntags. Thus, with sufﬁcient human intervention, the accuracy of\nstatistic-baesd methods in AI2 can boost to 88.64%, much higher\nthan that of tree-based methods. Nevertheless, these statistic-based\nmethods are considered as brittle and rigid [12] and not scalable\nto handle large and diversiﬁed datasets, primarily due to the heavy\nannotation cost to train an accurate mapping between the text and\nthe logic representation.\n",
        "font-size": 9,
        "token": 304
      },
      {
        "text": "Second, the results of tree-based methods in AI2, IL and CC\nare collected from [17] where the same experimental setting of 3-\nfold cross validation is applied. It is interesting to observe that\nALGES [31], ExpressionTree [30] and UNITDEP [32] cannot\nperform equally well on the three datasets. ALGES works poorly\nin AI2 because irrelevant quantities exist in its math problems\nand ALGES is not trained with a classiﬁer to get rid of them.\nHowever, it outperforms ExpressionTree and UNITDEP by a wide\nmargin in the CC dataset because CC does not involve irrelevant\nquantities. In addition, this dataset only contains multi-step math\nproblems. ALGES exploits the whole search space to enumerate\nall the possible trees, whereas ExpressionTree and ALGES use\nbeam search for efﬁciency concern. UNITDEP does not work\n",
        "font-size": 9,
        "token": 193
      },
      {
        "text": "well in AI2 because this dataset only involves operators + and\n− and the unit dependency graph does not take effect. Moreover,\nits proposed context feature poses a negative impact on the AI2\ndataset. After removing this feature from the input vector fed to\nthe classiﬁer, the accuracy in AI2 increases from 56.2% to 74.7%,\nbut the result in CC drops from the current accuracy of 53.5% to\n47.3%. Such an observation implies the limitation of hand-crafted\nfeatures in UNITDEP. Among the three datasets AI2, IL and CC,\nMathDQN [17] achieves leading or comparable performance. In\nthe CC dataset, which contains only multi-step problems and\nis considered as the most challenging one, MathDQN yields\nremarkable improvement and boosts the accuracy from 65%\n(derived by ALGES) to 75.5%. This is because MathDQN models\nthe tree construction as Markov Decision Process and leverage the\nstrengths of deep Q-network (DQN). By using a two-layer feed-\nforward neural network as the deep Q-network to approximate the\nQ-value function, the framework learns model parameters from\nthe reward feedback of the environment. Consequently, the RL\nframework demonstrates higher generality and robustness than the\nother tree-based methods when handling complicated scenarios. In\nthe IL dataset, its performance is not superior to ExpressionTree\nas IL only contains one-step math problems. There is no need\nfor hierarchical tree construction and cannot expose the strength\nof Markov Decision Process in MathDQN or the exhaustive\nenumeration strategy in ALGES.\n",
        "font-size": 9,
        "token": 347
      },
      {
        "text": "As to the datasets of SingleEQ and AllArith, UNITDEP is\na winner in both datasets, owning to the effectiveness of the\nproposed unit dependency graph (UDG). In the math problems\nwith operators {×, ÷},\nthe unit and rate are important clues\nto determine the correct quantities and operators in the math\nexpression. The UDG poses constraints on unit compatibility to\nﬁlter the false candidates in the expression tree construction. It\ncan alleviate the brittleness of the unit extraction system, even\nthough it requires additional annotation overhead in order to\ninduce UDGs.\n",
        "font-size": 9,
        "token": 127
      },
      {
        "text": "Last but not the least, these experiments were conducted on\nsmall-scale datasets and their performances on larger and more\ndiversiﬁed datasets remain unclear. Recently, Huang et al. have\nnoticed the gap and released Dolphin18K [12] which contains\n18, 460 problems and 5, 871 templates with one or multiple\nequations. The ﬁndings in [12] are astonishing. The accuracies\nof existing approaches for equation set problems, which will be\nintroduced in the next section, degrade sharply to less than 25%.\nThese methods cannot even perform better than a simple baseline\n",
        "font-size": 9,
        "token": 138
      }
    ]
  },
  {
    "page_num": 7,
    "content": [
      {
        "text": "that ﬁrst uses text similarity to ﬁnd the most similar text problem in\nthe training dataset and then ﬁlls the number slots in its associated\nequation template. Math23K is another large-scale dataset which\ncontains Chinese math word problems. The results are shown in\nthe last three columns of Table 2. We can see that T-RNN is\nstate-of-the-art method in the two largest datasets Dolphin-S and\nMath23K.\n",
        "font-size": 9,
        "token": 98
      },
      {
        "text": "3 EQUATION SET SOLVER\nThe equation set problems are much more challenging because\nthey involve multiple unknown variables to resolve and require\nto formulate a set of equations to obtain the ﬁnal solution. The\naforementioned arithmetic math problem can be viewed as a\nsimpliﬁed variant of equation set problem with only one unknown\nvariable. Hence, the methods introduced in this section can also\nbe applied to solve the problems in Section 2.\n",
        "font-size": 8,
        "token": 95
      },
      {
        "text": "Figure 4 shows an example of equation set problem. There\nincluding the acres of corn and\nare two unknown variables,\nthe acres of wheat,\nto be inferred from the text description.\nA standard solution to this problem is to use variables x and\nthe number of corn and wheat, respectively.\ny to represent\nFrom the text understanding, we can formulate two equations\n42x + 30y = 18600 and x + y = 500. Finally, the values of x and\ny can be inferred.\n",
        "font-size": 9,
        "token": 113
      },
      {
        "text": "Compared to the arithmetic problem, the equation set problem\ncontains more numbers of unknown variables and numbers in the\ntext, resulting in a much larger search space to enumerate valid\ncandidate equations. Hence, the methods designed for arithmetic\nproblems can be hardly applied to solve equation set problems.\nFor instance, the tree-based methods assume that the objective is\nto construct a single tree to maximize a scoring function. They\nrequire substantial revision to adjust the objective to building\nmultiple trees which has exponentially higher search space. This\nwould be likely to degrade the performance. In the following, we\nwill review the existing methods, categorize them into four groups\nfrom the technical perspective, and examine how they overcome\nthe challenge.\n",
        "font-size": 9,
        "token": 146
      },
      {
        "text": "Fig. 4. An example of equation set problem.\n",
        "font-size": 7,
        "token": 12
      },
      {
        "text": "3.1 Parsing-Based Methods\n",
        "font-size": 9,
        "token": 7
      },
      {
        "text": "The work of [51] can be viewed as an extension of tree-based\napproaches to solve a math problem with multiple equations.\n",
        "font-size": 9,
        "token": 28
      },
      {
        "text": "7\n",
        "font-size": 6,
        "token": 2
      },
      {
        "text": "Since the objective is no longer to build an equation tree, a\nmeaning representation language called DOL is designed as the\nstructural semantic representation of natural language text. The\ncore component is a semantic parser that transforms the textual\nsentences into DOL trees. The parsing algorithm is based on\ncontext-free grammar (CFG) [52], [53], a popular mathematical\nsystem for modeling constituent structure in natural languages.\nFor every DOL node type, the lexicon and grammar rules are\nconstructed in a semi-supervised manner. The association between\nmath-related concepts and their grammar rules is manually\nconstructed. Finally, the CFG parser is built on top of 9, 600\ngrammar rules. During the parsing, a score is calculated for each\nDOL node and the derivation of the DOL trees with the highest\nscore is selected to obtain the answer via a reasoning module.\n 3.2 Similarity-Based Methods\n",
        "font-size": 9,
        "token": 194
      },
      {
        "text": "The work of [12] plays an important role in the research\nline of automatic math problem solvers because it rectiﬁes the\nunderstanding of technology development in this area. It, for the\nﬁrst time, examines the performance of previous approaches in a\nlarge and diversiﬁed dataset and derives astonishing experimental\nﬁndings. The methods that claimed to achieve an accuracy higher\nthan 70% in a small-scale and self-collected dataset exhibit very\npoor performance in the new dataset. In other words, none of the\nmethods proposed before [12] is really general and robust. Hence,\nthe authors reach a conclusion that the math word problems still\nhave much room for improvement.\n",
        "font-size": 9,
        "token": 149
      },
      {
        "text": "A new baseline method based on text similarity, named SIM, is\nproposed in [12]. In the ﬁrst step, the problem text is converted into\na word vector whose values are the associated TF-IDF scores [54].\nThe similarity between two problems (one is the query problem\nto solve and the other is a candidate in the training dataset with\nknown solutions) is calculated by the Jaccard similarity between\ntheir converted vectors. The problem with the highest similarity\nscore is identiﬁed and its equation template is used to help solve\nthe query math problem. In the second step, the unknown slots\nin the template are ﬁlled. With the availability of a large training\ndataset, the number ﬁlling is conducted in a simple and effective\nway. It ﬁnds an instance in the training dataset associated with\nthe same target template and the minimum edit-distance to the\nquery problem, and aligns the numbers in these two problems\nwith ordered and one-to-one mapping. It is considered a failure if\nthese two problems do not contain the same number of quantities.\n",
        "font-size": 9,
        "token": 231
      },
      {
        "table": "||\n||\n||\n||\n||\n||\n||",
        "token": 7
      }
    ]
  },
  {
    "page_num": 8,
    "content": [
      {
        "text": "involves a scoring function or a rank-aware classiﬁer such as\nRankSVM [56]. A widely-adopted practice is to deﬁne the\nprobability of each instance of derivation y based on the feature\nrepresentation x for a text problem and a parameter vector θ, as\nin [34], [50], [55]:\n",
        "font-size": 9,
        "token": 77
      },
      {
        "text": "p(y|x; θ) =\n",
        "font-size": 9,
        "token": 8
      },
      {
        "text": "eθ·φ(x,y)\nPy ′∈Y eθ·φ(x,y ′)\n",
        "font-size": 7,
        "token": 21
      },
      {
        "text": "With the optimal derivation instance yopt, we can obtain the ﬁnal\nsolution.\n",
        "font-size": 7,
        "token": 18
      },
      {
        "text": "The objective of the work from Kushman et al. [50] is to\nmaximize the total probabilities of y that leads to the correct\nanswer. The latent variables θ are learned by directly optimizing\nthe marginal data log-likelihood. More speciﬁcally, L-BFGS [57]\nis used to optimize the parameters. The search space is exponential\nto the number of slots because each number in the text can be\nmapped to any number slot and the nouns are also candidates for\nthe unknown slots. In practice, the search space is too huge to ﬁnd\nthe optimal θ and beam search inference procedure is adopted\nto prevent enumerating all the possible y leading to the correct\nanswer. For the completion of each template, the next slot to be\nconsidered is selected according to a pre-deﬁned canonicalized\nordering and only top-k partial derivations are maintained.\n",
        "font-size": 9,
        "token": 189
      },
      {
        "text": "Zhou et al. proposed an enhanced algorithm for the template-\nbased learning framework [55]. First, they only consider assigning\nthe number slots with numbers extracted from the text. The\nunderlying logic is that when the number slots have been\nprocessed, it would be an easy task to ﬁll the unknown slots.\nIn this way, the hypothesis space can be signiﬁcantly reduced.\nSecond, the authors argue that the beam search used in [50] does\nnot exploit all the training samples, and its resulting model may be\nsub-optimal. To resolve the issue, the max-margin objective [58]\nis used to train the log-linear model. The training process is turned\ninto a QP problem that can be efﬁciently solved with the constraint\ngeneration algorithm [59].\n",
        "font-size": 9,
        "token": 169
      },
      {
        "text": "Since the annotation of equation templates is expensive, a key\nchallenge to KAZB and ZDC is the lack of sufﬁcient annotated\ndata. To resolve the issue, Upadhyay et al. attempted to exploit\nthe large number of algebra word problems that have been posted\nand discussed in online forums [60]. These data are not explicitly\nannotated with equation templates but their numeric answers are\nextracted with little or no manual effort. The goal of\n[60] is to\nimprove a strong solver trained by fully annotated data with a large\nnumber of math problems with noisy and implicit supervision\nsignals. The proposed MixedSP algorithm makes use of both\nexplicit and implicit supervised examples mixed at the training\nstage and learns the parameters jointly. With the learned model to\nformulate the mapping between an algebra word problem and an\nequation template, the math problem solving strategy is similar to\nKAZB and ZDC. All the templates in the training set have to be\nexploited to ﬁnd the best alignment strategy.\n",
        "font-size": 9,
        "token": 218
      },
      {
        "text": "The aforementioned template-based methods suffer from two\ndrawbacks [13]. First, the math concept is expressed as an entire\ntemplate and may fail to work well when the training instances\nare sparse. Second, the learning process relies on lexical and\nsyntactic features such as the dependency path between two\nslots in a template. Such a huge and sparse feature space may\nplay a negative impact on effective feature learning. Based on\nthese two arguments, FG-Expression [13] parses an equation\ntemplate into ﬁne-grained units, called template fragment. Each\ntemplate is represented in a tree structure as in Figure 3 and each\n",
        "font-size": 9,
        "token": 134
      },
      {
        "text": "8\n",
        "font-size": 6,
        "token": 2
      },
      {
        "text": "fragment represents a sub-tree rooted at an internal node. The\nmain objective and challenge in [13] are learning an accurate\nmapping between textual information and template fragments.\nFor instance, a text piece “20% discount” can be mapped to\na template fragment 1 − n1 with n1 = 0.2. Such mappings are\nextracted in a semi-supervised way from training datasets and\nstored as part of the sketch for templates. The proposed solution\nto a math problem consists of two stages. First, RankSVM\nmodel [56] is trained to select top-k templates. The features used\nfor the training incorporate textual features, quantity features and\nsolution features. It is worth noting that the proposed template\nfragment is applied in the feature selection for the classiﬁer.\nThe textual features preserve one dimension to indicate whether\nthe problem text contains textual expressions in each template\nfragment. In the second stage, the alignment is conducted for the\nk templates and the one with the highest probability is used to\nsolve the problem. The features and rank-based classiﬁer used to\nselect the best alignment are similar to those used in the ﬁrst\nstage. Compared to the previous template-based methods, FG-\nExpression also signiﬁcantly reduces the search space because\nonly top-k templates are examined whereas previous methods\nalign numbers for all the templates in the training dataset.\n 3.4 DL-Based Methods\n",
        "font-size": 9,
        "token": 300
      },
      {
        "text": "Deep Neural Solver (DNS) [14] is the ﬁrst deep learning based\nalgorithm that does not rely on hand-crafted features. This\nis a milestone contribution because all the previous methods\n(including MathDQN) require human intelligence to help extract\nfeatures that are effective. The deep model used in DNS is a\ntypical sequence to sequence (seq2seq) model [61], [62], [63].\nThe words in the problem are vectorized into features through\nword embedding techniques [64], [65]. In the encoding layer,\nGRU [66] is used as the Recurrent Neural Network (RNN)\nto capture word dependency because compared to LSTM [67],\nGRU has fewer parameters in the model and is less likely to\nbe over-ﬁtting in small datasets. This seq2seq model translates\nmath word problems to equation templates, followed by a number\nmapping step to ﬁll the slots in the equation with the quantities\nextracted from the text. To ensure that the output equations by\nthe model are syntactically correct, ﬁve rules are pre-deﬁned as\nvalidity constraints. For example, if the ith character in the output\nsequence is an operator in {+, −, ×, ÷}, then the model cannot\nresult in c ∈ {+, −, ×, ÷, ), =} for the (i + 1)th character.\n",
        "font-size": 9,
        "token": 302
      },
      {
        "text": "To further improve the accuracy, DNS enhances the model\nin two ways. First, it builds a LSTM-based binary classiﬁcation\nto determine whether a number is relevant. This is\nmodel\nsimilar to the relevance model trained in ExpressionTree [30]\nand UNITDEP [32]. The difference is that DNS uses LSTM\nas the classiﬁer with unsupervised word-embedding features\nwhereas ExpressionTree and UNITDEP use SVM with hand-\ncrafted features. Second, the seq2seq model is integrated with\na similarity-based method [12] introduced in Section 3.2. Given\na pre-deﬁned threshold, the similarity-based retrieval strategy is\nselected as the solver if the maximal similarity score is higher\nthe seq2seq model is used to\nthan the threshold. Otherwise,\nsolve the problem. Another follow-up of DNS was proposed\nrecently in [68]. Instead of using GRU and LSTM, the math\nsolver examines the performance of other seq2seq models when\napplied in mapping the problem text to equation templates. In\nparticular, two models including BiLSTM [67] and structured\n",
        "font-size": 9,
        "token": 242
      }
    ]
  },
  {
    "page_num": 9,
    "content": [
      {
        "text": "TABLE 3\nStatistics of datasets for equation set problems.\n 9\n",
        "font-size": 6,
        "token": 15
      },
      {
        "text": "Datasets\n Proposed in\n # of problems\n # of templates\n",
        "font-size": 8,
        "token": 14
      },
      {
        "text": "ALG514\nDolphin1878\nDRAW1K\nDolphin18K\n",
        "font-size": 8,
        "token": 25
      },
      {
        "text": "KAZB [50]\nDOL [51]\nMixedSP [60]\nSIM [12]\n",
        "font-size": 8,
        "token": 24
      },
      {
        "text": "514\n1,878\n1,000\n18,460\n",
        "font-size": 8,
        "token": 23
      },
      {
        "text": "28\n1,183\n232\n5,871\n",
        "font-size": 8,
        "token": 19
      },
      {
        "text": "# of problems\n# of templates\n18.36\n1.59\n4.31\n3.14\n",
        "font-size": 6,
        "token": 29
      },
      {
        "text": "# of single-eq problems\n # of words\n # of sentences\n",
        "font-size": 8,
        "token": 15
      },
      {
        "text": "91\n712\n255\n8,333\n",
        "font-size": 8,
        "token": 17
      },
      {
        "text": "1.62k\n3.30k\n1.38k\n49.9k\n",
        "font-size": 8,
        "token": 24
      },
      {
        "text": "19.3k\n41.4k\n13.8k\n604k\n",
        "font-size": 8,
        "token": 23
      },
      {
        "text": "self-attention [69], were examined respectively for the equation\ntemplate classiﬁcation task. Results show that both models achieve\ncomparable performance. CASS [18] is an extension of seq2seq\nmodel. It adjusts the output sequence generation process by\nincorporating the copy and alignment mechanism. Reinforcement\nlearning (RL) technique is adopted and the model is trained using\npolicy gradient. Results show that the model trained by RL is more\naccurate than using maximum likelihood estimation (MLE) as the\nobjective function.\n 3.5 Dataset Repository and Performance Analysis\n",
        "font-size": 9,
        "token": 123
      },
      {
        "text": "Similar to the organization of Section 2, we summarize the dataset\nrepository and performance analysis for the equation set solvers.\n",
        "font-size": 9,
        "token": 25
      },
      {
        "text": "3.5.1 Benchmark Datasets\nThere have been four datasets speciﬁcally collected for the\nequation set problems that involve multiple unknown variables.\nWe present their descriptions in the following and summarize the\nstatistics of the datasets in Table 3. We use # of problems\n# of templates to report\nthe average number of problems associated with each template.\nWe noticed that in each dataset, a small fraction of problems are\nassociated with one unknown variable in the template. Thus, we\nalso report the number of single-equation problems in each dataset.\n1) ALG514 [50]. The dataset is crawled from Algebra.com,\na crowd-sourced tutoring website. The problems are posted\nby students. The problems with information gap or require\nexplicit background knowledge are discarded. Consequently,\na set of 1024 questions is collected and cleaned by crowd-\nworkers in Amazon Mechanical Turk. These problems are\nfurther ﬁltered as the authors require each equation template\nto appear for at least 6 times. Finally, 514 problems are left\nin the dataset.\n",
        "font-size": 9,
        "token": 228
      },
      {
        "text": "2) Dolphin1878 [51]. Its math problems are crawled from two\nwebsites: albegra.com and answers.yahoo.com. For math\nproblems in answers.yahoo.com, the math equations and\nanswers are manually added by human annotators. Finally,\nthe dataset combined from the two sources contains 1, 878\nmath problems with 1183 equation templates.\n",
        "font-size": 9,
        "token": 83
      },
      {
        "text": "3) DRAW1K [70]. The authors of DRAW1K argued that\nDolphin1878 has limited textual variations and lacks nar-\nrative. This motivated them to construct a new dataset that\nis diversiﬁed in both vocabularies and equation systems.\nWith these two objectives, they constructed DRAW1K with\nexactly 1, 000 linear equation problems that are crawled and\nﬁltered from algebra.com.\n4) Dolphin18K [12]. The dataset\n",
        "font-size": 9,
        "token": 109
      },
      {
        "text": "is collected and rectiﬁed\nmainly from the math category of Yahoo! Answers1. The\nproblems, equation system annotations, and answers are ex-\ntracted semi-automatically, with great intervention of human\n 1. https://answers.yahoo.com/\n",
        "font-size": 8,
        "token": 54
      },
      {
        "text": "efforts. The procedure consists of four stages: removing\nirrelevant problems, cleaning problem text, extracting gold\nanswers and constructing equation system annotations. The\nharvested dataset is so far the largest one, with 18, 460\nproblems and 5, 871 equation templates. Since the dataset\nis collected from online forum, there could exist errors in\nthe annotations and answers. The quality may not as high\nas those crawled from professional websites or constructed\nby paid crowd workers. Nevertheless, the large volume and\nhigh diversify make Dolphin18K a challenging dataset that is\nuseful to examine the generality and robustness of a MWP\nsolver. It has become a popular dataset as a number of MWP\nsolvers used it for experimental evaluation.\n",
        "font-size": 9,
        "token": 166
      },
      {
        "text": "5) AQuA [49] is a large-scale dataset published by DeepMind.\nThe authors ﬁrst collected 34,202 multi-choice math ques-\ntions covering a broad range of topics and difﬁculty levels.\nThe dataset is rather challenging because many problems are\nfrom GMAT (Graduate Management Admission Test) and\nGRE (General Test). In contrast, most of the aforementioned\nMWP datasets are targeted for elementary school students.\nMoreover, crowd workers are also involved in the dataset\nconstruction of AQuA, contributing 70,318 additional ques-\ntions. In total, the dataset contains around 100, 000 annotated\nmath problems.\n 3.5.2 Performance Analysis\n",
        "font-size": 9,
        "token": 158
      },
      {
        "text": "The performances of the equation set solvers on the existing\ndatasets are reported in Table 4. From the experimental results,\nwe derive the following observations and discussions.\n",
        "font-size": 9,
        "token": 33
      },
      {
        "text": "First, there are many empty cells in the table. In the ideal case,\nthe algorithms should be conducted on all the available benchmark\ndatasets and compared with all the previous competitors. The\nreasons are multi-fold, such as limitation of the implementation\n(e.g., as Upadhyay stated in [60], they could not run ZDC on\nDRAW1K because ZDC can only handle limited types of equation\nsystems), delayed release of the dataset (e.g., the Dolphin18K\ndataset has not been released when the work of DNS is published)\nor unﬁtness in certain scenarios (e.g., the experiments of FG-\nExpression [13] were only conducted on Dolphin18K because\nthe authors considered that the previous datasets are not suitable\ndue to their limitation in scalability and diversity). It is noticeable\nthat such an incomplete picture brings difﬁculty to judge the\nperformance and may miss certain insightful ﬁndings.\n",
        "font-size": 9,
        "token": 198
      },
      {
        "text": "Second, ALG514 is the smallest dataset and also the most\nwidely adopted dataset for performance evaluation. Among the\ntemplate-based methods, MixedSP outperforms KAZB and ZDC\nbecause it beneﬁts from the mined implicit supervision from an\nexternal source of additional 2, 000 samples. As reported in [60],\nif only the explicit dataset (i.e., the problems in ALG514) is used,\nits performance is slightly inferior to ZDC. A possible reason to\nexplain this is that ZDC uses a richer set of features based on POS\n",
        "font-size": 9,
        "token": 127
      }
    ]
  },
  {
    "page_num": 10,
    "content": [
      {
        "text": "10\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "TABLE 4\nAccuracies of equation set problem solvers on existing datasets.\n",
        "font-size": 7,
        "token": 17
      },
      {
        "text": "# of problems\n# of templates\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "Statistic-based\nSimilarity-based\n Template-based\n DL-based\n",
        "font-size": 8,
        "token": 13
      },
      {
        "text": "DOL [51]\nSIM [12]\nKAZB [50]\nZDC [55]\nMixedSP [60]\nFG-Expression [13]\nDNS [14]\nCASS [18]\n",
        "font-size": 8,
        "token": 48
      },
      {
        "text": "2015\n2016\n2014\n2015\n2016\n2017\n2017\n2018\n",
        "font-size": 8,
        "token": 40
      },
      {
        "text": "ALG514\n514\n28\n",
        "font-size": 8,
        "token": 13
      },
      {
        "text": "70.1\n68.7\n79.7\n83.0\n",
        "font-size": 8,
        "token": 20
      },
      {
        "text": "70.1\n82.5\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "Dolphin1878\n1,878\n1,183\n60.2\n29\n",
        "font-size": 8,
        "token": 28
      },
      {
        "text": "28.29\n29.7\n DRAW1K Dolphin18K\n",
        "font-size": 8,
        "token": 19
      },
      {
        "text": "1,000\n232\n",
        "font-size": 8,
        "token": 10
      },
      {
        "text": "25.5\n43.2\n 59.5\n",
        "font-size": 8,
        "token": 16
      },
      {
        "text": "31\n-\n",
        "font-size": 8,
        "token": 4
      },
      {
        "text": "18,460\n5,871\n 18.4\n 17.9\n",
        "font-size": 8,
        "token": 25
      },
      {
        "text": "28.4\n21.6\n29.0\n",
        "font-size": 8,
        "token": 15
      },
      {
        "text": "tags, coreference and dependency parses. In contrast, MixedSP\nonly uses features based on POS tags. It is also interesting to\nsee that SIM and DNS obtain the same accuracy on ALG514.\nThis is the dataset is too small to train an effective deep learning\nmodel. The reported accuracy of seq2seq model is only 16.1%\nin ALG514. DNS is a hybrid approach that combines a seq2seq\nmodel and similarity retrieval model. It means the deep learning\nmodel does not\ntake any effect when handling problems in\nALG514.\n Finally,\n",
        "font-size": 9,
        "token": 127
      },
      {
        "text": "the datasets of Dolphin1878 and DRAW1K are\nreleased for the approaches of DOL and MixedSP, respectively.\nIn the experimental settings, simple baselines such as SIM based\non textual similarity or KAZB which is the earliest template-based\nmethod, are selected. It is not surprising to see that Dolphin1878\nand DRAW1K outperform their competitors by a large margin.\nNevertheless, the research for equation set solvers has shifted to\nproposing methods that can work well in a large and diversiﬁed\ndataset such as Dolphin18K. We implemented our own version\nof DNS and evaluated its performance on the large dataset.\nUnfortunately, we did not observe a higher accuracy derived from\nDNS. The reasons could be two-fold. First, our implementation\nmay not be optimized and the model parameters may not be\nwell\nthere are thousands of templates in the\ndatasets, which may bring challenges for the classiﬁcation task.\nCASS further improved the accuracy on Dolphin18K to 29%,\nverifying the effectiveness of their proposed copy and alignment\nmechanism, as well as the optimization based on policy gradient.\n tuned. Second,\n",
        "font-size": 9,
        "token": 247
      },
      {
        "text": "4 FEATURE EXTRACTION\nBefore moving to the third class of math word problems (namely,\ngeometry problems), we take a detour to discuss feature extraction,\nwhich plays a vital component in all systems (except DSN)\ndiscussed in sections 2 and 3. Effective feature construction, in\nan either supervised or semi-supervised manner, can signiﬁcantly\nboost\nthe accuracy. For instance, SIFT [71], [72] and other\nlocal descriptors [73], [74], [75] have been intensively used in\nthe domain of object recognition and image retrieval [76], [77],\n[78] for decades as they are invariant to scaling, orientation and\nillumination changes. Consequently, a large amount of research\nefforts have been devoted to design effective features to facilitate\nML tasks. Such a discipline was partially changed by the\nemergence of deep learning. In the past years, deep learning\nhas transformed the world of artiﬁcial intelligence, due to the\nincreasing processing power afforded by graphical processing\nunits (GPUs), the enormous amount of available data, and the\ndevelopment of more advanced algorithms. With well-annotated\n",
        "font-size": 10,
        "token": 253
      },
      {
        "text": "sufﬁcient training data, the methodology can automatically learn\nthe effective latent feature representation for the classiﬁcation or\nprediction tasks. Hence, it can help replace the manual feature\nengineering process, which is non-trivial and labor-intensive.\n",
        "font-size": 9,
        "token": 51
      },
      {
        "text": "In the area of automatic math problem solver, as reviewed\nin the previous sections, the only DL-based approach without\nfeature engineering is DSN [14]. It applies word embedding to\nvectorize the text information and encodes these vectors by GRU\nnetwork for automatic feature extraction. The limitation is that\na large amount of labeled training data is required to make the\nmodel effective. Before the appearance of DSN, most of the math\nproblem solvers were designed with the availability of small-scale\ndatasets. Thus, feature engineering plays an important role in these\nworks to help achieve a high accuracy. In this section, we provide\na comprehensive review on the features engineering process in\nthe literature and show how they help bridge the gap between\ntextual/visual information and the semantic/logical representation.\n 4.1 Preprocessing\n",
        "font-size": 9,
        "token": 173
      },
      {
        "text": "Before we review the feature space deﬁned in various MWP\nsolvers, we ﬁrst present the preliminary background on the pre-\nprocessing steps that have been commonly adopted to facilitate\nthe subsequent feature extraction.\n 4.1.1 Syntactic Parsing\n",
        "font-size": 9,
        "token": 55
      },
      {
        "text": "Syntactic parsing focuses on organizing the lexical units and\ntheir semantic dependency in a tree structure, which serves as\na useful resource for effective feature selection. Sorts of parsers\nhave been developed, among which the Stanford parser works\nis a\nas the most comprehensive and widely-adopted one. It\nlanguage\npackage consisting of different probabilistic natural\nparsers. To be more speciﬁc, its neural-network parser [79] is a\ntransition-based dependency parser that uses high-order features\nto achieve high speed and good accuracy; the Compositional\n[80] can be seen as factoring discrete\nVector Grammar parser\nand continuous parsing in one model; and the (English) Stanford\nDependencies representation\n[81] is an automatic system to\nextract typed dependency parses from phrase structure parses,\nwhere a dependency parse represents dependencies between\nindividual words and a phrase structure parse represents nesting of\nmulti-word constituents. Besides Stanford parser, there exist other\neffective dependency parsers with their own traits. For example,\n[82] presents an easy-ﬁst parsing algorithm that iteratively selects\nthe best pair of neighbors in the tree structure to connect at each\nparsing step.\n",
        "font-size": 9,
        "token": 247
      }
    ]
  },
  {
    "page_num": 11,
    "content": [
      {
        "text": "Those parsers account in WMP solvers. For instance, the\nneural-network parser [79] is adopted in [83] for coreference\nresolution, which is another pre-processing step for MWP solvers.\nUnitDep [32] automatically generates features from a given\nmath problem by analyzing its derived parser tree using the\nCompositional Vector Grammar parser [80]. Additionally, the\nStanford Dependencies representation [81] has been applied in\nmultitple solvers. We observed its occurrence in Formula [27]\nand ARIS [25] to extract attributes of entities (the subject, verb,\nobject, preposition and temporal information), in KAZB [50]\nto generate part-of-speech tags, lematizations, and dependency\nparses to compute features, and in ALGES [31] to obtain\nsyntactic information used for grounding and feature computation.\nExpressionTree [30] is an exceptional case without using Stanford\nParser. Instead, it uses the easy-ﬁst parsing algorithm [82] to detect\nthe verb associated with each quantity.\n 4.1.2 Coreference Resolution\n",
        "font-size": 9,
        "token": 242
      },
      {
        "text": "Co-reference resolution involves the identiﬁcation and clustering\nof noun phrases mentions that refer to the same real-world entity.\nThe MWP solvers use it as a pre-processing step to ensure\nthe correct arithmetic operations or value update on the same\nentity.\n[84] is an early deterministic approach which is driven\nentirely by the syntactic and semantic compatibility learned from\na large, unlabeled corpus. It allows proper and nominal mentions\nto only corefer with antecedents that have the same head, but\npronominal mentions to corefer with any antecedent. On top\nof [84], Raghunathan et al. [85] proposed an architecture based on\ntiers of deterministic coreference models. The tiers are processed\nfrom the highest to the lowest precision and the entity output of\na tier is forwarded to the next tier for further processing. [86]\nis another model that integrates a collection of deterministic co-\nreference resolution models. Targeting at exploring rich feature\nspace, [87] proposed a simple classiﬁcation model for coreference\nresolution with a well-designed set of features. NECo is proposed\nin [88] and capable of solving both named entity linking and\nco-reference resolution jointly.\n",
        "font-size": 9,
        "token": 263
      },
      {
        "text": "As to applying coreference resolvers in MWP sovers, the\nIllinois Coreference Resolver [87] [89] is used in [24] to identify\npronoun referents and facilitate semantic labeling. Alternatively, a\nrule function Coref(A, B), which is true when A and B represent\nthe same entity, is derived in [83] as a component of the declarative\nrules to determine the math operators. Given a pair of sentences,\neach containing a quantity, ZDC [55] takes into account the\nexistence of coreference relationship between these two sentences\nfor feature exploitation. Meanwhile, ARIS [25] adopts the [85]\nfor coreference resolution and uses the predicted coreference\nrelationships to replace pronouns with their coreferenent links.\n 4.2 Common Features\n",
        "font-size": 9,
        "token": 175
      },
      {
        "text": "There have been various types of features proposed in the past\nliterature. We separate them into common features and unique\nfeatures, according to the number of solvers that have adopted\na particular type of feature. The unique features were proposed\nonce and not reused in another work, implying that their effect\ncould be limited. The common features are considered to be more\ngeneral and effective, and they are the focus of this survey.\n",
        "font-size": 9,
        "token": 88
      },
      {
        "text": "In Table 5, we categorize the common features according to\ntheir syntactic sources for feature extraction, such as quantities,\nquestions, verbs, etc. For each type of proposed feature, we\n",
        "font-size": 9,
        "token": 41
      },
      {
        "text": "11\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "identify its related MWP solvers, and provide necessary examples\nto explain features that are not straightforward to ﬁgure out.\n",
        "font-size": 9,
        "token": 27
      },
      {
        "text": "4.2.1 Quantity-related Features\nThe basic units in an arithmetic expression or an equation set\nconsist of quantities, unknown variables and operators. Hence, a\nnatural idea is to extract quantity-related features to help identify\nthe relevant operands and their associated operators. As shown in\nTable 5, a binary indicator to determine whether a quantity refers\nto a rate is adopted in many solvers [30] [32] [17] [55] [60]. It\nsignals a strong connection between the quantity and operators\nof {×, ÷}. The value of the quantity is also useful for operator\nclassiﬁer or quantity relevance classiﬁer. For instance, a quantity\nwhose value is a real number between [0, 1] is likely to be\nassociated with multiplication or division operators [55], [60]. It is\nalso observed that quantities in the text format of “one” or “two”\nare unlikely to be relevant with the solution [50] [55], [60], [13].\nExamples include “if one airplane averages 400 miles per hour,...”\nand “the difference between two numbers is 36”.\n",
        "font-size": 9,
        "token": 254
      },
      {
        "text": "4.2.2 Context-related Features\nThe information embedded in the text window centered at a\nparticular quantity can also provide important clues for solving\nmath word problems. To differentiate two quantities both in\nthe numeric format, we can leverage the word lemmas, part of\nspeech (POS) tags and dependence types within the window as\nthe features. In this manner, quantities associated with the same\noperators would to likely to share similar context information. A\ntrivial trick used in [30] [32] [17] is to examine whether there\nexists comparative adverbs. For example, terms “more”, “less”\nand “than” indicate operators of {+, −}.\n",
        "font-size": 9,
        "token": 144
      },
      {
        "text": "4.2.3 Quantity-pair Features\nThe relationship between two quantities is helpful to determine\ntheir associated operator. A straightforward example is that if two\nquantities are associated with the same unit, they can be applied\nwith addition and subtraction [30] [32] [17] [55]. If one quantity\nis related to a rate and the other is associated with a unit that\nis part of the rate, their operator is likely to be multiplication or\ndivision [30] [31] [32] [17].\n",
        "font-size": 9,
        "token": 117
      },
      {
        "text": "Numeric relation and context similarity are two types of\nquantity-pair features proposed in [55] [60]. The former obtains\ntwo sets of nouns located within the same sentence as the two\nquantities and sorts them by the distance in the dependency tree.\nThen, a scoring function is deﬁned to measure the similarity\nbetween these two sorted noun lists. Higher similarity implies that\nthe two quantities are more likely to be connected by addition\nor subtraction operators. The latter extracts features for equation\ntemplate classiﬁer. It is observed that the contextual information\nbetween two numbers is similar, they are likely to be located\nwithin in a template with symmetric number slots. For example,\ngiven a template n1 × u1 + n2 × u2, “n1” and “n2” are symmetric.\nThe context similarity is measured by the Jaccard similarity on\ntwo sets of words among the context windows. Given a problem\ntext “A plum costs 2 dollars and a peach costs 1 dollars”, “2” and\n“1” are two quantities with similar context.\n",
        "font-size": 7,
        "token": 226
      },
      {
        "text": "Two types of quantity-pair features were both adopted in the\ntemplate-based solutions to equation set problems [50] [55]. The\nﬁrst type is the dependency path between a pair of quantities. Their\nsimilarity may be helpful to determine the corresponding positions\n(or number slots) in the equation template. For example, given\n",
        "font-size": 9,
        "token": 70
      }
    ]
  },
  {
    "page_num": 12,
    "content": [
      {
        "text": "Feature Type\n",
        "font-size": 8,
        "token": 3
      },
      {
        "text": "Quantity-related\nFeatures\n",
        "font-size": 8,
        "token": 5
      },
      {
        "text": "Description\nWhether the quantity refers to a rate\nIs between 0 and 1\nIs equal to one or two\nWord lemma\n",
        "font-size": 8,
        "token": 28
      },
      {
        "text": "Context-related\nFeatures\n POS tags\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "TABLE 5\nCommon Features.\n",
        "font-size": 7,
        "token": 7
      },
      {
        "text": "Used In\n[30] [32] [17] [55] [60]\n[55] [60]\n[50] [55] [60] [13]\n[50] [55] [60]\n [32] [50] [55] [60] [24]\n Dependence type\n [50] [55] [60]\n Comparative adverbs\n [30] [32] [17]\n",
        "font-size": 8,
        "token": 111
      },
      {
        "text": "Quantity-pair\nFeatures\n",
        "font-size": 8,
        "token": 6
      },
      {
        "text": "Question-related\nFeatures\n",
        "font-size": 8,
        "token": 5
      },
      {
        "text": "Whether both quantities have the same\nunit\nIf one quantity is related to a rate and\nthe other is associated with a unit that\nis part of the rate\nNumeric relation of two quantities\n",
        "font-size": 8,
        "token": 40
      },
      {
        "text": "Context similarity between two quanti-\nties\nDependency path between two quanti-\nties.\n",
        "font-size": 8,
        "token": 18
      },
      {
        "text": "Whether both quantities appear in the\nsame sentence\nWhether the value of the ﬁrst quantity\nis greater than the other\nWhether the unit or related noun phrase\nof a quantity appears in the question\nWhether the unit or related noun phrase\nof a quantity has the highest number of\nmatch tokens with the question text\nNumber of quantities which happen to\nhave the maximum number of matching\ntokens with the question\n [30] [32] [17] [55]\n [30] [31] [32] [17]\n [55] [60]\n [55] [60]\n [50] [55]\n [50] [55]\n [50]\n [17]\n",
        "font-size": 8,
        "token": 157
      },
      {
        "text": "[32]\n[30]\n[55] [60]\n[24] [30] [31] [32] [17]\n[50]\n[30] [32] [17]\n [30] [32] [17]\n",
        "font-size": 8,
        "token": 64
      },
      {
        "text": "Whether any component of the rate is\npresent in the question\n [30] [32] [17]\n",
        "font-size": 8,
        "token": 25
      },
      {
        "text": "12\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "Remark\nFor “each ride cost 5 tickets”, the quantity “5” is a rate\n",
        "font-size": 8,
        "token": 20
      },
      {
        "text": "For “Connie has 41.0 red markers.”, the word lemmas around the\nquantity “41.0” are {Connie, have, red, marker}.\nFor “A chef needs to cook 16.0 potatoes.”, the POS tags within\na window of size 2 centered at the quantity “16.0” are {TO, VB,\nNNS}.\nFor “Ned bought 14.0 boxes of chocolates candy.”, we can detect\nmultiple dependencies within the window of size 2 around the\n“14.0”: (boxes, 14.0) → (num), (boxes, of)→ (prep), (bought,\nNed) → (nsubj). The dependence root is “bought”.\nFor “If she drank 25 of them and then bought 30 more.”, “more”\nis a comparative term in the window of quantity “30”.\nFor “Student tickets cost 4 dollars and general admission tickets\ncost 6 dollars”, quantities “4” and “6” have the same unit.\nFor “each box has 9 pieces” and “Paul bought 6 boxes of chocolate\ncandy”, “9” is related to a rate ( i.e., pieces/box) and “6” is\nassociated to the unit “box”.\nFor each quantity, the nouns around it are extracted and sorted by\nthe distance in the dependency tree. Then, a scoring function is\ndeﬁned on the two sorted lists to measure the numeric relation.\nThe context is represented by the set of words around the quantity.\n",
        "font-size": 8,
        "token": 339
      },
      {
        "text": "For “2 footballs and 3 soccer balls cost 220 dollars”,\nthe\ndependency path for the quantity pair (2, 3) is num(footballs,2)\n– conj(footballs, balls) – num(balls, 3).\n",
        "font-size": 8,
        "token": 56
      },
      {
        "text": "For the question “How many apples are left in the box?” and a\nquantity 77 that appears in “77 apples in a box”, there are two\nmatching tokens (”apples” and “box”).\nFor “Rose have 9 apples and 12 erasers. ... 3 friends. How many\napples dose each friend get?”, the number of matching tokens for\nquantities 9, 12 and 3 is 1, 0 and 1. Hence, there are two quantities\nwith the maximum matching token number.\nGiven a question “How many blocks does George have?” and a\nquantity 6 associated with rate “blocks/box”, the feature indicator\nis set to 1 since block appears in the question.\n the\n question\n",
        "font-size": 8,
        "token": 164
      },
      {
        "text": "Whether the question contains terms\nlike “each” or “per”\ncontains\nWhether\ncomparison-related terms like “more”\nor “less”\nWhether the question contains terms\nlike “how many”\nDependent verb of a quantity\nDistance vector between the dependent\nverb and a small collection of pre-\ndeﬁned verbs\nfor\nthat\narithmetic operator classiﬁcation\nWhether two quantities have the same\ndependent verbs\n are useful\n",
        "font-size": 8,
        "token": 91
      },
      {
        "text": "Verb-related\nFeatures\n [30] [32] [17]\n [30] [32] [17]\n [50] [55] [60] [13]\n It implies that the solution is positive.\n",
        "font-size": 8,
        "token": 53
      },
      {
        "text": "[30] [31] [32] [17]\n[25] [28] [31]\n [30] [32] [17]\n the verb closest to the quantity in the dependency tree\n",
        "font-size": 8,
        "token": 51
      },
      {
        "text": "For “In the ﬁrst round she scored 40 points and in the second\nround she scored 50 points”, the quantities “40” and “50” both\nhave the same verb “scored”. Note that “scored” appeared twice\nin the sentence.\nFor “She baked 4 cupcakes and 29 cookies.”, the quantities “4” and\n“29” both shared the verb “baked”. Note that “baked” appeared\nonly once in the sentence.\n",
        "font-size": 8,
        "token": 108
      },
      {
        "text": "Whether both dependent verbs refer to\nthe same verb mention\n [30] [32] [17]\n Global Features\n",
        "font-size": 8,
        "token": 27
      },
      {
        "text": "Number of quantities mentioned in text\nUnigrams and bigrams of sentences in\nthe problem text\n",
        "font-size": 8,
        "token": 20
      },
      {
        "text": "[30] [32] [17]\n[24] [50]\n",
        "font-size": 8,
        "token": 20
      }
    ]
  },
  {
    "page_num": 13,
    "content": [
      {
        "text": "a sentence “2 footballs and 3 soccer balls cost 220 dollars”, the\ndependency paths between two quantity pairs (2, 220) and (3, 220)\nare identical, implying that 2 and 3 refer to similar types of number\nslots in the template. The other feature is whether two quantities\nappear in the same sentence. If so, they are likely to appear in\nthe same equation of the template. Finally, a popular quantity-pair\nfeature used in [30] [32] [17] [50]\n[55] [60] examines whether\nthe value of one quantity is greater than the other, which is helpful\nto determine the correct operands for subtraction operator.\n",
        "font-size": 9,
        "token": 158
      },
      {
        "text": "4.2.4 Question-related Features\nDistinguishing features can also be derived from questions. It is\nstraightforward to ﬁgure out that the unknown variable can be\ninferred from the question and if a quantity whose unit appears\nin the question, this quantity is likely to be relevant. The remain\nquestion-related features presented in Table 5 were proposed by\nRoy et al. [30], [32] and followed by MathDQN [17]. Their\nfeature design leverages the number of matching tokens between\nthe related noun phrase of a quantity and the question text.\nThe quantities with the highest number of matching tokens are\nconsidered as useful clues. They also check whether the question\ncontains rate indicators such as “each” and “per”, or comparison\nindicators such as “more” or “less”. The former is related to {×, ÷}\nand the latter is related to {+, −}. Moreover, if the question text\ncontains “how many”, it implies that the solution is a positive\nnumber.\n",
        "font-size": 9,
        "token": 219
      },
      {
        "text": "4.2.5 Verb-related Features\nVerbs are important indicators for correct operator determination.\nFor example, “lose” is a verb indicating quantity loss for an entity\nand related to the subtraction operator. Given a quantity, we call\nthe verb closest to it in the dependency tree as its dependent\nverb.\n[30] [31] [32] [17] directly use dependent verb as one\nof the features. Another widely-adopted verb-related feature is a\nvector capturing the distance between the dependent verb and a\nsmall pre-deﬁned collection of verbs that are found to be useful in\ncategorizing arithmetic operations. Again, the remaining features\ncome from the works [30], [32], [17]. The features indicate\nwhether two quantities have the same dependent verbs or whether\ntheir dependent verbs refer to the same verb mention. As we can\nsee from the examples in Table 5, the difference between these\ntwo types of features is the occurrence number of the dependent\nverb in the sentence.\n",
        "font-size": 9,
        "token": 217
      },
      {
        "text": "4.2.6 Global Features\nThere are certain types of global features in the document-level\nproposed by existing solvers. [30], [32], [17] use the number of\nquantities in the problem text as part of feature space. Unigrams\nand bigrams are also applied in [24] [50]. They may play certain\neffect in determining the quantities and their order. Note that the\nunigrams and bigrams are deﬁned in the word level rather than the\ncharacter level.\n",
        "font-size": 9,
        "token": 112
      },
      {
        "text": "5 GEOMETRIC WORD PROBLEM (GWP) SOLVER\nGeometry solvers have been studied for a long history. Visual di-\nagram understanding is a sub-domain that has attracted signiﬁcant\nattention. As an early work for understanding line drawings, [90]\npresented an efﬁcient characteristic pattern detection method by\nscanning the distribution of black pixels and generating feature\npoints graph. A structure mapping engine named GeoRep was\n",
        "font-size": 10,
        "token": 92
      },
      {
        "text": "13\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "proposed in [91] to generate qualitative spatial descriptions from\nline diagrams. After that, the visual elements can be formulated\nthrough a two-level representation architecture. This work was\nalso applied to the repetition and symmetry detection model in\nMAGI [92]. Inspired by human cognitive process of reading\njuxtaposition diagrams, MAGI detects repetition by aligning visual\nand conceptual relational structure to analyze repetition-based\ndiagrams.\n",
        "font-size": 9,
        "token": 90
      },
      {
        "text": "The problem of rectangle and parallelogram detection in\ndiagram understanding has also received a considerable amount\nof interest. The proposed techniques fall into two main categories,\neither based on primitive or Hough transform [93]. The primitive-\nbased methods combine line segments or curves to form possible\nedges of a quadrangle. For examples, Lin and Nevatia [94] pro-\nposed the approach of parallelogram detection from a single aerial\nimage by linear feature extraction and formation of hypothesis\nfollowing certain geometric constraints. Similarly, Lagunovsky\nand Ablameyko [95] studied the problem of rectangular detection\nbased on line primitives. As to the Hough transform based\ntechniques, [96] presented an approach for automatic rectangular\nparticle detection in cryo-electron microscopy through Hough\ntransform, but this method can only work well when all rectangles\nhave the same dimensions and the dimensions must be aware in\nadvance. Jung et.al. [97] proposed a window Hough transform\nalgorithm to tackle the problem of rectangle detection with varying\ndimensions and orientations.\n",
        "font-size": 9,
        "token": 226
      },
      {
        "text": "Geometry theorem proving (GTP) [98], [99] was initially\nviewed as an artiﬁcial intelligence problem that was expected\nto be easily tackled by machines. The difﬁculties of solving\nGTP problems lie in the visual reasoning in geometry and the\ngeneration of elegant and concise geometry proofs. Moreover,\ncompleting the proof requires the ingenuity and insights to the\nproblem. The ﬁrst automated GTP was developed by Gelernter\n[98], which used the diagram as pruning heuristic. The system\nrejects geometry goals that fail to satisfy the diagram. Whereas\nthe limitation of the method is the true sub-goal may be pruned\nerroneously due to the insufﬁcient precise arithmetic applied to\nthe diagram. Fleuriot et. al. [99] studied Newton’s geometric\nreasoning procedures in his work Principia and presented theorem\nprover Isabelle to formalize and generate the style of reasoning\nperformed by Newton. By combining the existing geometry\ntheorem proving techniques and the concepts of Nonstandard\nAnalysis, the prover Isabelle can produce proofs of lemmas and\ntheorem in Principia. Readers can refer to the book chapter [99]\nfor the survey of early development of GTP.\n",
        "font-size": 9,
        "token": 266
      },
      {
        "text": "In this survey, we are more interested to examine the math\nproblems that are required to consider visual diagram and textual\nmentions simultaneously. As illustrated in Figure 5, a typical\ngeometry word problem contains text descriptions or attribute\nvalues of geometric objects. The visual diagram may contain\nessential information that are absent from the text. For instance,\npoints O, B and C are located on the same line segment\nand there is a circle passing points A, B, C and D. To well\nsolve geometry word problems, three main challenges need to\nbe tackled: 1) diagram parsing requires the detection of visual\nmentions, geometric characteristics, the spatial information and\nthe co-reference with text; 2) deriving visual semantics which\nrefer to the textual information related to visual analogue involves\nthe semantic and syntactic interpretation to the text; and 3) the\ninherent ambiguities lie in the task of mapping visual mentions in\nthe diagram to the concepts in real world.\n",
        "font-size": 9,
        "token": 200
      }
    ]
  },
  {
    "page_num": 14,
    "content": [
      {
        "text": "x\n y\n C\n D\n BB\n o\n A\n Fig. 5. An example of geometric problem.\n",
        "font-size": 7,
        "token": 25
      },
      {
        "text": "5.1 Text-Aligned Diagram Understanding\n",
        "font-size": 9,
        "token": 9
      },
      {
        "text": "The very early computer program, BEATRIX [100], [101], parses\nthe English text and diagram components of the elementary\nphysics problems together by establishing the coreference between\nthe text and diagram. Watanabe et al. proposed a framework\nto combine layout information and natural language to analyze\nthe pictorial book of ﬂora diagrams [102]. An overview of the\nresearch on integration of visual and linguistic information was\nprovided in the survey paper by Srihar [103]. However, these\nearly approaches rely on written rules or manual regulations,\ni.e., the visual elements needed to be recognized with human\nintervention and their performances were usually dependent on\nspeciﬁed diagrams.\n",
        "font-size": 9,
        "token": 151
      },
      {
        "text": "G-ALINGER [104] is an algorithmic work that addresses\nthe geometry understanding and text understanding simultane-\nously. To detect primitives from a geometric diagram, Hough\ntransform [105] is ﬁrst applied to initialize lines and circles\nsegments. An objective function that incorporates pixel coverage,\nvisual coherence and textual-visual alignment. The function is\nsub-modular and a greedy algorithm is designed to pick the\nprimitive with the maximum gain in each iteration. The algorithm\nstops when no positive gain can be obtained according to the\nobjective function. In [106], the problem of visual understanding\nis addressed in the context of science diagrams. The objective is to\nidentify the graphic representation for the visual entities and their\nrelations such as temporal transitions, phase transformations and\ninter object dependencies. An LSTM-based network is proposed\nfor syntactic parsing of diagrams and learns the graphic structure.\n 5.2 GWP Solvers\n",
        "font-size": 9,
        "token": 197
      },
      {
        "text": "GEOS [107] can be considered as the ﬁrst work to tackle\na complete geometric word problem as shown in Figure 5.\nThe method consists of two main steps: 1) parsing text and\ndiagram respectively by generating a piece of logical expression\n",
        "font-size": 9,
        "token": 57
      },
      {
        "text": "14\n",
        "font-size": 6,
        "token": 3
      },
      {
        "table": "||\n|C x BB y o A D|\n||\n||\n||\n||\n||",
        "token": 15
      }
    ]
  },
  {
    "page_num": 15,
    "content": [
      {
        "text": "they can use fewer number of models in the semantic parsing.\nThe experiments were conducted on a very small-scale dataset\nwith 104 problems. Recently, there has been the ﬁrst attempt\nto solve Arabic arithmetic word problems [120]. Its test dataset\nwas collected by translating the AI2 dataset [25] from English\nto Arabic. The proposed techniques also rely on the verb\ncategorization, similar to those proposed in [25], except that\ncustomization for the Arabic language needs to be made for\nthe tasks of syntactic parser and named entity recognition. To\nconclude, the math word problem solvers in other languages than\nEnglish are still at a very early stage. The datasets used are\nneither large-scale nor challenging and the proposed techniques\nare obsolete. This research area has great room for improvement\nand calls for more efforts to be involved.\n 6.3 Math Problem Generator\n",
        "font-size": 9,
        "token": 192
      },
      {
        "text": "We also review automatic math word problem generators that\ncan efﬁciently produce a large, diverse and conﬁgurable corpus\nof question-answer database. The topics covered in this survey\ninclude algebra word problems with basic operators {+, −, ×, ÷}\nand geometry problems.\n",
        "font-size": 9,
        "token": 59
      },
      {
        "text": "In [121], Wang et al. leveraged the concept of expression\ntree to generate a math word problem. The tree structure can\nprovide the skeleton of the story, and meanwhile allow the story\nto be constructed recursively from the sub-stories. Each sub-story\ncan be seen as a text template with value slots to be ﬁlled.\nThese sub-stories will be concatenated into an entire narrative.\nDifferent from [121], the work of [122] rewrites a given math\nword problem to ﬁt a particular theme such as Star War. In\nthis way, students may stay more engaged with their homework\nassignments. The candidate are scored with the coherence in\nmultiple factors (e.g., syntactic, semantic and thematic). [123]\ngenerates math word problems that match the personal interest\nof students. The generator uses Answer Set Programming [124],\nin which programs are composed of facts and rules in a ﬁrst-order\nlogic representation, to satisfy a collection of pedagogical and\nnarrative requirements. Its objective is to produce coherent and\npersonalized story problems that meet pedagogical requirements.\nIn the branch of geometry problem generator, GeoTutor [125],\n[126] is designed to generate geometry proof problems for high\nschool students. The input contains a ﬁgure and a set of geometry\nis a pair (I, G), where I refers to the\naxioms. The output\nassumptions for the ﬁgure and goals in G are sets of explicit facts\nto be inferred. Singhal et al. also tackled the automated generation\nof geometry questions for high school students [127], [128]. Its\ninput interface allows users to select geometric objects, concepts\nand theorems. Compared with [125], [126], its geometric ﬁgure\nis generated by the algorithm rather than speciﬁed by the user.\nBased on the ﬁgure, the next step of generating facts and solutions\nis similar to that in [125], [126]. It requires pre-knowledge on\naxioms and theorems and results in the formation capturing the\nrelationships between its objects.\n",
        "font-size": 9,
        "token": 471
      },
      {
        "text": "15\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "and conducted accountable experimental analysis. Moreover, we\ntook a close examination on the subject of feature engineering\nproposed for MWP solvers and summarized the diversiﬁed\nproposal of syntactic features.\n",
        "font-size": 9,
        "token": 42
      },
      {
        "text": "Overall speaking, the current status of MWP solvers still has\ngreat room for improvement. There is no doubt that the topic\nwould continue to attract more and more research attention in the\nnext few years, especially after the public release of large-scale\ndatasets such as Dolphin18K and Math23K. In the following, we\npresent a number of possible future directions that may be of\ninterest to the community.\n",
        "font-size": 9,
        "token": 88
      },
      {
        "text": "Firstly, DNS [14] was the ﬁrst attempt that used deep learning\nmodels in MWP solvers so as to avoid non-trivial feature\nengineering. This work shed light on the feasibility of designing\nend-to-end models to enhance the accuracy and reduce human\nintervention. We observed that there have been a number of\npublications following this direction. For example, T-RNN is a\nrecent work which uses Bi-LSTM and self-attention to generate\nquantity representation and applies recursive neural networks to\ninfer the unknown variables in the expression tree.\n",
        "font-size": 9,
        "token": 118
      },
      {
        "text": "Secondly, aligning visual understanding with text mention\nis an emerging direction that is particularly important to solve\ngeometry word problems. However, this challenging problem has\nonly been evaluated in self-collected and small-scale datasets,\nsimilar to those early efforts on evaluating the accuracy of solving\nalgebra word problem. There is a chance that these proposed\naligning methods fail to work well in a large and diversiﬁed\ndataset. Hence, it calls for a new round of evaluation for generality\nand robustness with a better benchmark dataset for geometry\nproblems.\n",
        "font-size": 9,
        "token": 115
      },
      {
        "text": "Thirdly, interpretability plays a key role in measuring the\nusability of MWP solvers in the application of online tutoring,\nbut may pose new challenges for the deep learning based\nsolvers [113]. For instance, AlphaGo [129] and AlphaZero [130]\nhave achieved astonishing superiority over human players, but\ntheir near-optimal actions could be difﬁcult for human to interpret.\nSimilarly, for MWP solvers, domain knowledge and reasoning\ncapability are useful and they are friendly for interpretation. It\nmay be interesting to combine the merits of DL models, domain\nknowledge and reasoning capability to develop more powerful\nMWP solvers.\n",
        "font-size": 9,
        "token": 140
      },
      {
        "text": "Last but not the least, solving math word problems in English\nplays a dominating role in the literature. We only observed a\nvery rare number of math solvers proposed to cope with other\nlanguages. This research topic may grow into a direction with\nsigniﬁcant impact. To our knowledge, many companies in China\nhave harvested an enormous number of word problems in K12\neducation. As reported in 20152, Zuoyebang, a spin off from\nBaidu, has collected 950 million questions and solutions in its\ndatabase. When coupled with deep learning models, this is an\narea with immense imagination and exciting achievements can be\nexpected.\n",
        "font-size": 7,
        "token": 142
      },
      {
        "text": "7 CONCLUSIONS AND FUTURE DIRECTIONS\nIn this paper, we present a comprehensive survey to review the\ndevelopment of math word problem solvers in recent years. The\ntopics discussed in this survey cover arithmetic word problems,\nequation set problems, geometry word problems and miscel-\nlaneous others related to math. We compared the techniques\nproposed for each math task, provided a rational categorization,\n",
        "font-size": 9,
        "token": 84
      },
      {
        "text": "REFERENCES\n [1]\n [2]\n",
        "font-size": 8,
        "token": 8
      },
      {
        "text": "D. Bobrow, “Natural language input for a computer problem solving\nsystem,” in Semantic information processing, M. Minsky, Ed. MIT\nPress, 1964, pp. 146–226.\nE. A. Feigenbaum and J. Feldman, Computers and Thought. New\nYork, NY, USA: McGraw-Hill, Inc., 1963.\n 2. http://www.marketing-interactive.com/baidus-zuoyebang-attracts-\n outside-investors/\n",
        "font-size": 8,
        "token": 115
      }
    ]
  },
  {
    "page_num": 16,
    "content": [
      {
        "text": "[3]\n [4]\n [5]\n [6]\n [7]\n [8]\n [9]\n",
        "font-size": 8,
        "token": 21
      },
      {
        "text": "E. Charniak, “Computer solution of calculus word problems,” in IJCAI,\n1969, pp. 303–316.\nP. Clark, “Elementary school science and math tests as a driver for AI:\ntake the aristo challenge!” in AAAI, 2015, pp. 4019–4021.\nP. Clark and O. Etzioni, “My computer is an honor student - but how\nintelligent is it? standardized tests as a measure of AI,” AI Magazine,\nvol. 37, no. 1, pp. 5–12, 2016.\nJ. R. Slagle, “Experiments with a deductive question-answering\nprogram,” Commun. ACM, vol. 8, no. 12, pp. 792–798, Dec. 1965.\nC. R. Fletcher, “Understanding and solving arithmetic word problems:\nA computer simulation,” Behavior Research Methods, Instruments, &\nComputers, vol. 17, no. 5, pp. 565–571, Sep 1985.\nY. Bakman, “Robust Understanding of Word Problems with Extraneous\nInformation,” ArXiv Mathematics e-prints, 2007.\nA. Mukherjee and U. Garain, “A review of methods for automatic\nunderstanding of natural language mathematical problems,” Artif. Intell.\nRev., vol. 29, no. 2, pp. 93–122, Apr. 2008.\n [10] D. Goldwasser and D. Roth, “Learning from natural instructions,” in\n IJCAI, 2011, pp. 1794–1800.\n",
        "font-size": 8,
        "token": 400
      },
      {
        "text": "[11] T. Kwiatkowski, E. Choi, Y. Artzi, and L. S. Zettlemoyer, “Scaling\nsemantic parsers with on-the-ﬂy ontology matching,” in EMNLP, 2013,\npp. 1545–1556.\n",
        "font-size": 8,
        "token": 67
      },
      {
        "text": "[12] D. Huang, S. Shi, C. Lin, J. Yin, and W. Ma, “How well do\ncomputers solve math word problems? large-scale dataset construction\nand evaluation,” in ACL, 2016.\n",
        "font-size": 8,
        "token": 54
      },
      {
        "text": "[13] D. Huang, S. Shi, J. Yin, and C.-Y. Lin, “Learning ﬁne-grained\nexpressions to solve math word problems,” in EMNLP, 2017, pp. 805–\n814.\n [14] Y. Wang, X. Liu, and S. Shi, “Deep neural solver for math word\n problems,” in EMNLP, 2017, pp. 845–854.\n",
        "font-size": 8,
        "token": 111
      },
      {
        "text": "[15] L. Wang, Y. Wang, D. Cai, D. Zhang, and X. Liu, “Translating\nmath word problem to expression tree,” in EMNLP. Association for\nComputational Linguistics, 2018, pp. 1064–1069.\n",
        "font-size": 8,
        "token": 70
      },
      {
        "text": "[16] T. Chiang and Y. Chen, “Semantically-aligned equation generation\nsolving and reasoning math word problems,” CoRR, vol.\n",
        "font-size": 8,
        "token": 33
      },
      {
        "text": "for\nabs/1811.00720, 2018.\n",
        "font-size": 8,
        "token": 21
      },
      {
        "text": "[17] L. Wang, D. Zhang, L. Gao, J. Song, L. Guo, and H. T. Shen, “Mathdqn:\nSolving arithmetic word problems via deep reinforcement learning,” in\nAAAI. AAAI Press, 2018.\n",
        "font-size": 8,
        "token": 63
      },
      {
        "text": "[18] D. Huang, J. Liu, C. Lin, and J. Yin, “Neural math word problem solver\nwith reinforcement learning,” in COLING, 2018, pp. 213–223.\n[19] L. Wang, D. Zhang, J. Zhang, X. Xu, L. Gao, B. Dai, and H. T.\nShen, “Template-based mathword problem solvers with recursive neural\nnetworks,” in AAAI. AAAI Press, 2019.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:\nA Large-Scale Hierarchical Image Database,” in CVPR09, 2009.\n[21] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and\nD. Parikh, “Vqa: Visual question answering,” in ICCV, Dec 2015, pp.\n2425–2433.\n [20]\n",
        "font-size": 8,
        "token": 255
      },
      {
        "text": "[22] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making\nthe V in VQA matter: Elevating the role of image understanding in\nvisual question answering,” in CVPR, 2017, pp. 6325–6334.\n[23] R. Yun, M. Yuhui, C. Guangzuo, H. Ronghuai,\n",
        "font-size": 8,
        "token": 104
      },
      {
        "text": "“Frame-based calculus of\nZ. Ying,\nstep addition and subtraction word problems,”\nTechnology\non(ETCS), vol. 02, 03 2010, pp. 476–479.\ndoi.ieeecomputersociety.org/10.1109/ETCS.2010.316\n and Computer\n Science,\n",
        "font-size": 8,
        "token": 85
      },
      {
        "text": "and\nsolving arithmetic multi-\nin Education\nInternational Workshop\n[Online]. Available:\n [24] S. Roy, T. Vieira, and D. Roth, “Reasoning about quantities in natural\n language,” TACL, vol. 3, pp. 1–13, 2015.\n",
        "font-size": 8,
        "token": 68
      },
      {
        "text": "[25] M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman, “Learning to\nsolve arithmetic word problems with verb categorization,” in EMNLP,\n2014, pp. 523–533.\n",
        "font-size": 8,
        "token": 64
      },
      {
        "text": "[26] S. S. Sundaram and D. Khemani, “Natural language processing for\nsolving simple word problems,” in Proceedings of the 12th International\nConference on Natural Language Processing. Trivandrum, India: NLP\nAssociation of India, December 2015, pp. 394–402.\n [27] A. Mitra and C. Baral, “Learning to use formulas to solve simple\n arithmetic problems,” in ACL, 2016.\n",
        "font-size": 8,
        "token": 112
      },
      {
        "text": "[28] C.-C. Liang, K.-Y. Hsu, C.-T. Huang, C.-M. Li, S.-Y. Miao, and K.-Y.\nSu, “A tag-based english math word problem solver with understanding,\nreasoning and explanation,” in NAACL, 2016.\n",
        "font-size": 8,
        "token": 71
      },
      {
        "text": "[29] C. Liang, K. Hsu, C. Huang, C. Li, S. Miao, and K. Su, “A tag-\nbased statistical english math word problem solver with understanding,\nreasoning and explanation,” in IJCAI, 2016, pp. 4254–4255.\n [30] S. Roy and D. Roth, “Solving general arithmetic word problems,” in\n EMNLP, 2015, pp. 1743–1752.\n",
        "font-size": 8,
        "token": 121
      },
      {
        "text": "16\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "[31] R. Koncel-Kedziorski, H. Hajishirzi, A. Sabharwal, O. Etzioni, and\nS. D. Ang, “Parsing algebraic word problems into equations,” TACL,\nvol. 3, pp. 585–597, 2015.\n",
        "font-size": 8,
        "token": 74
      },
      {
        "text": "[32] S. Roy and D. Roth, “Unit dependency graph and its application to\narithmetic word problem solving,” in AAAI. AAAI Press, 2017, pp.\n3082–3088.\n [33] ——, “Illinois math solver: Math reasoning on the web,” in NAACL,\n 2016.\n [34] S. Roy, S. Upadhyay, and D. Roth, “Equation parsing : Mapping\n sentences to grounded equations,” in EMNLP, 2016, pp. 1088–1097.\n",
        "font-size": 8,
        "token": 138
      },
      {
        "text": "[35] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and\nD. Parikh, “VQA: visual question answering,” in ICCV, 2015, pp. 2425–\n2433.\n [37]\n",
        "font-size": 8,
        "token": 79
      },
      {
        "text": "[36] D. Zhang, R. Cao, and S. Wu, “Information fusion in visual question\nanswering: A survey,” Information Fusion, vol. 52, pp. 268 – 280, 2019.\nJ. Song, Y. Guo, L. Gao, X. Li, A. Hanjalic, and H. T. Shen,\n“From deterministic to generative: Multimodal stochastic rnns for video\ncaptioning,” IEEE Transactions on Neural Networks and Learning\nSystems, 2018.\n",
        "font-size": 8,
        "token": 125
      },
      {
        "text": "[38] Y. Bin, Y. Yang, F. Shen, N. Xie, H. T. Shen, and X. Li, “Describing\nvideo with attention based bidirectional lstm,” IEEE Transactions on\nCybernetics, 2018.\n",
        "font-size": 8,
        "token": 59
      },
      {
        "text": "[39] Y. Yang, J. Zhou, J. Ai, Y. Bin, A. Hanjalic, and H. T. Shen,\n“Video captioning by adversarial lstm,” IEEE Transactions on Image\nProcessing, 2018.\n",
        "font-size": 8,
        "token": 55
      },
      {
        "text": "[40] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, “Video captioning\nwith attention-based LSTM and semantic consistency,” IEEE Trans.\nMultimedia, vol. 19, no. 9, pp. 2045–2055, 2017.\n",
        "font-size": 8,
        "token": 78
      },
      {
        "text": "[41] L. Guo, D. Zhang, Y. Wang, H. Wu, B. Cui, and K. Tan, “Co2: Inferring\npersonal interests from raw footprints by connecting the ofﬂine world\nwith the online world,” ACM Trans. Inf. Syst., vol. 36, no. 3, pp. 31:1–\n31:29, 2018.\n",
        "font-size": 8,
        "token": 97
      },
      {
        "text": "[42] S. Wu, W. Ren, C. Yu, G. Chen, D. Zhang, and J. Zhu, “Personal\nrecommendation using deep recurrent neural networks in netease,” in\nICDE.\n",
        "font-size": 8,
        "token": 48
      },
      {
        "text": "IEEE Computer Society, 2016, pp. 1218–1229.\n[43] X. Wang, L. Nie, X. Song, D. Zhang, and T. Chua, “Unifying virtual and\nphysical worlds: Learning toward local and global consistency,” ACM\nTrans. Inf. Syst., vol. 36, no. 1, pp. 4:1–4:26, 2017.\n",
        "font-size": 8,
        "token": 103
      },
      {
        "text": "[44] Y. Wang, D. Zhang, Y. Liu, B. Dai, and L. H. Lee, “Enhancing\ntransportation systems via deep learning: A survey,” Transportation\nResearch Part C: Emerging Technologies, vol. 99, pp. 144 – 163, 2019.\n[45] K. Narasimhan, T. D. Kulkarni, and R. Barzilay, “Language\nunderstanding for text-based games using deep reinforcement learning,”\nin EMNLP, 2015, pp. 1–11.\n",
        "font-size": 8,
        "token": 136
      },
      {
        "text": "[46] K. Narasimhan, A. Yala, and R. Barzilay, “Improving information\nextraction by acquiring external evidence with reinforcement learning,”\nin EMNLP, 2016, pp. 2355–2365.\n [47] H. Guo, “Generating Text with Deep Reinforcement Learning,” ArXiv\n [48]\n",
        "font-size": 8,
        "token": 90
      },
      {
        "text": "e-prints, Oct. 2015.\nJ. C. Caicedo and S. Lazebnik, “Active object localization with deep\nreinforcement learning,” in ICCV, 2015, pp. 2488–2496.\n[49] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman,\n",
        "font-size": 8,
        "token": 89
      },
      {
        "text": "and\nH. Hajishirzi, “MAWPS: A math word problem repository,” in NAACL,\n2016, pp. 1152–1157.\n",
        "font-size": 8,
        "token": 42
      },
      {
        "text": "[50] N. Kushman, L. Zettlemoyer, R. Barzilay, and Y. Artzi, “Learning to\nautomatically solve algebra word problems,” in ACL, 2014, pp. 271–\n281.\n",
        "font-size": 8,
        "token": 61
      },
      {
        "text": "[51] S. Shi, Y. Wang, C. Lin, X. Liu, and Y. Rui, “Automatically solving\nnumber word problems by semantic parsing and reasoning,” in EMNLP,\n2015, pp. 1132–1142.\n [52] D. E. Knuth, “Semantics of context-free languages.” Mathematical\n [53]\n",
        "font-size": 8,
        "token": 88
      },
      {
        "text": "Systems Theory, vol. 2, no. 2, pp. 127–145, 1968.\nJ. Earley, “An efﬁcient context-free parsing algorithm,” Commun. ACM,\nvol. 13, no. 2, pp. 94–102, 1970.\n",
        "font-size": 8,
        "token": 76
      },
      {
        "text": "[54] D. Zhang, L. Nie, H. Luan, K. Tan, T. Chua, and H. T. Shen, “Compact\nindexing and judicious searching for billion-scale microblog retrieval,”\nACM Trans. Inf. Syst., vol. 35, no. 3, pp. 27:1–27:24, 2017.\n",
        "font-size": 8,
        "token": 90
      },
      {
        "text": "[55] L. Zhou, S. Dai, and L. Chen, “Learn to solve algebra word problems\nusing quadratic programming,” in EMNLP, 2015, pp. 817–822.\n[56] R. Herbrich, T. Graepel, and K. Obermayer, “Large margin rank\nboundaries for ordinal regression,” in Advances in Large Margin\nClassiﬁers, P. J. Bartlett, B. Schölkopf, D. Schuurmans, and A. J. Smola,\nEds. MIT Press, 2000, pp. 115–132.\nJ. Nocedal and S. J. Wright, Numerical Optimization, 2nd ed. New\nYork: Springer, 2006.\n [57]\n [58] V. Vapnik, The Nature of Statistical Learning Theory.\n",
        "font-size": 8,
        "token": 203
      }
    ]
  },
  {
    "page_num": 17,
    "content": [
      {
        "text": "[59] D. Koller and N. Friedman, Probabilistic Graphical Models: Principles\nand Techniques - Adaptive Computation and Machine Learning. The\nMIT Press, 2009.\n",
        "font-size": 8,
        "token": 43
      },
      {
        "text": "[86] H. Lee, Y. Peirsman, A. X. Chang, N. Chambers, M. Surdeanu, and\nD. Jurafsky, “Stanford’s multi-pass sieve coreference resolution system\nat the conll-2011 shared task,” in CoNLL, 2011, pp. 28–34.\n",
        "font-size": 8,
        "token": 83
      },
      {
        "text": "17\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "[60] S. Upadhyay, M. Chang, K. Chang, and W. Yih, “Learning from explicit\nand implicit supervision jointly for algebra word problems,” in EMNLP,\n2016, pp. 297–306.\nI. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\nwith neural networks,” in NIPS, 2014, pp. 3104–3112.\n [61]\n",
        "font-size": 8,
        "token": 117
      },
      {
        "text": "[62] M. Luong, Q. V. Le, I. Sutskever, O. Vinyals, and L. Kaiser, “Multi-task\nsequence to sequence learning,” CoRR, vol. abs/1511.06114, 2015.\n[63] S. Wiseman and A. M. Rush, “Sequence-to-sequence learning as beam-\n search optimization,” in EMNLP, 2016, pp. 1296–1306.\n",
        "font-size": 8,
        "token": 117
      },
      {
        "text": "[64] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\nand their\n",
        "font-size": 8,
        "token": 36
      },
      {
        "text": "“Distributed representations of words\ncompositionality,” in NIPS, 2013, pp. 3111–3119.\nJ. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in EMNLP, 2014, pp. 1532–1543.\n and phrases\n [65]\n",
        "font-size": 8,
        "token": 93
      },
      {
        "text": "[66] K. Cho, B. van Merrienboer, Ç. Gülçehre, F. Bougares, H. Schwenk,\nand Y. Bengio, “Learning phrase representations using RNN encoder-\ndecoder for statistical machine translation,” CoRR, vol. abs/1406.1078,\n2014.\n [67] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\n Computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n",
        "font-size": 8,
        "token": 135
      },
      {
        "text": "[68] B. Robaidek, R. Koncel-Kedziorski, and H. Hajishirzi, “Data-Driven\nMethods for Solving Algebra Word Problems,” ArXiv e-prints, Apr.\n2018.\n",
        "font-size": 8,
        "token": 56
      },
      {
        "text": "[69] Z. Lin, M. Feng, C. Nogueira dos Santos, M. Yu, B. Xiang, B. Zhou, and\nY. Bengio, “A Structured Self-attentive Sentence Embedding,” ArXiv e-\nprints, Mar. 2017.\n",
        "font-size": 8,
        "token": 68
      },
      {
        "text": "[70] S. Upadhyay and M. Chang, “Annotating derivations: A new evaluation\nstrategy and dataset for algebra word problems,” in EACL, 2017, pp.\n494–504.\n [71] D. G. Lowe, “Object recognition from local scale-invariant features,” in\n ICCV, 1999, pp. 1150–1157.\n",
        "font-size": 8,
        "token": 98
      },
      {
        "text": "[72] ——, “Distinctive image features from scale-invariant keypoints,”\nInternational Journal of Computer Vision, vol. 60, no. 2, pp. 91–110,\n2004.\n [73] N. Dalal and B. Triggs, “Histograms of oriented gradients for human\n",
        "font-size": 8,
        "token": 74
      },
      {
        "text": "[87] E. Bengtson and D. Roth, “Understanding the value of features for\ncoreference resolution,” in Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, ser. EMNLP ’08, 2008, pp.\n294–303.\n",
        "font-size": 8,
        "token": 66
      },
      {
        "text": "[88] H. Hajishirzi, L. Zilles, D. S. Weld, and L. S. Zettlemoyer, “Joint\ncoreference resolution and named-entity linking with multi-pass sieves,”\nin EMNLP, 2013, pp. 289–299.\n [89] K.-W. Chang, R. Samdani, and D. Roth, “A Constrained Latent Variable\n Model for Coreference Resolution,” in EMNLP, 2013.\n",
        "font-size": 8,
        "token": 117
      },
      {
        "text": "[90] X. Lin, S. Shimotsuji, M. Minoh, and T. Sakai, “Efﬁcient diagram\nunderstanding with characteristic pattern detection,” Computer Vision,\nGraphics, and Image Processing, vol. 30, no. 1, pp. 84–106, 1985.\n[91] R. W. Ferguson and K. D. Forbus, “Georep: A ﬂexible tool for spatial\n representation of line drawings,” in AAAI, 2000, pp. 510–516.\n [92] ——, “Telling juxtapositions: Using repetition and alignable difference\n in diagram understanding,” 1998.\n",
        "font-size": 8,
        "token": 164
      },
      {
        "text": "[93] R. O. Duda and P. E. Hart, “Use of the hough transformation to detect\nlines and curves in pictures,” Commun. ACM, vol. 15, no. 1, pp. 11–15,\n1972.\n",
        "font-size": 8,
        "token": 62
      },
      {
        "text": "[94] C. Lin and R. Nevatia, “Building detection and description from a single\nintensity image,” Computer Vision and Image Understanding, vol. 72,\nno. 2, pp. 101–121, 1998.\n",
        "font-size": 8,
        "token": 61
      },
      {
        "text": "[95] D. Lagunovsky and S. Ablameyko, “Straight-line-based primitive\nextraction in grey-scale object recognition,” Pattern Recognition Letters,\nvol. 20, no. 10, pp. 1005–1014, 1999.\n",
        "font-size": 8,
        "token": 67
      },
      {
        "text": "[96] Y. Zhu, B. Carragher, F. Mouche, and C. S. Potter, “Automatic\nparticle detection through efﬁcient hough transforms,” IEEE Trans.\nMed. Imaging, vol. 22, no. 9, pp. 1053–1062, 2003.\n [97] C. R. Jung and R. Schramm, “Rectangle detection based on a windowed\n hough transform,” in SIBGRAPI, 2004, pp. 113–120.\n",
        "font-size": 8,
        "token": 127
      },
      {
        "text": "[98] H. Gelernter, “Computers &amp; thought,” E. A. Feigenbaum and\nJ. Feldman, Eds. MIT Press, 1995, ch. Realization of a Geometry-\ntheorem Proving Machine, pp. 134–152.\nJ. Fleuriot, Geometry Theorem Proving.\n2001, pp. 11–30.\n London: Springer London,\n [99]\n detection,” in CVPR, 2005, pp. 886–893.\n [100] W. C. Bulko, “Understanding text with an accompanying diagram,” in\n",
        "font-size": 8,
        "token": 149
      },
      {
        "text": "[74] H. Bay, A. Ess, T. Tuytelaars, and L. J. V. Gool, “Speeded-up robust\nfeatures (SURF),” Computer Vision and Image Understanding, vol. 110,\nno. 3, pp. 346–359, 2008.\n",
        "font-size": 8,
        "token": 77
      },
      {
        "text": "[75] G. Mori, S. J. Belongie, and J. Malik, “Efﬁcient shape matching using\nshape contexts,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 11,\npp. 1832–1837, 2005.\n",
        "font-size": 8,
        "token": 74
      },
      {
        "text": "[76] D. Zhang, D. Agrawal, G. Chen, and A. K. H. Tung, “Hashﬁle:\nAn efﬁcient index structure for multimedia data,” in ICDE.\nIEEE\nComputer Society, 2011, pp. 1103–1114.\n",
        "font-size": 8,
        "token": 71
      },
      {
        "text": "[77] X. Xu, F. Shen, Y. Yang, H. T. Shen, and X. Li, “Learning discriminative\nbinary codes for large-scale cross-modal retrieval,” IEEE Trans. Image\nProcessing, vol. 26, no. 5, pp. 2494–2507, 2017.\n",
        "font-size": 8,
        "token": 78
      },
      {
        "text": "[78] F. Shen, Y. Xu, L. Liu, Y. Yang, Z. Huang, and H. T. Shen,\n“Unsupervised deep hashing with similarity-adaptive and discrete\noptimization,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 12,\npp. 3034–3044, 2018.\n [79] D. Chen and C. D. Manning, “A fast and accurate dependency parser\n using neural networks,” in ACL, 2014, pp. 740–750.\n [80] R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng, “Parsing with\n compositional vector grammars,” in ACL, 2013, pp. 455–465.\n",
        "font-size": 8,
        "token": 191
      },
      {
        "text": "[81] M. de Marneffe, B. MacCartney, and C. D. Manning, “Generating typed\ndependency parses from phrase structure parses,” in LREC, 2006, pp.\n449–454.\n",
        "font-size": 8,
        "token": 55
      },
      {
        "text": "[82] Y. Goldberg and M. Elhadad, “An efﬁcient algorithm for easy-ﬁrst non-\ndirectional dependency parsing,” in Human Language Technologies: the\n2010 Conference of the North American Chapter of the Association for\nComputational Linguistics, 2010, pp. 742–750.\n [83] S. Roy and D. Roth, “Mapping to Declarative Knowledge for Word\n Problem Solving,” ArXiv e-prints, dec 2017.\n",
        "font-size": 8,
        "token": 116
      },
      {
        "text": "[84] A. Haghighi and D. Klein, “Simple coreference resolution with rich\nsyntactic and semantic features,” in Proceedings of the 2009 Conference\non Empirical Methods in Natural Language Processing: Volume 3-\nVolume 3. Association for Computational Linguistics, 2009, pp. 1152–\n1161.\n",
        "font-size": 8,
        "token": 83
      },
      {
        "text": "[85] K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu,\nD. Jurafsky, and C. D. Manning, “A multi-pass sieve for coreference\nresolution,” in EMNLP, 2010, pp. 492–501.\n IEA/AIE (Vol. 2), 1988, pp. 894–898.\n",
        "font-size": 8,
        "token": 106
      },
      {
        "text": "[101] G. S. Novak and W. C. Bulko, “Understanding natural language with\ndiagrams,” in Proceedings of the 8th National Conference on Artiﬁcial\nIntelligence. Boston, Massachusetts, July 29 - August 3, 1990, 2\nVolumes., 1990, pp. 465–470.\n",
        "font-size": 8,
        "token": 87
      },
      {
        "text": "[102] Y. Watanabe and M. Nagao, “Diagram understanding using integration\nof layout information and textual information,” in COLING-ACL, 1998,\npp. 1374–1380.\n",
        "font-size": 8,
        "token": 54
      },
      {
        "text": "[103] R. K. Srihari, “Computational models for integrating linguistic and\nvisual information: A survey,” Artif. Intell. Rev., vol. 8, no. 5-6, pp.\n349–369, 1994.\n",
        "font-size": 8,
        "token": 63
      },
      {
        "text": "[104] M. J. Seo, H. Hajishirzi, A. Farhadi, and O. Etzioni, “Diagram\nunderstanding in geometry questions,” in AAAI, 2014, pp. 2831–2838.\n [105] L. G. Shapiro and G. C. Stockman, Computer Vision.\n Prentice Hall,\n 2001.\n",
        "font-size": 8,
        "token": 95
      },
      {
        "text": "[106] A. Kembhavi, M. Salvato, E. Kolve, M. J. Seo, H. Hajishirzi, and\nA. Farhadi, “A diagram is worth a dozen images,” in ECCV, 2016,\npp. 235–251.\n",
        "font-size": 8,
        "token": 74
      },
      {
        "text": "[107] M. J. Seo, H. Hajishirzi, A. Farhadi, O. Etzioni, and C. Malcolm, “Solv-\ning geometry problems: Combining text and diagram interpretation,” in\nEMNLP, 2015, pp. 1466–1476.\n",
        "font-size": 8,
        "token": 75
      },
      {
        "text": "[108] M. Sachan, A. Dubey, and E. P. Xing, “From textbooks to knowledge: A\ncase study in harvesting axiomatic knowledge from textbooks to solve\ngeometry problems,” in EMNLP, 2017, pp. 784–795.\n",
        "font-size": 8,
        "token": 69
      },
      {
        "text": "[109] C. Alvin, S. Gulwani, R. Majumdar, and S. Mukhopadhyay, “Synthesis\nof problems for shaded area geometry reasoning,” in AIED, 2017, pp.\n455–458.\n",
        "font-size": 8,
        "token": 63
      },
      {
        "text": "[110] P. Clark, O. Etzioni, T. Khot, A. Sabharwal, O. Tafjord, P. D. Turney,\nand D. Khashabi, “Combining retrieval, statistics, and inference to\nanswer elementary science questions,” in AAAI, 2016, pp. 2580–2586.\n[111] G. Cheng, W. Zhu, Z. Wang, J. Chen, and Y. Qu, “Taking up the\ngaokao challenge: An information retrieval approach,” in IJCAI, 2016,\npp. 2479–2485.\n",
        "font-size": 8,
        "token": 151
      },
      {
        "text": "[112] J. Hernández-Orallo, F. Martínez-Plumed, U. Schmid, M. Siebers, and\nD. L. Dowe, “Computer models solving intelligence test problems:\nProgress and implications (extended abstract),” in IJCAI, 2017, pp.\n5005–5009.\n",
        "font-size": 8,
        "token": 81
      }
    ]
  },
  {
    "page_num": 18,
    "content": [
      {
        "text": "[113] H. Wang, F. Tian, B. Gao, C. Zhu, J. Bian, and T. Liu, “Solving\nverbal questions in IQ test by knowledge-powered word embedding,”\nin EMNLP, 2016, pp. 541–550.\n",
        "font-size": 8,
        "token": 70
      },
      {
        "text": "[114] I. Lev, B. MacCartney, C. D. Manning, and R. Levy, “Solving logic\npuzzles: From robust processing to precise semantics,” in TextMean,\n2004, pp. 9–16.\n",
        "font-size": 8,
        "token": 59
      },
      {
        "text": "[115] A. Dries, A. Kimmig, J. Davis, V. Belle, and L. D. Raedt, “Solving\nprobability problems in natural language,” in IJCAI, 2017, pp. 3981–\n3987.\n",
        "font-size": 8,
        "token": 69
      },
      {
        "text": "[116] L. De Raedt, A. Kimmig, and H. Toivonen, “Problog: A probabilistic\nprolog and its application in link discovery,” in Proceedings of the 20th\nInternational Joint Conference on Artiﬁcal Intelligence, ser. IJCAI’07,\n2007, pp. 2468–2473.\n",
        "font-size": 8,
        "token": 89
      },
      {
        "text": "[117] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom, “Program induction\nby rationale generation: Learning to solve and explain algebraic word\nproblems,” in ACL, 2017, pp. 158–167.\n [118] X. Yu, M. Wang, Z. Zeng, and J. Fan, “Solving directly-stated arithmetic\n word problems in chinese,” in EITT, Oct 2015, pp. 51–55.\n",
        "font-size": 8,
        "token": 122
      },
      {
        "text": "[119] X. Yu, P. Jian, M. Wang, and S. Wu, “Extraction of implicit quantity\nrelations for arithmetic word problems in chinese,” in EITT, Sept 2016,\npp. 242–245.\n [120] B. Siyam, A. A. Saa, O. Alqaryouti, and K. Shaalan, “Arabic arithmetic\n word problems solver,” in ACLING, 2017, pp. 153–160.\n [121] K. Wang and Z. Su, “Dimensionally guided synthesis of mathematical\n word problems,” in IJCAI, 2016, pp. 2661–2668.\n",
        "font-size": 8,
        "token": 170
      },
      {
        "text": "[122] R. Koncel-Kedziorski, I. Konstas, L. Zettlemoyer, and H. Hajishirzi,\n“A theme-rewriting approach for generating algebra word problems,” in\nEMNLP, 2016, pp. 1617–1628.\n",
        "font-size": 8,
        "token": 73
      },
      {
        "text": "[123] O. Polozov, E. O’Rourke, A. M. Smith, L. Zettlemoyer, S. Gulwani, and\nZ. Popovic, “Personalized mathematical word problem generation,” in\nIJCAI, 2015, pp. 381–388.\n",
        "font-size": 8,
        "token": 75
      },
      {
        "text": "[124] M. Gebser, R. Kaminski, B. Kaufmann, and T. Schaub, Answer Set\nSolving in Practice, ser. Synthesis Lectures on Artiﬁcial Intelligence\nand Machine Learning. Morgan & Claypool Publishers, 2012.\n[125] C. Alvin, S. Gulwani, R. Majumdar, and S. Mukhopadhyay, “Synthesis\n of geometry proof problems,” in AAAI, 2014, pp. 245–252.\n [126] ——, “Automatic synthesis of geometry problems for an intelligent\n tutoring system,” CoRR, vol. abs/1510.08525, 2015.\n",
        "font-size": 8,
        "token": 169
      },
      {
        "text": "[127] R. Singhal, M. Henz, and K. McGee, “Automated generation of high\nschool geometric questions involving implicit construction,” in CSEDU,\n2014, pp. 467–472.\n [128] ——, “Automated generation of geometry questions for high school\n mathematics,” in CSEDU, 2014, pp. 14–25.\n J. Schrittwieser,\n",
        "font-size": 8,
        "token": 105
      },
      {
        "text": "[129] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den\nDriessche,\nI. Antonoglou, V. Panneershelvam,\nM. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,\nI. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and\nD. Hassabis, “Mastering the game of go with deep neural networks and\ntree search,” 2016.\n",
        "font-size": 8,
        "token": 148
      },
      {
        "text": "[130] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, and A. Bolton, “Mastering the\ngame of go without human knowledge.” Nature, vol. 550, no. 7676, p.\n354, 2017.\n",
        "font-size": 8,
        "token": 98
      },
      {
        "text": "18\n",
        "font-size": 6,
        "token": 3
      },
      {
        "text": "Lei Wang is currently a graduate student\nin the\nUniversity of Electronic Science and Technology of\nChina. He has been a research intern in Tencent AI\nLab, Singapore Management University, and Afanti\nResearch. His research interests mainly focus on\nnature language processing, reinforcement learning\nand machine learning.\n",
        "font-size": 7,
        "token": 62
      },
      {
        "text": "Luming Zhang (M’14) is a researcher at Zhejiang\nUniversity, China. His research interests include im-\nage enhancement and pattern recognition. He has\nauthored/co-authored more than 70 scientiﬁc arti-\ntop venues including IEEE T-IP, T-MM, T-\ncles at\nCYB, CVPR ACM MM,\nIJCAI and AAAI. He was\nthe program committee member/organizer of many\ninternational conferences, such as MMM, PCM, ACM\nMultimedia. He is/was the associate editor of many\njournals like Neurocomputing and KSII\ninternational\nTransactions on Internet and Information Systems.\n",
        "font-size": 7,
        "token": 139
      },
      {
        "text": "Bing Tian Dai is currently an assistant professor at\nthe School of Information Systems, Singapore Man-\nagement University. He is also the director of MITB\n(Artiﬁcial Intelligence) Programme. Bing Tian received\nhis PhD from National University of Singapore in 2011\nwith research works on applying machine learning\ntechniques to solve database problems for data min-\ning applications. Prior to his faculty position, he was a\nresearch scientist at Living Analytics Research Cen-\ntre, working on graph mining and machine learning\nproblems on social media and social networks.\n",
        "font-size": 7,
        "token": 116
      },
      {
        "text": "Dongxiang Zhang is a research in Zhejiang Univer-\nsity, China. He received the B.Sc. degree from Fudan\nUniversity, China in 2006 and the PhD degree from\nNational University of Singapore in 2012. He worked\nas a research fellow at the NeXT research center in\nSingapore from 2012 to 2014, and he was promoted\nto senior research fellow in 2015. His current research\ninterests include smart education and natural\nlan-\nguage processing.\n",
        "font-size": 7,
        "token": 121
      },
      {
        "text": "Heng Tao Shen is currently a Professor of National\n“Thousand Talents Plan”, the Dean of School of Com-\nputer Science and Engineering, and the Director of\nCenter for Future Media at the University of Electronic\nScience and Technology of China. He is also an\nHonorary Professor at the University of Queensland.\nHe obtained his BSc with 1st class Honours and\nPhD from Department of Computer Science, National\nUniversity of Singapore in 2000 and 2004 respectively.\nHe then joined the University of Queensland as a\nLecturer, Senior Lecturer, Reader, and became a\nProfessor in late 2011. His research interests mainly include Multimedia Search,\nComputer Vision, Artiﬁcial Intelligence, and Big Data Management. He has\npublished 200+ peer-reviewed papers, most of which appeared in top ranked\npublication venues, such as ACM Multimedia, CVPR,\nIJCAI,\nSIGMOD, VLDB, ICDE, TOIS, TIP, TPAMI, TKDE, VLDB Journal, etc. He has\nreceived 6 Best Paper Awards from international conferences, including the Best\nPaper Award from ACM Multimedia 2017 and Best Paper Award - Honorable\nMention from ACM SIGIR 2017. He has served as a PC Co-Chair for ACM\nMultimedia 2015 and currently is an Associate Editor of IEEE Transactions on\nKnowledge and Data Engineering.\n ICCV, AAAI,\n",
        "font-size": 7,
        "token": 312
      }
    ]
  }
]